export const mlEngineerQuestions = [
  // ===== EASY (30 questions) =====
  { questionText: "What is the difference between a data scientist and an ML engineer?", idealAnswer: "Data scientists focus on exploratory analysis, model development, and deriving insights. ML engineers focus on building production-ready ML systems — deploying models, building pipelines, ensuring reliability and scalability. Data scientists prototype in notebooks; ML engineers write production code, CI/CD pipelines, and monitoring. ML engineering bridges the gap between model development and production deployment.", category: "Career", difficulty: "EASY" as const, tags: ["career", "roles", "ml-engineering"] },
  { questionText: "What is the difference between training and inference in ML?", idealAnswer: "Training is the process of learning model parameters from data — computationally expensive, done periodically (batch). Inference is using the trained model to make predictions on new data — must be fast, done continuously in production. Training optimizes weights; inference applies them. Different hardware needs: training benefits from GPUs/TPUs for parallelism; inference may use CPUs or specialized inference chips (TensorRT, ONNX Runtime) for low latency.", category: "ML Fundamentals", difficulty: "EASY" as const, tags: ["training", "inference", "fundamentals"] },
  { questionText: "What is a neural network and how does it learn?", idealAnswer: "A neural network is composed of layers of interconnected nodes (neurons). Each connection has a weight. Learning: (1) Forward pass — input flows through layers, each node applies weights + activation function, (2) Loss calculation — compare prediction to actual, (3) Backward pass (backpropagation) — calculate gradients of loss with respect to each weight, (4) Update weights using optimizer (SGD, Adam). Repeated over many iterations (epochs) until loss converges. Activation functions (ReLU, sigmoid) add non-linearity.", category: "Deep Learning", difficulty: "EASY" as const, tags: ["neural-network", "backpropagation", "deep-learning"] },
  { questionText: "What is overfitting and how do you prevent it?", idealAnswer: "Overfitting occurs when a model memorizes training data instead of learning generalizable patterns — high training accuracy, low test accuracy. Prevention: (1) More training data, (2) Regularization (L1/L2, dropout), (3) Early stopping (stop when validation loss increases), (4) Data augmentation, (5) Simpler model architecture, (6) Cross-validation, (7) Batch normalization. Detect by comparing train vs validation loss curves — divergence indicates overfitting.", category: "ML Fundamentals", difficulty: "EASY" as const, tags: ["overfitting", "regularization", "generalization"] },
  { questionText: "What is PyTorch and how does it differ from TensorFlow?", idealAnswer: "Both are deep learning frameworks. PyTorch: dynamic computation graph (define-by-run), Pythonic, preferred in research, eager execution by default. TensorFlow: originally static graph (define-then-run), now supports eager mode (TF 2.0), better production tooling (TF Serving, TF Lite). PyTorch has better debugging (standard Python debugger works). TensorFlow has broader deployment ecosystem. Both support GPU acceleration, distributed training, and model export (ONNX bridges them).", category: "Frameworks", difficulty: "EASY" as const, tags: ["pytorch", "tensorflow", "frameworks"] },
  { questionText: "What is a loss function and why is it important?", idealAnswer: "A loss function measures how far the model's predictions are from actual values — it's what the model minimizes during training. Common losses: MSE (regression), Cross-Entropy (classification), Binary Cross-Entropy (binary classification), Focal Loss (imbalanced classification). The choice of loss function directly affects model behavior. Custom losses can encode domain knowledge (e.g., penalizing false negatives more in medical diagnosis).", category: "ML Fundamentals", difficulty: "EASY" as const, tags: ["loss-function", "optimization", "training"] },
  { questionText: "What is gradient descent and its variants?", idealAnswer: "Gradient descent minimizes loss by iteratively updating weights in the direction of steepest descent. Variants: Batch GD (entire dataset per step — slow but stable), Stochastic GD (one sample — fast but noisy), Mini-batch GD (subset — balanced, most common). Optimizers: SGD with momentum, Adam (adaptive learning rates per parameter, most popular), AdamW (weight decay fix), LAMB (for large batch training). Learning rate scheduling: warmup, cosine decay, step decay.", category: "ML Fundamentals", difficulty: "EASY" as const, tags: ["gradient-descent", "optimizers", "adam"] },
  { questionText: "What is a GPU and why is it important for ML?", idealAnswer: "GPUs (Graphics Processing Units) have thousands of cores optimized for parallel matrix operations — the core computation in deep learning. Training a model that takes days on CPU can take hours on GPU. NVIDIA GPUs (A100, H100) are standard for ML. CUDA is NVIDIA's parallel computing platform. Alternatives: TPUs (Google, optimized for TensorFlow), Apple M-series (unified memory). Cloud: AWS p4d/p5 instances, Google Cloud TPUs. GPU memory limits batch size and model size.", category: "Infrastructure", difficulty: "EASY" as const, tags: ["gpu", "cuda", "hardware"] },
  { questionText: "What is transfer learning?", idealAnswer: "Transfer learning reuses a model pre-trained on a large dataset for a new task. Process: take a pretrained model (ResNet for images, BERT for text), replace the final layer(s), fine-tune on your dataset. Benefits: requires much less data and training time, leverages learned features. Common: ImageNet-pretrained CNNs for computer vision, BERT/GPT for NLP. Techniques: freeze early layers (general features), fine-tune later layers (task-specific). Dramatically reduces the data and compute needed for good performance.", category: "ML Fundamentals", difficulty: "EASY" as const, tags: ["transfer-learning", "fine-tuning", "pretrained"] },
  { questionText: "What is a dataset split and why do you need train/validation/test sets?", idealAnswer: "Training set: model learns from this. Validation set: tune hyperparameters and monitor overfitting during training. Test set: final unbiased performance evaluation — never used during development. Typical split: 70/15/15 or 80/10/10. Never tune based on test set performance (data leakage). Use stratified splits for classification (preserve class ratios). For time series: temporal split (train on past, test on future). Cross-validation uses multiple train/validation splits for more reliable estimates.", category: "ML Fundamentals", difficulty: "EASY" as const, tags: ["data-split", "validation", "evaluation"] },
  { questionText: "What is Docker and why is it important for ML engineering?", idealAnswer: "Docker packages applications with all dependencies into containers — ensuring consistent environments. For ML: (1) Reproducible training environments (Python version, library versions, CUDA), (2) Model serving with consistent dependencies, (3) CI/CD pipeline consistency, (4) GPU support (nvidia-docker), (5) Easy deployment to cloud (ECS, Kubernetes). Dockerfile defines the environment. Use multi-stage builds: build stage (heavy, with compilers) → runtime stage (lightweight). Pin all dependency versions for reproducibility.", category: "Infrastructure", difficulty: "EASY" as const, tags: ["docker", "containers", "reproducibility"] },
  { questionText: "What is feature engineering for ML?", idealAnswer: "Feature engineering transforms raw data into features that improve model performance. Techniques: normalization/standardization, one-hot encoding, embedding lookup, binning continuous variables, interaction features, polynomial features, time-based features (day of week, hour), text vectorization (TF-IDF, embeddings), log transforms for skewed distributions. Good features often matter more than model choice. Deep learning can learn features automatically but still benefits from domain-informed engineering.", category: "Data", difficulty: "EASY" as const, tags: ["feature-engineering", "preprocessing", "data"] },
  { questionText: "What is batch size and how does it affect training?", idealAnswer: "Batch size is the number of samples processed before updating model weights. Small batch: noisier gradients (regularization effect), slower convergence, less memory. Large batch: smoother gradients, faster training (GPU parallelism), risk of sharp minima (poor generalization). Common sizes: 32-256 for most tasks, larger (1024+) for language models with learning rate scaling. GPU memory limits maximum batch size. Gradient accumulation simulates larger batches on limited memory.", category: "Training", difficulty: "EASY" as const, tags: ["batch-size", "training", "hyperparameters"] },
  { questionText: "What is MLOps?", idealAnswer: "MLOps applies DevOps practices to ML systems. Core pillars: (1) Version control: code + data + models + experiments, (2) CI/CD: automated training, validation, deployment pipelines, (3) Monitoring: model performance, data drift, prediction quality, (4) Reproducibility: tracked experiments, versioned datasets, containerized environments, (5) Automation: automated retraining, feature computation, model promotion. Tools: MLflow (tracking), DVC (data versioning), Kubeflow (pipelines), Seldon/BentoML (serving). MLOps reduces the time from prototype to production.", category: "MLOps", difficulty: "EASY" as const, tags: ["mlops", "devops", "automation"] },
  { questionText: "What is model serialization and common formats?", idealAnswer: "Model serialization saves a trained model to disk for later loading or deployment. Formats: pickle/joblib (Python, scikit-learn), PyTorch (.pt/.pth state_dict), TensorFlow SavedModel, ONNX (Open Neural Network Exchange — framework-agnostic), TorchScript (optimized PyTorch), TensorRT (NVIDIA inference optimization). ONNX enables converting between frameworks (PyTorch → ONNX → TensorRT). For deployment: use optimized formats (ONNX Runtime, TensorRT) for faster inference.", category: "Deployment", difficulty: "EASY" as const, tags: ["serialization", "onnx", "deployment"] },
  { questionText: "What are activation functions and why are they needed?", idealAnswer: "Activation functions introduce non-linearity into neural networks — without them, stacking layers would be equivalent to a single linear transformation. Common: ReLU (max(0,x) — simple, fast, most popular), Sigmoid (0-1, used in output for binary classification), Tanh (-1 to 1), Softmax (probability distribution for multi-class), GELU (used in transformers). ReLU can cause 'dying neurons' (output always 0); Leaky ReLU and GELU address this.", category: "Deep Learning", difficulty: "EASY" as const, tags: ["activation-functions", "relu", "neural-networks"] },
  { questionText: "What is data augmentation?", idealAnswer: "Data augmentation artificially increases training dataset size by applying transformations to existing data. Images: rotation, flipping, cropping, color jittering, cutout, mixup. Text: synonym replacement, back-translation, random insertion/deletion. Audio: time stretching, pitch shifting, noise injection. Benefits: reduces overfitting, improves generalization, especially with limited data. Apply only to training set, not validation/test. Libraries: Albumentations (images), nlpaug (text). Can be done on-the-fly during training or pre-computed.", category: "Data", difficulty: "EASY" as const, tags: ["data-augmentation", "training", "generalization"] },
  { questionText: "What is a confusion matrix?", idealAnswer: "A confusion matrix shows True Positives (correctly predicted positive), False Positives (incorrectly predicted positive), True Negatives, and False Negatives for classification. Derived metrics: Accuracy = (TP+TN)/total, Precision = TP/(TP+FP), Recall = TP/(TP+FN), F1 = 2×Precision×Recall/(Precision+Recall). For multi-class: one row/column per class. Helps identify which classes the model confuses. Essential for understanding performance beyond accuracy, especially with imbalanced classes.", category: "Evaluation", difficulty: "EASY" as const, tags: ["confusion-matrix", "evaluation", "metrics"] },
  { questionText: "What is an embedding in ML?", idealAnswer: "An embedding is a learned dense vector representation of discrete data (words, items, categories) in a continuous space. Word embeddings (Word2Vec, GloVe): similar words have similar vectors. Token embeddings in transformers: learned during pre-training. Item embeddings in recommender systems: similar items cluster together. Benefits: captures semantic relationships, reduces dimensionality vs one-hot encoding, enables similarity computations. Embedding layers are trainable neural network layers that map integer indices to dense vectors.", category: "Deep Learning", difficulty: "EASY" as const, tags: ["embeddings", "representation", "nlp"] },
  { questionText: "What is hyperparameter tuning?", idealAnswer: "Hyperparameters are settings not learned during training (learning rate, batch size, architecture choices). Tuning methods: Grid Search (exhaustive, slow), Random Search (often more efficient), Bayesian Optimization (Optuna, uses past results to guide search), Population-Based Training (PBT, adjusts during training). Key hyperparameters: learning rate (most impactful), batch size, model depth/width, regularization strength, dropout rate. Use validation set for evaluation, never test set. Start broad, then narrow the range.", category: "Training", difficulty: "EASY" as const, tags: ["hyperparameters", "tuning", "optuna"] },
  { questionText: "What is the difference between CNN and RNN architectures?", idealAnswer: "CNNs (Convolutional Neural Networks) use filters/kernels to detect spatial patterns — designed for grid-structured data (images). Key: convolution layers, pooling layers, feature hierarchies (edges → textures → objects). RNNs (Recurrent Neural Networks) process sequential data by maintaining hidden state across time steps — designed for sequences (text, time series). Key: hidden state, LSTM/GRU for long-range dependencies. Transformers have largely replaced RNNs for sequences due to parallelization and attention mechanism.", category: "Deep Learning", difficulty: "EASY" as const, tags: ["cnn", "rnn", "architectures"] },
  { questionText: "What is a learning rate and why is it important?", idealAnswer: "The learning rate controls how much weights are updated per step: w = w - lr × gradient. Too high: overshoots optimal values, training diverges. Too low: converges very slowly, may get stuck in local minima. Typically: 1e-3 to 1e-5 for Adam. Scheduling: warmup (start low, increase), cosine decay (gradually decrease), step decay (reduce at milestones). Learning rate is usually the most important hyperparameter to tune. Learning rate finder: sweep across rates, find where loss decreases fastest.", category: "Training", difficulty: "EASY" as const, tags: ["learning-rate", "training", "optimization"] },
  { questionText: "What is a REST API for model serving?", idealAnswer: "Model serving via REST API exposes a trained model as an HTTP endpoint. Client sends input data (JSON), server runs inference, returns prediction. Frameworks: FastAPI (Python, async, auto-docs), Flask, BentoML, TorchServe, TF Serving. Endpoint: POST /predict with input features in body, returns prediction + confidence. Considerations: input validation, batch prediction support, model versioning, health checks, authentication, latency requirements. FastAPI with uvicorn is the simplest production-ready option.", category: "Deployment", difficulty: "EASY" as const, tags: ["api", "model-serving", "fastapi"] },
  { questionText: "What is cross-validation?", idealAnswer: "Cross-validation evaluates model performance by splitting data into k folds. K-Fold: train on k-1 folds, validate on remaining fold, repeat k times, average metrics. Provides more reliable performance estimate than a single split. Stratified K-Fold preserves class distribution per fold. Leave-One-Out: k = dataset size (expensive but thorough for small datasets). Use for: model selection, hyperparameter tuning. k=5 or k=10 is standard. Report mean ± standard deviation of metrics across folds.", category: "Evaluation", difficulty: "EASY" as const, tags: ["cross-validation", "evaluation", "model-selection"] },
  { questionText: "What is a data pipeline in ML?", idealAnswer: "An ML data pipeline automates the flow from raw data to model-ready features. Steps: (1) Ingest from sources (databases, APIs, files), (2) Validate data quality (schema, completeness), (3) Clean (handle missing values, duplicates), (4) Transform (feature engineering, normalization, encoding), (5) Split (train/validation/test), (6) Serve features to training and inference. Tools: Apache Airflow (orchestration), Spark (large-scale processing), dbt (SQL transforms), Feast (feature store). Pipelines must be reproducible, testable, and monitored.", category: "Infrastructure", difficulty: "EASY" as const, tags: ["data-pipeline", "etl", "infrastructure"] },
  { questionText: "What is model versioning and why is it important?", idealAnswer: "Model versioning tracks different iterations of models with their code, data, hyperparameters, and metrics. Important for: reproducibility (recreate any model), comparison (which version is best), rollback (revert to previous version on issues), auditing (regulatory compliance). Tools: MLflow Model Registry, DVC, Weights & Biases. Track: model artifacts, training code (git hash), dataset version, hyperparameters, metrics, environment. A model without versioning is a model you can't reproduce or debug.", category: "MLOps", difficulty: "EASY" as const, tags: ["versioning", "mlflow", "reproducibility"] },
  { questionText: "What is the vanishing gradient problem?", idealAnswer: "In deep networks, gradients can become extremely small as they're propagated backward through many layers, effectively stopping early layers from learning. Causes: sigmoid/tanh activations squash gradients to near-zero. Solutions: ReLU activation (gradient is 1 for positive values), residual connections (skip connections in ResNets), batch normalization, LSTM/GRU gates (for RNNs), careful weight initialization (He, Xavier). This is why modern networks use ReLU variants and skip connections.", category: "Deep Learning", difficulty: "EASY" as const, tags: ["vanishing-gradient", "deep-learning", "training"] },
  { questionText: "What are common metrics for evaluating ML models?", idealAnswer: "Classification: accuracy, precision, recall, F1-score, AUC-ROC, log loss. Regression: MSE, RMSE, MAE, R², MAPE. Ranking: NDCG, MAP, MRR. NLP: BLEU (translation), ROUGE (summarization), perplexity (language models). Computer Vision: IoU, mAP (object detection). Choose based on business goals: precision (minimize false positives, e.g., spam), recall (minimize false negatives, e.g., cancer detection). Always compare to a baseline.", category: "Evaluation", difficulty: "EASY" as const, tags: ["metrics", "evaluation", "classification"] },
  { questionText: "What is scikit-learn and when do you use it?", idealAnswer: "Scikit-learn is Python's standard ML library for classical algorithms. Includes: linear/logistic regression, SVM, random forest, gradient boosting, K-means, PCA, preprocessing utilities, model selection tools. Use for: tabular data problems, quick prototyping, when deep learning isn't needed. API: fit(X, y) → predict(X) → score(X, y). Pipelines chain preprocessing and models. Not for: deep learning (use PyTorch/TensorFlow), large-scale data (use Spark ML), GPU acceleration. Excellent documentation and consistent API.", category: "Frameworks", difficulty: "EASY" as const, tags: ["scikit-learn", "classical-ml", "frameworks"] },

  // ===== MEDIUM (40 questions) =====
  { questionText: "How do you design a model training pipeline for production?", idealAnswer: "Pipeline components: (1) Data ingestion: versioned dataset loading from feature store or data warehouse, (2) Preprocessing: reproducible transformations (same pipeline for training and inference), (3) Training: configurable hyperparameters, distributed training support, checkpointing, (4) Evaluation: metrics computation, comparison to baseline and previous best, (5) Validation: data quality checks, model sanity tests, fairness evaluation, (6) Registration: store model artifact with metadata in model registry, (7) Orchestration: Airflow/Kubeflow for scheduling and dependency management. All steps: logged, reproducible, version-controlled, monitored.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["training-pipeline", "mlops", "production"] },
  { questionText: "Explain the transformer architecture and why it's dominant in modern ML.", idealAnswer: "Transformers use self-attention to process entire sequences in parallel. Architecture: input embeddings + positional encoding → multi-head self-attention → feed-forward network → layer normalization (repeated N times). Self-attention: each token attends to all others, capturing long-range dependencies. Multi-head: multiple attention patterns in parallel. Why dominant: (1) Parallelizable (unlike RNNs), (2) Scales well with data and compute, (3) Transfer learning (pretrain once, fine-tune many tasks), (4) Works across modalities (text, images, audio). Foundation for BERT, GPT, ViT, Whisper.", category: "Deep Learning", difficulty: "MEDIUM" as const, tags: ["transformers", "attention", "architecture"] },
  { questionText: "How do you deploy an ML model to production?", idealAnswer: "Deployment options: (1) REST API: FastAPI/Flask wrapping model, containerized with Docker, deployed to Kubernetes/ECS, (2) Managed serving: SageMaker Endpoints, Vertex AI, (3) Batch inference: scheduled job processes data in bulk (Spark, SageMaker Batch Transform), (4) Edge: ONNX/TFLite on mobile/IoT devices, (5) Streaming: model in Kafka consumer for real-time events. Steps: export model → create serving container → deploy with auto-scaling → add health checks → monitor predictions. Always: input validation, model versioning, A/B testing capability, rollback plan.", category: "Deployment", difficulty: "MEDIUM" as const, tags: ["deployment", "serving", "production"] },
  { questionText: "What is distributed training and when is it needed?", idealAnswer: "Distributed training parallelizes model training across multiple GPUs/machines. Types: (1) Data parallelism: same model on each GPU, different data batches, sync gradients (most common — PyTorch DDP), (2) Model parallelism: split model across GPUs when it doesn't fit in one GPU's memory (tensor parallelism, pipeline parallelism), (3) Fully Sharded Data Parallel (FSDP): shards parameters, gradients, and optimizer states across GPUs. Needed when: training takes too long on single GPU, model doesn't fit in memory, large batch sizes needed. Libraries: PyTorch DDP, DeepSpeed, FSDP, Horovod.", category: "Training", difficulty: "MEDIUM" as const, tags: ["distributed-training", "ddp", "scaling"] },
  { questionText: "How do you handle model monitoring in production?", idealAnswer: "Monitor: (1) Data drift: input feature distribution changes (KS test, PSI against training data baseline), (2) Prediction drift: output distribution shifts, (3) Model performance: accuracy on labeled data when available, (4) Latency: inference time P50/P99, (5) Throughput: requests per second, (6) Resource usage: GPU/CPU utilization, memory, (7) Business metrics: impact on downstream KPIs. Tools: Evidently AI, WhyLabs, SageMaker Model Monitor, custom Prometheus metrics. Alert on: drift exceeding threshold, latency spikes, error rate increases. Trigger retraining when performance degrades.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["monitoring", "drift", "production"] },
  { questionText: "What is a feature store and why use one?", idealAnswer: "A feature store centralizes feature computation, storage, and serving for ML. Components: (1) Feature registry: metadata, lineage, documentation, (2) Offline store: batch-computed features in data warehouse for training, (3) Online store: low-latency key-value store (Redis/DynamoDB) for serving, (4) Feature pipelines: batch (Spark) and streaming (Flink) computation. Benefits: feature reuse across models, consistency between training and serving (prevents training-serving skew), point-in-time correct joins for training data. Tools: Feast (open source), Tecton, SageMaker Feature Store.", category: "Infrastructure", difficulty: "MEDIUM" as const, tags: ["feature-store", "feast", "infrastructure"] },
  { questionText: "Explain how to fine-tune a large language model.", idealAnswer: "Steps: (1) Choose base model (LLaMA, Mistral, GPT) based on size and capability, (2) Prepare dataset: instruction-response pairs formatted per model's template, (3) Choose method: full fine-tuning (all weights, expensive) or parameter-efficient (LoRA — adds small trainable adapters, 1-10% of parameters), (4) Configure: learning rate (1e-4 to 1e-5), batch size, epochs (1-3), (5) Train with mixed precision (bfloat16) for memory efficiency, (6) Evaluate: task-specific metrics + human evaluation, (7) Merge LoRA weights for deployment. Tools: Hugging Face Transformers + PEFT, Axolotl, LLaMA Factory. QLoRA: quantized base model + LoRA adapters for training on consumer GPUs.", category: "NLP", difficulty: "MEDIUM" as const, tags: ["fine-tuning", "llm", "lora"] },
  { questionText: "How do you handle class imbalance in training?", idealAnswer: "Techniques: (1) Data level: oversample minority (SMOTE), undersample majority, (2) Loss level: weighted cross-entropy (higher weight for minority class), focal loss (down-weights easy examples), (3) Architecture: ensemble of models trained on balanced subsets, (4) Threshold: adjust classification threshold from 0.5 based on precision-recall curve, (5) Augmentation: augment minority class samples, (6) Metrics: use F1, precision-recall AUC (not accuracy). Choice depends on: degree of imbalance, data availability, task requirements. For extreme imbalance (>100:1): anomaly detection approaches may be better.", category: "Training", difficulty: "MEDIUM" as const, tags: ["class-imbalance", "training", "focal-loss"] },
  { questionText: "What is model quantization and why is it useful?", idealAnswer: "Quantization reduces model precision from 32-bit floating point to lower bit-widths (16-bit, 8-bit, 4-bit). Types: (1) Post-training quantization (PTQ): quantize after training, may lose some accuracy, (2) Quantization-aware training (QAT): simulate quantization during training for better accuracy. INT8 inference is ~2-4x faster with ~1% accuracy loss. 4-bit quantization (GPTQ, AWQ) enables running large LLMs on consumer GPUs. Use for: reducing model size, faster inference, lower memory usage, edge deployment. Tools: PyTorch quantization, TensorRT, bitsandbytes, llama.cpp.", category: "Optimization", difficulty: "MEDIUM" as const, tags: ["quantization", "optimization", "inference"] },
  { questionText: "How do you design an experiment tracking system?", idealAnswer: "Track per experiment: (1) Code version (git commit hash), (2) Dataset version (DVC hash or timestamp), (3) Hyperparameters (all configurable values), (4) Metrics (loss curves, evaluation metrics), (5) Artifacts (model checkpoints, plots), (6) Environment (Python version, library versions, hardware), (7) Random seeds. Tools: MLflow (open source, self-hosted), Weights & Biases (cloud, rich visualizations), Neptune, CometML. Best practices: log automatically (autolog), tag experiments meaningfully, compare runs visually, link to model registry for promotion. Never rely on memory or file naming for tracking.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["experiment-tracking", "mlflow", "wandb"] },
  { questionText: "What is mixed precision training?", idealAnswer: "Mixed precision uses both FP16 (half precision) and FP32 (full precision) during training. Forward pass and gradient computation in FP16 (faster, less memory), weight updates in FP32 (maintain precision). Loss scaling prevents gradient underflow in FP16. Benefits: ~2x training speedup, ~50% memory reduction, enabling larger batch sizes. Supported on NVIDIA Volta+ GPUs (Tensor Cores). PyTorch: `torch.cuda.amp.autocast()` + `GradScaler`. BFloat16 (BF16) is preferred for training when available — same range as FP32 with FP16 memory savings.", category: "Training", difficulty: "MEDIUM" as const, tags: ["mixed-precision", "fp16", "training"] },
  { questionText: "How do you build a data versioning system for ML?", idealAnswer: "Tools and approaches: (1) DVC (Data Version Control): Git-like for data — stores metadata in Git, data in S3/GCS. Commands: `dvc add`, `dvc push`, `dvc pull`, (2) Delta Lake/Iceberg: versioned data lake tables with time travel, (3) Feature store: versioned feature sets with timestamps, (4) Dataset registries: metadata catalog linking datasets to models. Best practices: hash-based versioning for integrity, link datasets to experiments, point-in-time snapshots for reproducibility, store preprocessing code alongside data. Never store large datasets in Git. Every model should reference its exact training data version.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["data-versioning", "dvc", "reproducibility"] },
  { questionText: "Explain model distillation and its applications.", idealAnswer: "Knowledge distillation trains a smaller 'student' model to mimic a larger 'teacher' model. Process: (1) Train large teacher model, (2) Generate soft labels (probability distributions) from teacher on training data, (3) Train student on combination of hard labels and soft labels (with temperature scaling). Soft labels contain 'dark knowledge' — relationships between classes. Benefits: smaller, faster models for deployment with minimal accuracy loss. Applications: compressing BERT to DistilBERT (40% smaller, 60% faster, 97% performance), deploying to edge devices. Also used for: ensembles → single model.", category: "Optimization", difficulty: "MEDIUM" as const, tags: ["distillation", "compression", "optimization"] },
  { questionText: "What is A/B testing for ML models?", idealAnswer: "A/B testing compares a new model (treatment) against the current model (control) in production. Setup: (1) Define primary metric (click-through rate, conversion), (2) Calculate required sample size for statistical power, (3) Random traffic split (e.g., 90/10 control/treatment), (4) Run for predetermined duration, (5) Analyze: statistical significance, practical significance, guardrail metrics. Considerations: user-level randomization (not request-level), novelty effects, segment analysis. Alternatives: multi-armed bandits (explore/exploit), interleaving (for ranking models). Always monitor guardrail metrics to detect degradation.", category: "Deployment", difficulty: "MEDIUM" as const, tags: ["ab-testing", "deployment", "evaluation"] },
  { questionText: "How do you handle training-serving skew?", idealAnswer: "Training-serving skew occurs when the feature computation differs between training and inference, causing degraded performance. Causes: (1) Different preprocessing code (Python notebook vs production Java), (2) Stale features in serving, (3) Data leakage in training but not serving. Solutions: (1) Feature store: same feature computation for both, (2) Shared preprocessing pipeline (Scikit-learn Pipeline, PyTorch transforms, saved with model), (3) Integration tests comparing training and serving outputs, (4) Monitor feature distributions in production vs training. This is one of the most common and insidious ML production issues.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["training-serving-skew", "feature-store", "production"] },
  { questionText: "What is ONNX and how does it help with model deployment?", idealAnswer: "ONNX (Open Neural Network Exchange) is a framework-agnostic model format. Convert models from PyTorch, TensorFlow, scikit-learn to ONNX, then run with ONNX Runtime for optimized inference. Benefits: (1) Framework portability (train in PyTorch, serve with ONNX Runtime), (2) Optimized inference (graph optimization, hardware-specific acceleration), (3) Language flexibility (serve Python-trained models in C++/Java), (4) Edge deployment. Export: `torch.onnx.export()`. ONNX Runtime supports CPU, GPU, TensorRT, DirectML. Typically 2-5x faster inference than native framework.", category: "Deployment", difficulty: "MEDIUM" as const, tags: ["onnx", "deployment", "optimization"] },
  { questionText: "How do you implement model retraining pipelines?", idealAnswer: "Triggers: (1) Scheduled (daily/weekly), (2) Data drift detected, (3) Performance degradation, (4) New data threshold reached. Pipeline: (1) Fetch latest data, (2) Feature computation, (3) Data validation (Great Expectations), (4) Train new model, (5) Evaluate against current production model on holdout + recent data, (6) Automated promotion if metrics improve, (7) Canary deployment, (8) Monitor and rollback if needed. Tools: Kubeflow Pipelines, Airflow, SageMaker Pipelines. Store training artifacts in model registry. Always compare to current production model, not just a baseline.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["retraining", "pipeline", "mlops"] },
  { questionText: "Explain batch normalization and layer normalization.", idealAnswer: "Both normalize activations to stabilize and speed up training. Batch normalization: normalizes across the batch dimension — computes mean and variance per feature across all samples in a batch. Requires sufficiently large batches, stores running statistics for inference. Layer normalization: normalizes across the feature dimension per sample — independent of batch size. BatchNorm: standard for CNNs. LayerNorm: standard for transformers and RNNs (batch-independent). GroupNorm: compromise for small batches. Normalization reduces internal covariate shift, enabling higher learning rates.", category: "Deep Learning", difficulty: "MEDIUM" as const, tags: ["batch-norm", "layer-norm", "normalization"] },
  { questionText: "How do you handle large datasets that don't fit in memory?", idealAnswer: "Techniques: (1) Data loaders: PyTorch DataLoader with num_workers for parallel loading, reads batches on-the-fly, (2) Memory-mapped files: numpy memmap, HDF5, access data without loading fully, (3) Streaming datasets: Hugging Face datasets streaming mode, (4) Distributed data: shard across machines (WebDataset format), (5) Data format: use columnar formats (Parquet, Arrow) for efficient reads, (6) Feature store: compute and serve features from optimized storage, (7) Cloud: stream directly from S3/GCS. For training: only the current batch needs to be in GPU memory. Prefetch next batch while current is training.", category: "Data", difficulty: "MEDIUM" as const, tags: ["large-data", "dataloader", "streaming"] },
  { questionText: "What is RAG (Retrieval-Augmented Generation)?", idealAnswer: "RAG combines information retrieval with language generation. Architecture: (1) Offline: chunk documents, embed with model (e.g., sentence-transformers), store in vector database (Pinecone, Weaviate, Chroma), (2) Online: embed user query, retrieve top-k similar chunks, (3) Augment: inject retrieved context into LLM prompt, (4) Generate: LLM produces answer grounded in retrieved context. Benefits: reduces hallucination, enables domain-specific knowledge without fine-tuning, updatable knowledge (just update the index). Key decisions: chunk size, embedding model, retrieval strategy (semantic + keyword hybrid), reranking.", category: "NLP", difficulty: "MEDIUM" as const, tags: ["rag", "llm", "retrieval"] },
  { questionText: "How do you optimize inference latency for real-time serving?", idealAnswer: "Techniques: (1) Model optimization: quantization (INT8/INT4), pruning, distillation, (2) Runtime: ONNX Runtime, TensorRT, vLLM (for LLMs), (3) Batching: dynamic batching to improve GPU utilization, (4) Caching: cache predictions for repeated inputs, (5) Hardware: GPU inference (faster than CPU for large models), right-sized instances, (6) Architecture: smaller models, early exit, (7) Preprocessing: move heavy preprocessing to async/pre-computation, (8) Infrastructure: deploy close to users, use dedicated inference nodes. Profile first (PyTorch Profiler, NVIDIA Nsight) — optimize the bottleneck. Target: P99 latency, not just average.", category: "Optimization", difficulty: "MEDIUM" as const, tags: ["inference", "optimization", "latency"] },
  { questionText: "What is continual/online learning?", idealAnswer: "Continual learning updates models with new data without retraining from scratch. Approaches: (1) Fine-tune on new data periodically, (2) Online learning: update weights with each new sample (SGD), (3) Incremental: update with batches of new data. Challenges: catastrophic forgetting (new data overwrites old knowledge). Solutions: elastic weight consolidation (EWC), experience replay (mix old and new data), progressive neural networks. Use cases: recommendation systems (evolving preferences), fraud detection (changing patterns). Most production systems use scheduled retraining rather than true online learning.", category: "Training", difficulty: "MEDIUM" as const, tags: ["online-learning", "continual", "adaptation"] },
  { questionText: "How do you build reproducible ML experiments?", idealAnswer: "Reproducibility checklist: (1) Fix random seeds (Python, NumPy, PyTorch, CUDA), (2) Version code (Git commit hash), (3) Version data (DVC, dataset hash), (4) Log all hyperparameters (MLflow/W&B), (5) Pin dependency versions (requirements.txt with exact versions), (6) Containerize environment (Docker), (7) Log hardware info (GPU type, CUDA version), (8) Deterministic operations (set CUBLAS_WORKSPACE_CONFIG for CUDA), (9) Store model checkpoints. Note: some GPU operations are non-deterministic by default. Trade-off: full determinism can reduce training speed. Document any known sources of non-determinism.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["reproducibility", "experiments", "best-practices"] },
  { questionText: "Explain the concept of attention mechanism in detail.", idealAnswer: "Attention computes weighted relevance between elements. Self-attention: Query (Q), Key (K), Value (V) are linear projections of the same input. Attention(Q,K,V) = softmax(QK^T / √d_k) × V. The scaling factor √d_k prevents softmax saturation. Multi-head attention: run h parallel attention operations with different projections, concatenate results. Cross-attention: Q from one sequence, K/V from another (used in encoder-decoder for translation). Complexity: O(n²) for sequence length n. Efficient variants: flash attention (memory-efficient), sparse attention, sliding window.", category: "Deep Learning", difficulty: "MEDIUM" as const, tags: ["attention", "transformers", "self-attention"] },
  { questionText: "How do you handle model fairness and bias?", idealAnswer: "Process: (1) Identify protected attributes (race, gender, age), (2) Measure bias: demographic parity (equal positive rates), equalized odds (equal TPR and FPR across groups), (3) Pre-processing: resampling, reweighting training data, (4) In-processing: adversarial debiasing, fairness constraints in loss function, (5) Post-processing: adjust thresholds per group, (6) Evaluate: disaggregated metrics per subgroup. Tools: Fairlearn, AI Fairness 360, What-If Tool. Document: model card with intended use, limitations, fairness metrics. Fairness definition depends on context — there's no universal metric. Regular auditing is essential.", category: "Ethics", difficulty: "MEDIUM" as const, tags: ["fairness", "bias", "responsible-ai"] },
  { questionText: "What is model pruning?", idealAnswer: "Pruning removes unnecessary weights or neurons to create smaller, faster models. Types: (1) Unstructured: zero out individual weights below a threshold (sparse matrix, needs special hardware for speedup), (2) Structured: remove entire neurons, channels, or attention heads (directly reduces computation), (3) Magnitude pruning: remove smallest weights, (4) Movement pruning: prune weights that move toward zero during fine-tuning. Process: train → prune → fine-tune (iteratively). Can remove 50-90% of parameters with minimal accuracy loss. Combined with quantization and distillation for maximum compression.", category: "Optimization", difficulty: "MEDIUM" as const, tags: ["pruning", "compression", "optimization"] },
  { questionText: "How do you implement CI/CD for ML models?", idealAnswer: "ML CI/CD extends traditional CI/CD with data and model validation. CI: (1) Code linting and unit tests, (2) Data validation tests (schema, statistics), (3) Model training on small data subset (smoke test), (4) Model quality gates (accuracy > threshold). CD: (1) Train on full data, (2) Evaluate against production model, (3) Register in model registry, (4) Automated deployment to staging, (5) Integration tests with real serving infrastructure, (6) Canary/shadow deployment to production. Tools: GitHub Actions + MLflow, Kubeflow Pipelines, SageMaker Pipelines. Trigger on: code changes, data updates, scheduled retraining.", category: "MLOps", difficulty: "MEDIUM" as const, tags: ["ci-cd", "mlops", "automation"] },
  { questionText: "What is a vector database and when do you use it?", idealAnswer: "Vector databases store and search high-dimensional embeddings efficiently using approximate nearest neighbor (ANN) algorithms. Algorithms: HNSW (hierarchical navigable small world graphs), IVF (inverted file index), product quantization. Tools: Pinecone (managed), Weaviate, Milvus, Qdrant, Chroma (lightweight), pgvector (PostgreSQL extension). Use for: semantic search, RAG systems, recommendation engines, image similarity, deduplication. Key considerations: index build time, query latency, recall accuracy, filtering support, scalability. Hybrid search combines vector similarity with keyword/metadata filtering.", category: "Infrastructure", difficulty: "MEDIUM" as const, tags: ["vector-database", "embeddings", "search"] },
  { questionText: "How do you evaluate a language model?", idealAnswer: "Automatic metrics: (1) Perplexity: how well the model predicts next tokens (lower = better), (2) BLEU/ROUGE: n-gram overlap for generation quality, (3) Task-specific benchmarks: MMLU (knowledge), HumanEval (coding), GSM8K (math), (4) Embedding similarity: cosine similarity between generated and reference. Human evaluation: (5) Helpfulness ratings, (6) Factuality checks, (7) Pairwise comparisons (ELO scoring), (8) Red-teaming for safety. Best practice: combine automatic metrics (fast, scalable) with human evaluation (nuanced). For instruction-following: use LLM-as-judge (GPT-4 evaluating outputs).", category: "Evaluation", difficulty: "MEDIUM" as const, tags: ["evaluation", "llm", "benchmarks"] },
  { questionText: "Explain the concept of model explainability for production systems.", idealAnswer: "Methods: (1) SHAP values: game-theoretic feature attribution per prediction (gold standard), (2) LIME: local surrogate models for individual predictions, (3) Attention visualization: which input tokens the model attends to, (4) Feature importance: permutation or tree-based, (5) Grad-CAM: visual explanations for CNNs, (6) Counterfactual explanations: what minimal change would change the prediction. Production implementation: compute SHAP in batch, store with predictions, expose via API. Important for: regulatory compliance (lending, healthcare), debugging, user trust. Trade-off: computation cost of explanations at inference time.", category: "Deployment", difficulty: "MEDIUM" as const, tags: ["explainability", "shap", "production"] },

  // ===== HARD (30 questions) =====
  { questionText: "Design an end-to-end ML platform for a company with multiple ML teams.", idealAnswer: "Components: (1) Data platform: data lake (S3 + Delta Lake), feature store (Feast/Tecton) with offline/online serving, (2) Experimentation: shared compute cluster (Kubernetes + GPU nodes), experiment tracking (MLflow/W&B), notebook environment (JupyterHub), (3) Training: distributed training infrastructure (PyTorch DDP on Kubernetes), hyperparameter optimization (Optuna), (4) Model registry: versioned models with metadata and approval workflow, (5) Serving: unified serving layer (Seldon/KServe) with A/B testing, auto-scaling, GPU inference, (6) Monitoring: data drift, model performance, resource utilization dashboards, (7) CI/CD: automated pipelines for training and deployment, (8) Governance: model cards, access control, audit trail. Self-service: teams should deploy independently with platform guardrails.", category: "Architecture", difficulty: "HARD" as const, tags: ["ml-platform", "architecture", "infrastructure"] },
  { questionText: "How would you scale model training to handle a 1TB dataset with a model that doesn't fit in a single GPU?", idealAnswer: "Strategy: (1) Data: shard across workers, use efficient formats (WebDataset, Parquet), streaming data loaders, (2) Model parallelism: tensor parallelism (split layers across GPUs — Megatron-LM), pipeline parallelism (split layers sequentially across GPUs), (3) Data parallelism: FSDP (Fully Sharded Data Parallel) — shards parameters, gradients, optimizer states across all GPUs, (4) Mixed precision: BF16 training reduces memory by 50%, (5) Gradient checkpointing: recompute activations during backward pass instead of storing (trades compute for memory), (6) Activation offloading: move activations to CPU, (7) ZeRO optimization (DeepSpeed): stages 1-3 for progressive memory reduction. Infrastructure: multi-node cluster with fast interconnect (NVLink, InfiniBand).", category: "Training", difficulty: "HARD" as const, tags: ["distributed-training", "scaling", "large-models"] },
  { questionText: "Explain how to implement a real-time ML serving system with sub-10ms latency.", idealAnswer: "Architecture: (1) Model optimization: quantization (INT8/TensorRT), ONNX Runtime, pruned model, (2) Pre-computation: cache frequent predictions, precompute user embeddings, (3) Feature serving: Redis for online features with sub-ms lookups, avoid feature computation in hot path, (4) Serving infrastructure: dedicated GPU inference nodes, model loaded in GPU memory, (5) Batching: micro-batching incoming requests (1-5ms window) for GPU efficiency, (6) Networking: gRPC (faster than REST), deploy in same region as clients, (7) Warm-up: pre-load models, warm inference cache, (8) Profiling: identify bottleneck (feature fetch vs inference vs serialization). Monitor P99 latency, not just average. Load test to verify under production traffic patterns.", category: "Deployment", difficulty: "HARD" as const, tags: ["real-time", "serving", "low-latency"] },
  { questionText: "How would you build a production RAG system that handles millions of documents?", idealAnswer: "Architecture: (1) Ingestion: async pipeline — chunk documents (500-1000 tokens, with overlap), extract metadata, (2) Embedding: batch embed with efficient model (e5-large, BGE), GPU-accelerated, (3) Vector store: distributed vector DB (Milvus, Qdrant cluster) with metadata filtering, (4) Retrieval: hybrid search (dense + sparse/BM25), reranking with cross-encoder, (5) Generation: LLM with retrieved context, streaming responses, (6) Caching: cache frequent queries + results, (7) Freshness: incremental indexing for new/updated documents, (8) Evaluation: retrieval quality (MRR, recall@k), generation quality (faithfulness, relevance), (9) Guardrails: hallucination detection, source citation. Scale: shard vectors by collection, horizontal scaling for serving.", category: "NLP", difficulty: "HARD" as const, tags: ["rag", "production", "search"] },
  { questionText: "Design a model monitoring and alerting system for detecting data and concept drift.", idealAnswer: "Architecture: (1) Data drift detection: compute feature statistics per time window, compare to reference (training data) using KS test (continuous), chi-squared (categorical), PSI (Population Stability Index), (2) Prediction drift: monitor output distribution changes with Jensen-Shannon divergence, (3) Concept drift: track performance on labeled data when available, proxy metrics when not, (4) Implementation: log all predictions + features to data warehouse, batch analysis job computes drift metrics, (5) Alerting: threshold-based (PSI > 0.2 = significant drift), trend-based (gradual degradation), (6) Dashboard: feature-level drift heatmap, prediction distribution over time, (7) Response: automated retraining trigger, manual investigation workflow. Tools: Evidently AI, WhyLabs, custom Prometheus metrics.", category: "MLOps", difficulty: "HARD" as const, tags: ["monitoring", "drift-detection", "production"] },
  { questionText: "Explain how to train and deploy a multi-modal model combining text and images.", idealAnswer: "Architecture: (1) Text encoder: pretrained BERT/sentence-transformer, (2) Image encoder: pretrained ViT/CLIP visual encoder, (3) Fusion: (a) Early fusion — concatenate embeddings, feed through MLP, (b) Cross-attention — text attends to image and vice versa, (c) CLIP-style contrastive learning — align text and image in shared space, (4) Training: multi-task loss combining modality-specific and cross-modal objectives, (5) Handling missing modalities: learned default embeddings, masking during training, (6) Deployment: separate encoding (cache embeddings) or joint model depending on latency requirements. Pre-compute image embeddings offline, text encoding can be real-time. Tools: Hugging Face Transformers, timm for vision.", category: "Deep Learning", difficulty: "HARD" as const, tags: ["multi-modal", "clip", "architecture"] },
  { questionText: "How do you implement efficient LLM serving for production?", idealAnswer: "Optimizations: (1) KV-cache: store key-value pairs from previous tokens to avoid recomputation, (2) Continuous batching (vLLM): dynamically batch requests at different generation stages, (3) PagedAttention: efficient GPU memory management for KV-cache, (4) Quantization: AWQ/GPTQ for 4-bit inference, (5) Tensor parallelism: split model across GPUs for large models, (6) Speculative decoding: use small model to draft tokens, large model to verify (faster generation), (7) Streaming: return tokens as generated, (8) Request scheduling: priority queues, preemption for SLA management. Frameworks: vLLM (best throughput), TGI (Hugging Face), TensorRT-LLM (NVIDIA). Metrics: time-to-first-token, tokens-per-second, concurrent request capacity.", category: "Deployment", difficulty: "HARD" as const, tags: ["llm-serving", "vllm", "optimization"] },
  { questionText: "Design a feature engineering pipeline that works consistently for training and serving.", idealAnswer: "Architecture: (1) Feature definitions as code: declarative feature specs (SQL, Python functions) stored in Git, (2) Batch pipeline: Spark/dbt computes features for training, stores in offline feature store, (3) Real-time pipeline: Flink/Kafka Streams computes streaming features, writes to online store (Redis), (4) Point-in-time joins: feature store handles temporal correctness — no future data leakage, (5) Serving: model requests features from online store by entity key, same transformation logic applied, (6) Validation: automated tests compare offline/online feature values, (7) Monitoring: feature freshness, null rates, distribution shifts. Key: one definition, two execution paths (batch + real-time), validated consistency.", category: "Infrastructure", difficulty: "HARD" as const, tags: ["feature-engineering", "pipeline", "consistency"] },
  { questionText: "How would you implement a reinforcement learning from human feedback (RLHF) pipeline?", idealAnswer: "Pipeline: (1) Supervised fine-tuning (SFT): fine-tune base LLM on high-quality instruction-following data, (2) Reward model training: collect human preference data (choose better of two responses), train reward model to predict human preference, (3) RL training: optimize SFT model using PPO (Proximal Policy Optimization) to maximize reward while staying close to SFT model (KL divergence penalty), (4) Evaluation: human evaluation, automated benchmarks, safety tests. Alternatives: DPO (Direct Preference Optimization) — skips reward model, directly optimizes from preferences (simpler, often competitive). Infrastructure: requires significant GPU compute (multiple model copies in memory for PPO). Tools: TRL (Hugging Face), DeepSpeed-Chat.", category: "NLP", difficulty: "HARD" as const, tags: ["rlhf", "alignment", "llm"] },
  { questionText: "How do you design an ML system that handles adversarial attacks?", idealAnswer: "Defenses: (1) Input validation: detect anomalous inputs (out-of-distribution detection, input sanitization), (2) Adversarial training: include adversarial examples in training data (PGD attack + train), (3) Certified robustness: randomized smoothing provides provable guarantees within radius, (4) Ensemble defenses: multiple models vote, harder to attack all simultaneously, (5) Input preprocessing: JPEG compression, spatial smoothing to remove perturbations, (6) Monitoring: detect adversarial patterns in production (unusual confidence distributions, input anomalies), (7) Rate limiting: prevent automated attacks. For LLMs: prompt injection defense with input/output filtering, instruction hierarchy. Defense in depth: no single defense is sufficient.", category: "Security", difficulty: "HARD" as const, tags: ["adversarial", "robustness", "security"] },
  { questionText: "Explain how to build a model that handles concept drift automatically.", idealAnswer: "System: (1) Detection: statistical tests on feature distributions (ADWIN, Page-Hinkley) and model performance metrics, (2) Adaptation strategies: (a) Periodic retraining with sliding window of recent data, (b) Ensemble of models from different time periods with weighted voting based on recent performance, (c) Online learning with elastic weight consolidation to prevent forgetting, (3) Implementation: continuous monitoring pipeline → drift detector → triggers retraining pipeline → evaluation → automatic promotion if improved, (4) Fallback: rule-based model as fallback during retraining, (5) Types: gradual drift (sliding window works), sudden drift (detect and retrain quickly), recurring drift (maintain model bank). Always maintain the ability to quickly retrain and deploy.", category: "MLOps", difficulty: "HARD" as const, tags: ["concept-drift", "adaptation", "production"] },
  { questionText: "Design a cost-efficient GPU training infrastructure.", idealAnswer: "Infrastructure: (1) Spot/preemptible instances: 60-90% cheaper, handle interruptions with checkpointing (save every N steps), (2) Right-sizing: match GPU type to workload (T4 for inference/small training, A10G for medium, A100 for large), (3) Multi-GPU efficiency: maximize GPU utilization with data parallel training, dynamic batching, (4) Scheduling: Kubernetes with GPU operator, priority queues, preemption for high-priority jobs, fair-share scheduling between teams, (5) Storage: fast shared storage (FSx for Lustre, EFS) for data loading, (6) Monitoring: GPU utilization dashboards (DCGM), alert on underutilized GPUs, (7) Auto-scaling: scale GPU node pool based on queue depth. Cost tracking: per-experiment GPU hours. Target: >80% GPU utilization during training.", category: "Infrastructure", difficulty: "HARD" as const, tags: ["gpu", "cost-optimization", "infrastructure"] },
  { questionText: "How would you implement a model serving system that supports multiple model versions and A/B testing?", idealAnswer: "Architecture: (1) Model registry: MLflow storing versioned models with metadata (metrics, data version, stage), (2) Serving layer: KServe/Seldon on Kubernetes — each model version as a separate deployment, (3) Traffic routing: Istio virtual services for percentage-based splitting (90% v1, 10% v2), (4) A/B testing: assign users to model versions consistently (user ID hash), log predictions with version tag, (5) Analysis: compare metrics between versions with statistical significance tests, (6) Promotion: automated if metrics improve, rollback if guardrails breached, (7) Shadow mode: new model receives traffic but predictions aren't served to users (safe testing), (8) Canary: gradual traffic increase with automated analysis. Support: multiple models simultaneously, instant rollback, zero-downtime updates.", category: "Deployment", difficulty: "HARD" as const, tags: ["serving", "ab-testing", "versioning"] },
  { questionText: "Explain how to implement efficient attention for very long sequences.", idealAnswer: "Approaches: (1) Flash Attention: IO-aware exact attention, reduces memory from O(n²) to O(n) by tiling and kernel fusion — now standard in PyTorch, (2) Sliding window attention: each token attends to local window only (Mistral, Longformer), O(n×w), (3) Sparse attention: attend to fixed patterns (strided, local + global tokens), (4) Linear attention: approximate softmax attention with kernel trick, O(n) but less accurate, (5) Ring attention: distribute sequence across GPUs, each processes local chunk with communication, (6) KV-cache compression: quantize cached keys/values, evict less important entries. For LLMs: Flash Attention + sliding window + KV-cache quantization enables million-token contexts. RoPE (Rotary Position Embeddings) with NTK scaling extends position encoding beyond training length.", category: "Deep Learning", difficulty: "HARD" as const, tags: ["attention", "long-context", "efficiency"] },
  { questionText: "Design an end-to-end computer vision pipeline for production.", idealAnswer: "Pipeline: (1) Data: collection, labeling (Label Studio, Scale AI), versioning (DVC), augmentation pipeline, (2) Training: pretrained backbone (ResNet, EfficientNet, ViT), task head (classification/detection/segmentation), distributed training, hyperparameter optimization, (3) Optimization: quantization (INT8), TensorRT compilation, ONNX export, (4) Serving: preprocessing (resize, normalize) → model inference → postprocessing (NMS for detection), batch inference support, (5) Monitoring: input image distribution, prediction confidence distribution, (6) Edge deployment: TFLite, CoreML, ONNX Mobile. Specific considerations: image size vs speed tradeoff, data augmentation strategy (Albumentations), handling class imbalance, evaluation metrics (mAP for detection, IoU for segmentation).", category: "Computer Vision", difficulty: "HARD" as const, tags: ["computer-vision", "pipeline", "production"] },
  { questionText: "How do you implement responsible AI practices in ML engineering?", idealAnswer: "Framework: (1) Data: audit training data for biases, document data sources and collection process, check representation of demographic groups, (2) Model: fairness metrics per subgroup (equalized odds, demographic parity), bias mitigation techniques, (3) Evaluation: disaggregated evaluation — report metrics per demographic/segment, (4) Documentation: model cards (purpose, limitations, performance per group, ethical considerations), datasheets for datasets, (5) Deployment: human-in-the-loop for high-stakes decisions, confidence thresholds, opt-out mechanisms, (6) Monitoring: track fairness metrics in production, detect bias amplification over time, (7) Process: ethics review board for high-risk applications, red-teaming, regular audits. Tools: Fairlearn, AI Fairness 360, Google What-If Tool.", category: "Ethics", difficulty: "HARD" as const, tags: ["responsible-ai", "fairness", "ethics"] },
  { questionText: "How would you build a recommendation system that serves millions of users in real-time?", idealAnswer: "Architecture: two-stage system — (1) Candidate generation (fast, broad): use lightweight embedding models, retrieve top-1000 candidates via ANN search (FAISS/ScaNN) from item embeddings, multiple retrieval channels (collaborative filtering, content-based, popular items), (2) Ranking (accurate, focused): neural ranking model scores 100-1000 candidates with rich features (user history, item features, context), (3) Re-ranking: apply business rules (diversity, freshness, promotions), (4) Serving: precomputed user embeddings updated periodically, item embeddings in vector store, ranking model on GPU, (5) Real-time signals: recent interactions update candidate scoring via feature store, (6) Evaluation: offline (NDCG, recall@k) + online (A/B test on engagement/revenue). Cache heavy: user profiles, item features, frequent recommendation lists.", category: "Architecture", difficulty: "HARD" as const, tags: ["recommendations", "real-time", "architecture"] },
  { questionText: "Explain how to implement model compression for edge deployment.", idealAnswer: "Compression pipeline: (1) Pruning: structured (remove channels/heads) for hardware-friendly sparsity, iterative magnitude pruning with fine-tuning, target 50-80% sparsity, (2) Quantization: post-training INT8 for minimal accuracy loss, QAT for aggressive quantization (INT4), (3) Distillation: train compact student model mimicking pruned/quantized teacher, (4) Architecture: use efficient architectures (MobileNet, EfficientNet-Lite), neural architecture search for target hardware, (5) Hardware-aware: optimize for specific target (TFLite for mobile, CoreML for iOS, TensorRT for NVIDIA edge), (6) Benchmark: measure latency/throughput/memory on actual target hardware. Combined: prune → distill → quantize can achieve 10-50x compression with <2% accuracy loss. Always validate on edge device, not just desktop.", category: "Optimization", difficulty: "HARD" as const, tags: ["edge-deployment", "compression", "mobile"] },
  { questionText: "Design a system for automated machine learning (AutoML).", idealAnswer: "Components: (1) Data preprocessing: automated feature type detection, missing value handling, encoding selection, (2) Feature engineering: automated feature generation (interactions, aggregations, embeddings), feature selection, (3) Model selection: search space of algorithms (linear, tree-based, neural), (4) Hyperparameter optimization: Bayesian optimization (Optuna) with early stopping, (5) Architecture search: for neural networks, search over layer types, depths, widths (NAS), (6) Ensemble: stack top-N models for best performance, (7) Pipeline optimization: optimize full preprocessing + model pipeline jointly. Key design decisions: search budget (time/compute limit), evaluation strategy (cross-validation), meta-learning (warm-start from similar datasets). Tools: AutoGluon (tabular), Auto-sklearn, FLAML. AutoML complements, not replaces, ML engineers.", category: "Architecture", difficulty: "HARD" as const, tags: ["automl", "nas", "automation"] },
  { questionText: "How do you handle ML model governance and compliance in regulated industries?", idealAnswer: "Framework: (1) Model inventory: catalog all models with risk classification, owners, business purpose, (2) Documentation: model cards, validation reports, fairness assessments — required before deployment, (3) Approval workflow: model risk committee review for high-risk models, (4) Validation: independent model validation team tests before production, (5) Monitoring: continuous performance and fairness monitoring with documented escalation procedures, (6) Audit trail: all training runs, data versions, model versions, deployment decisions logged immutably, (7) Access control: role-based access to models, data, deployment actions, (8) Regulatory: SR 11-7 (banking), FDA (medical devices), EU AI Act classification. Automate as much as possible with model registry + CI/CD + monitoring pipeline. Regular reviews and revalidation.", category: "Governance", difficulty: "HARD" as const, tags: ["governance", "compliance", "regulation"] },
  { questionText: "How would you build a self-improving ML system with human-in-the-loop?", idealAnswer: "Architecture: (1) Active learning: model identifies uncertain predictions, routes to human annotators for labeling, (2) Feedback loop: collect user feedback (explicit ratings, implicit signals like clicks), (3) Data flywheel: predictions → corrections → retrain → better predictions, (4) Quality control: inter-annotator agreement, annotation guidelines, review pipeline, (5) Scheduling: batch retrain on accumulated labels, evaluate improvement, (6) Prioritization: focus labeling on high-value, uncertain examples (uncertainty sampling, diversity sampling), (7) Interface: labeling tool integrated with model predictions (pre-labels for efficiency), (8) Monitoring: track labeling velocity, model improvement per annotation batch, diminishing returns threshold. Balance: automation (scale) with human oversight (quality). Target: reduce human effort over time as model improves.", category: "Architecture", difficulty: "HARD" as const, tags: ["active-learning", "human-in-the-loop", "data-flywheel"] },
];
