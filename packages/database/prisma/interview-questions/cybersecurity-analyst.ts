export const cybersecurityAnalystQuestions = [
  // ===== NETWORK SECURITY (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What is a firewall and what role does it play in network security?",
    idealAnswer: "A firewall is a network security device or software that monitors and filters incoming and outgoing network traffic based on predefined security rules. It acts as a barrier between a trusted internal network and untrusted external networks such as the internet. Firewalls can be hardware-based, software-based, or a combination of both, and they enforce access control policies by allowing or blocking traffic based on IP addresses, ports, and protocols. They are a foundational layer of defense in any network security architecture.",
    category: "Network Security",
    difficulty: "EASY" as const,
    tags: ["firewall", "network security", "access control", "perimeter defense"],
  },
  {
    questionText: "What is the difference between an IDS and an IPS?",
    idealAnswer: "An Intrusion Detection System (IDS) passively monitors network traffic and alerts administrators when suspicious activity is detected, but it does not take action to block the traffic. An Intrusion Prevention System (IPS), on the other hand, sits inline with network traffic and can actively block or reject malicious packets in real time. IDS is useful for visibility and forensic analysis, while IPS provides active defense but must be carefully tuned to avoid blocking legitimate traffic. Many modern solutions combine both capabilities into a single platform.",
    category: "Network Security",
    difficulty: "EASY" as const,
    tags: ["IDS", "IPS", "intrusion detection", "network monitoring"],
  },
  {
    questionText: "Explain what a VPN is and why organizations use them.",
    idealAnswer: "A Virtual Private Network (VPN) creates an encrypted tunnel between a user's device and a remote server or network, ensuring that data transmitted over public or untrusted networks remains confidential and tamper-proof. Organizations use VPNs to allow remote employees to securely access internal resources as if they were on the corporate LAN. VPNs rely on protocols such as IPSec, OpenVPN, or WireGuard to provide encryption, authentication, and data integrity. They are a critical tool for securing remote work and protecting sensitive communications from eavesdropping.",
    category: "Network Security",
    difficulty: "EASY" as const,
    tags: ["VPN", "encryption", "remote access", "tunneling"],
  },
  {
    questionText: "How does a DNS poisoning attack work, and what are effective countermeasures?",
    idealAnswer: "DNS poisoning, also known as DNS spoofing, involves corrupting the DNS cache of a resolver so that it returns an incorrect IP address for a domain, redirecting users to malicious websites. Attackers can accomplish this by sending forged DNS responses that match the transaction ID of a legitimate query before the real response arrives. Effective countermeasures include implementing DNSSEC to cryptographically verify DNS responses, using DNS over HTTPS (DoH) or DNS over TLS (DoT) to encrypt queries, randomizing source ports and transaction IDs, and regularly flushing DNS caches. Network segmentation and monitoring for anomalous DNS traffic patterns also help detect and prevent such attacks.",
    category: "Network Security",
    difficulty: "MEDIUM" as const,
    tags: ["DNS", "spoofing", "DNSSEC", "cache poisoning"],
  },
  {
    questionText: "What is network segmentation and why is it important for security?",
    idealAnswer: "Network segmentation involves dividing a larger network into smaller, isolated sub-networks or segments, each with its own security controls and access policies. This limits lateral movement by attackers who have compromised one segment, preventing them from easily accessing other parts of the network. Segmentation is typically implemented using VLANs, firewalls, and access control lists, and it follows the principle of least privilege by restricting traffic flow between segments to only what is necessary. It is especially important for protecting sensitive assets like databases, payment systems, and critical infrastructure from broader network compromises.",
    category: "Network Security",
    difficulty: "MEDIUM" as const,
    tags: ["segmentation", "VLAN", "lateral movement", "defense in depth"],
  },
  {
    questionText: "Explain the concept of a Zero Trust network architecture.",
    idealAnswer: "Zero Trust is a security model based on the principle of 'never trust, always verify,' meaning that no user or device is automatically trusted regardless of whether they are inside or outside the network perimeter. Every access request is authenticated, authorized, and continuously validated using multiple signals such as user identity, device health, location, and behavior analytics. Zero Trust architectures typically employ micro-segmentation, multi-factor authentication, least-privilege access, and continuous monitoring. This approach significantly reduces the attack surface and limits the blast radius of potential breaches compared to traditional perimeter-based security models.",
    category: "Network Security",
    difficulty: "MEDIUM" as const,
    tags: ["zero trust", "micro-segmentation", "identity verification", "least privilege"],
  },
  {
    questionText: "What is ARP spoofing and how can it be mitigated in an enterprise environment?",
    idealAnswer: "ARP spoofing is an attack where a malicious actor sends falsified ARP (Address Resolution Protocol) messages on a local network, linking their MAC address with the IP address of a legitimate host such as the default gateway. This allows the attacker to intercept, modify, or stop network traffic in a man-in-the-middle attack. Mitigation strategies include enabling Dynamic ARP Inspection (DAI) on managed switches, using static ARP entries for critical infrastructure, implementing 802.1X port-based network access control, deploying network monitoring tools to detect ARP anomalies, and segmenting the network to limit the scope of layer-2 attacks.",
    category: "Network Security",
    difficulty: "MEDIUM" as const,
    tags: ["ARP spoofing", "man-in-the-middle", "layer 2", "DAI"],
  },
  {
    questionText: "Describe how BGP hijacking works and its implications for internet security.",
    idealAnswer: "BGP hijacking occurs when an attacker or misconfigured autonomous system (AS) announces IP prefixes that it does not own, causing internet routers to redirect traffic through the attacker's network. Because BGP relies on trust between peers and does not natively verify route announcements, any AS can claim ownership of any IP prefix. The implications are severe: attackers can intercept sensitive traffic for espionage, redirect users to phishing sites, or cause widespread denial of service. Countermeasures include implementing RPKI (Resource Public Key Infrastructure) to cryptographically validate route origins, using BGP monitoring services, configuring prefix filters, and deploying BGPsec for path validation.",
    category: "Network Security",
    difficulty: "HARD" as const,
    tags: ["BGP", "routing", "RPKI", "internet security"],
  },
  {
    questionText: "Explain how TLS 1.3 improves security over TLS 1.2 and what attacks it mitigates.",
    idealAnswer: "TLS 1.3 introduces several significant security improvements over TLS 1.2 by removing support for legacy cryptographic algorithms like RSA key exchange, CBC mode ciphers, and SHA-1, which were vulnerable to attacks such as BEAST, POODLE, and Lucky13. It mandates forward secrecy through ephemeral Diffie-Hellman key exchanges, meaning that compromise of long-term keys cannot decrypt past sessions. TLS 1.3 also reduces the handshake from two round trips to one (and supports 0-RTT resumption), reducing latency and the window for interception. The simplified cipher suite selection eliminates downgrade attacks, and the encrypted handshake prevents passive observers from seeing certificate information.",
    category: "Network Security",
    difficulty: "HARD" as const,
    tags: ["TLS", "encryption", "forward secrecy", "protocol security"],
  },
  {
    questionText: "How would you detect and respond to a sophisticated lateral movement attack using DNS tunneling within a corporate network?",
    idealAnswer: "DNS tunneling encodes data within DNS queries and responses to exfiltrate information or establish command-and-control channels, bypassing traditional firewalls since DNS traffic is typically allowed. Detection involves monitoring for anomalous DNS patterns such as unusually long domain names, high query volumes to uncommon domains, TXT record abuse, and entropy analysis of query strings. Response steps include isolating affected hosts, blocking identified C2 domains at the DNS resolver level, implementing DNS filtering solutions, and deploying DNS-layer security tools like passive DNS monitoring. Long-term mitigation requires limiting DNS resolution to trusted internal resolvers, enforcing DNS over HTTPS/TLS policies, and integrating DNS analytics into the SIEM for continuous behavioral analysis.",
    category: "Network Security",
    difficulty: "HARD" as const,
    tags: ["DNS tunneling", "lateral movement", "C2", "exfiltration", "detection"],
  },

  // ===== SIEM / LOG ANALYSIS (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What is a SIEM and what is its primary purpose?",
    idealAnswer: "A Security Information and Event Management (SIEM) system collects, aggregates, and analyzes log data from across an organization's IT infrastructure, including servers, firewalls, endpoints, and applications. Its primary purpose is to provide real-time visibility into security events, detect threats through correlation rules and anomaly detection, and support compliance reporting. SIEMs centralize security monitoring by normalizing log formats from diverse sources into a unified view, enabling security analysts to investigate incidents efficiently. Popular SIEM solutions include Splunk, IBM QRadar, Microsoft Sentinel, and Elastic Security.",
    category: "SIEM",
    difficulty: "EASY" as const,
    tags: ["SIEM", "log management", "security monitoring", "event correlation"],
  },
  {
    questionText: "What types of log sources would you typically integrate into a SIEM?",
    idealAnswer: "A comprehensive SIEM deployment integrates logs from a wide range of sources to provide holistic visibility across the environment. Common sources include firewall and IDS/IPS logs, Windows Event Logs and Syslog from servers, endpoint detection and response (EDR) agents, Active Directory and authentication systems, VPN and proxy servers, cloud service provider logs (AWS CloudTrail, Azure Activity Logs), email gateways, database audit logs, and application-specific logs. The goal is to ensure coverage across the network, endpoints, identity systems, and cloud environments so that security analysts can correlate events and detect multi-stage attacks.",
    category: "SIEM",
    difficulty: "EASY" as const,
    tags: ["log sources", "SIEM integration", "visibility", "data collection"],
  },
  {
    questionText: "What is log normalization and why is it important in SIEM operations?",
    idealAnswer: "Log normalization is the process of converting log data from diverse sources and formats into a common, standardized schema so that events can be consistently searched, correlated, and analyzed. Different devices and applications produce logs in varying formats such as CEF, JSON, Syslog, Windows Event XML, and custom text formats. Without normalization, analysts would struggle to correlate events across sources or write effective detection rules. Normalization ensures that fields like timestamps, IP addresses, usernames, and event types are mapped to consistent field names, enabling efficient cross-source queries and accurate correlation rules within the SIEM.",
    category: "SIEM",
    difficulty: "EASY" as const,
    tags: ["normalization", "log parsing", "data standardization", "SIEM"],
  },
  {
    questionText: "How would you create a SIEM correlation rule to detect brute force login attempts?",
    idealAnswer: "A brute force detection correlation rule would monitor authentication logs for a threshold of failed login attempts from the same source IP or against the same user account within a defined time window, typically five or more failures within five minutes. The rule should trigger an alert of higher severity if a successful login follows the failed attempts, as this may indicate a successful compromise. Implementation involves filtering for event IDs associated with failed authentication (e.g., Windows Event ID 4625), grouping by source IP and target username, and setting threshold and time-window parameters. The rule should also include suppression logic to prevent alert fatigue from known vulnerability scanners or service accounts with expected authentication patterns.",
    category: "SIEM",
    difficulty: "MEDIUM" as const,
    tags: ["correlation rules", "brute force", "detection engineering", "authentication"],
  },
  {
    questionText: "Explain the difference between SIEM correlation rules and behavioral analytics (UEBA).",
    idealAnswer: "SIEM correlation rules are deterministic, rule-based detection mechanisms that trigger alerts when predefined conditions are met, such as 'five failed logins followed by a success within ten minutes.' They are effective for known attack patterns but require manual creation and maintenance. User and Entity Behavior Analytics (UEBA) uses machine learning and statistical models to establish baselines of normal behavior for users and entities, then detects anomalies that deviate from these baselines. UEBA can identify insider threats, compromised accounts, and novel attack techniques that rules-based approaches would miss. Most modern SIEM platforms incorporate both approaches, using rules for known threats and UEBA for detecting unknown or evolving attack patterns.",
    category: "SIEM",
    difficulty: "MEDIUM" as const,
    tags: ["UEBA", "correlation", "behavioral analytics", "machine learning"],
  },
  {
    questionText: "How do you handle alert fatigue in a SIEM environment?",
    idealAnswer: "Alert fatigue occurs when analysts are overwhelmed by a high volume of low-fidelity or false positive alerts, causing them to miss genuine threats. Addressing it requires a multi-pronged approach: regularly tuning correlation rules to reduce false positives by incorporating context such as asset criticality and known benign activity, implementing alert prioritization and risk scoring to surface the most critical alerts first, creating suppression rules for known benign patterns, and establishing alert triage procedures with defined escalation paths. Additionally, automating repetitive investigation tasks through SOAR (Security Orchestration, Automation, and Response) playbooks can reduce analyst workload. Periodic review of alert metrics and feedback loops between analysts and detection engineers are essential for continuous improvement.",
    category: "SIEM",
    difficulty: "MEDIUM" as const,
    tags: ["alert fatigue", "tuning", "SOAR", "false positives"],
  },
  {
    questionText: "What is the MITRE ATT&CK framework and how is it used to improve SIEM detection coverage?",
    idealAnswer: "MITRE ATT&CK is a globally recognized knowledge base of adversary tactics, techniques, and procedures (TTPs) based on real-world observations of cyberattacks. It categorizes attack behaviors across the kill chain, from initial access through exfiltration and impact. Security teams use ATT&CK to map their existing SIEM detection rules against the framework, identify coverage gaps where no rules exist for known techniques, and prioritize the development of new detections. By systematically mapping alerts to ATT&CK technique IDs, analysts gain better context during investigations, and organizations can measure their detection maturity. Many SIEM vendors now include ATT&CK mapping as a built-in feature for their detection content.",
    category: "SIEM",
    difficulty: "MEDIUM" as const,
    tags: ["MITRE ATT&CK", "TTPs", "detection coverage", "threat intelligence"],
  },
  {
    questionText: "How would you design a SIEM architecture for a large enterprise with multiple geographic locations and cloud environments?",
    idealAnswer: "A scalable enterprise SIEM architecture would employ a distributed collection model with log forwarders or collectors at each geographic site and cloud environment to minimize bandwidth consumption and ensure local redundancy. Data would be forwarded to a centralized SIEM cluster, potentially using a tiered storage approach with hot, warm, and cold tiers to balance query performance with storage costs. For cloud environments, native integrations with AWS CloudTrail, Azure Monitor, and GCP Cloud Logging would feed into the SIEM via API connectors or event hubs. The architecture should include a data lake for long-term retention and advanced analytics, high-availability configurations for the SIEM indexers and search heads, and role-based access controls to enforce data sovereignty requirements across regions. A SOAR layer should be integrated for automated response workflows.",
    category: "SIEM",
    difficulty: "HARD" as const,
    tags: ["SIEM architecture", "scalability", "distributed systems", "cloud integration"],
  },
  {
    questionText: "Explain how you would build a detection-as-code pipeline for SIEM rule management.",
    idealAnswer: "Detection-as-code applies software engineering practices to SIEM detection rule development by storing detection rules in a version-controlled repository using standardized formats such as Sigma rules or custom YAML schemas. A CI/CD pipeline validates rule syntax, runs unit tests against sample log data to verify expected matches, and automatically deploys approved rules to the SIEM environment. The pipeline should include peer review processes via pull requests, automated testing against known true positive and false positive datasets, and integration with the MITRE ATT&CK framework for coverage tracking. This approach enables reproducibility, collaboration across teams, rollback capabilities, and audit trails for compliance. It also facilitates translation of generic detection logic across multiple SIEM platforms using tools like Sigma converters.",
    category: "SIEM",
    difficulty: "HARD" as const,
    tags: ["detection-as-code", "CI/CD", "Sigma", "automation"],
  },
  {
    questionText: "How would you investigate and correlate logs to detect a multi-stage supply chain attack that leverages a compromised software update?",
    idealAnswer: "Investigating a supply chain attack requires correlating events across multiple log sources and timeframes. Start by identifying the initial infection vector through software inventory logs showing the compromised update deployment, then trace process execution logs on affected endpoints for anomalous child processes spawned by the updated application. Correlate network logs for C2 communication patterns such as beaconing behavior to unusual domains, DNS queries for newly registered domains, and encrypted traffic to non-standard ports. Cross-reference with threat intelligence feeds for known indicators of compromise associated with the campaign. Examine authentication logs for lateral movement using stolen credentials, file integrity monitoring for unauthorized changes, and data access logs for potential exfiltration. Build a timeline using the SIEM to reconstruct the full attack chain, and use entity-based correlation to identify all affected hosts and accounts across the environment.",
    category: "SIEM",
    difficulty: "HARD" as const,
    tags: ["supply chain", "investigation", "correlation", "threat hunting"],
  },

  // ===== VULNERABILITY ASSESSMENT (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What is a vulnerability assessment and how does it differ from a penetration test?",
    idealAnswer: "A vulnerability assessment is a systematic process of identifying, quantifying, and prioritizing security weaknesses in systems, networks, and applications using automated scanning tools and manual review techniques. It differs from a penetration test in that a vulnerability assessment identifies and reports vulnerabilities without actively exploiting them, while a penetration test goes further by attempting to exploit discovered vulnerabilities to demonstrate real-world impact. Vulnerability assessments are typically broader in scope and performed more frequently, while penetration tests are deeper, targeted exercises. Both are complementary components of a mature security program.",
    category: "Vulnerability Assessment",
    difficulty: "EASY" as const,
    tags: ["vulnerability assessment", "penetration testing", "scanning", "risk"],
  },
  {
    questionText: "What is the CVSS scoring system and how is it used?",
    idealAnswer: "The Common Vulnerability Scoring System (CVSS) is an industry-standard framework for rating the severity of security vulnerabilities on a scale from 0.0 to 10.0. It considers base metrics such as attack vector, attack complexity, privileges required, and impact on confidentiality, integrity, and availability. Temporal metrics account for exploit code maturity and remediation level, while environmental metrics allow organizations to adjust scores based on their specific infrastructure. CVSS scores are used to prioritize remediation efforts, with critical (9.0-10.0) and high (7.0-8.9) vulnerabilities typically requiring immediate attention. The scoring system is maintained by FIRST and is widely used in vulnerability databases like the National Vulnerability Database (NVD).",
    category: "Vulnerability Assessment",
    difficulty: "EASY" as const,
    tags: ["CVSS", "scoring", "vulnerability prioritization", "NVD"],
  },
  {
    questionText: "Name some common vulnerability scanning tools and their primary use cases.",
    idealAnswer: "Nessus by Tenable is one of the most widely used commercial vulnerability scanners for network and host assessment, offering comprehensive plugin coverage and compliance checks. OpenVAS (now Greenbone) is a popular open-source alternative that provides similar network scanning capabilities. Qualys offers a cloud-based vulnerability management platform with continuous monitoring capabilities. For web application scanning, tools like Burp Suite, OWASP ZAP, and Nikto are commonly used to identify issues like SQL injection, XSS, and misconfigurations. Organizations typically use a combination of these tools to achieve comprehensive coverage across their infrastructure and application layers.",
    category: "Vulnerability Assessment",
    difficulty: "EASY" as const,
    tags: ["Nessus", "OpenVAS", "Qualys", "vulnerability scanning"],
  },
  {
    questionText: "How do you prioritize vulnerability remediation when dealing with thousands of findings?",
    idealAnswer: "Effective prioritization goes beyond raw CVSS scores by incorporating contextual risk factors specific to the organization. Key considerations include whether the vulnerability is actively being exploited in the wild (using threat intelligence from sources like CISA KEV), the criticality and exposure of the affected asset (internet-facing versus internal), the availability and reliability of patches or workarounds, and the potential business impact of exploitation. A risk-based vulnerability management approach assigns a composite risk score that combines vulnerability severity, asset value, threat context, and compensating controls. This ensures that a medium-severity vulnerability on a critical internet-facing server may be prioritized over a high-severity vulnerability on an isolated internal development machine.",
    category: "Vulnerability Assessment",
    difficulty: "MEDIUM" as const,
    tags: ["risk-based", "prioritization", "remediation", "threat intelligence"],
  },
  {
    questionText: "What are the challenges of vulnerability scanning in cloud environments and how do you address them?",
    idealAnswer: "Cloud environments present unique scanning challenges including ephemeral resources that may spin up and down before a traditional scan completes, auto-scaling groups that change the target set dynamically, and shared responsibility models where the cloud provider manages certain layers. Addressing these challenges requires shifting to agent-based or API-driven scanning that integrates with cloud provider APIs to discover and assess resources in real time. Infrastructure-as-Code (IaC) scanning with tools like Checkov or tfsec can identify misconfigurations before deployment. Container image scanning should be integrated into CI/CD pipelines, and cloud security posture management (CSPM) tools should continuously assess configuration against benchmarks like CIS. Organizations must also consider network restrictions that may block traditional scanner traffic in cloud VPCs.",
    category: "Vulnerability Assessment",
    difficulty: "MEDIUM" as const,
    tags: ["cloud scanning", "CSPM", "IaC security", "container scanning"],
  },
  {
    questionText: "Explain the concept of vulnerability chaining and why it matters in assessments.",
    idealAnswer: "Vulnerability chaining refers to the technique of combining multiple lower-severity vulnerabilities to achieve a higher-impact attack that no single vulnerability could accomplish alone. For example, an information disclosure vulnerability might reveal internal network topology, which combined with a low-privilege access vulnerability and a privilege escalation flaw could lead to full system compromise. This matters because individual vulnerabilities might be deprioritized based on their standalone CVSS scores, but when chained together, they represent a critical risk. Skilled assessors and penetration testers look for these chains to demonstrate realistic attack paths, and vulnerability management programs should consider the cumulative risk of multiple findings on the same system or attack surface.",
    category: "Vulnerability Assessment",
    difficulty: "MEDIUM" as const,
    tags: ["vulnerability chaining", "attack path", "risk assessment", "exploitation"],
  },
  {
    questionText: "How do you handle false positives in vulnerability scan results?",
    idealAnswer: "False positives are inevitable in vulnerability scanning and must be addressed through a structured validation process. First, verify the finding manually by checking the actual software version, configuration, or attempting to confirm the vulnerability using a different tool or method. Once confirmed as a false positive, document it with evidence and create an exception or suppression rule in the scanning tool to prevent it from recurring in future reports. Maintain an exceptions register with periodic review dates to ensure suppressed findings remain valid as the environment changes. Establishing feedback loops between the vulnerability management team and the scanning tool vendor helps improve detection accuracy over time. Importantly, never dismiss findings without investigation, as apparent false positives sometimes indicate unexpected configurations or shadow IT.",
    category: "Vulnerability Assessment",
    difficulty: "MEDIUM" as const,
    tags: ["false positives", "validation", "scanning accuracy", "exceptions"],
  },
  {
    questionText: "How would you design a comprehensive vulnerability management program for a large organization with hybrid infrastructure?",
    idealAnswer: "A comprehensive program begins with complete asset discovery and inventory across on-premises, cloud, and remote environments, establishing asset criticality classifications. Implement a multi-scanner strategy with network-based scanners for traditional infrastructure, agent-based scanning for endpoints and servers, cloud-native CSPM tools, container and IaC scanning in CI/CD pipelines, and web application scanners for customer-facing applications. Define risk-based SLAs for remediation tied to asset criticality and vulnerability severity, with clear escalation procedures and executive reporting. Integrate vulnerability data into a centralized platform that correlates findings with threat intelligence, generates risk scores, and tracks remediation progress against SLAs. Establish governance through regular vulnerability review boards, exception management processes, and metrics that measure mean time to remediate, scan coverage percentage, and vulnerability debt trends.",
    category: "Vulnerability Assessment",
    difficulty: "HARD" as const,
    tags: ["vulnerability management", "program design", "hybrid infrastructure", "governance"],
  },
  {
    questionText: "Explain how you would assess and remediate vulnerabilities in a complex software supply chain.",
    idealAnswer: "Software supply chain vulnerability assessment requires visibility into all dependencies through Software Bill of Materials (SBOM) generation using tools like Syft or CycloneDX. Integrate Software Composition Analysis (SCA) tools such as Snyk, Dependabot, or OWASP Dependency-Check into CI/CD pipelines to continuously scan for known vulnerabilities in third-party libraries and transitive dependencies. Assess the risk of each dependency by evaluating maintainer reputation, update frequency, and known vulnerability history. For critical supply chain risks, implement binary verification through code signing and provenance attestation using frameworks like SLSA (Supply-chain Levels for Software Artifacts). Establish policies for acceptable vulnerability thresholds that can block deployments, maintain an internal package mirror for critical dependencies, and develop incident response procedures specifically for supply chain compromise scenarios like the SolarWinds or Log4Shell events.",
    category: "Vulnerability Assessment",
    difficulty: "HARD" as const,
    tags: ["supply chain", "SBOM", "SCA", "dependency management"],
  },
  {
    questionText: "How do you assess zero-day vulnerabilities when no CVE or patch exists yet?",
    idealAnswer: "Assessing zero-day vulnerabilities requires a proactive defense approach since traditional scanning tools will not detect them. Monitor threat intelligence feeds, vendor security advisories, and security research communities for early warnings of emerging threats. When a zero-day is disclosed, immediately determine exposure by inventorying affected software versions and assessing which assets run them. Evaluate compensating controls such as network segmentation, WAF rules, EDR behavioral detection, or application-level workarounds recommended by the vendor. Conduct impact analysis to understand the worst-case scenario if the vulnerability is exploited, considering the attack vector, required privileges, and potential blast radius. Implement virtual patching through IPS signatures or WAF rules where possible, increase monitoring for exploitation attempts using IOCs shared by the security community, and prepare for rapid patch deployment once a fix is released by the vendor.",
    category: "Vulnerability Assessment",
    difficulty: "HARD" as const,
    tags: ["zero-day", "threat intelligence", "compensating controls", "virtual patching"],
  },

  // ===== LINUX SECURITY (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What is the purpose of file permissions in Linux and how do you read them?",
    idealAnswer: "Linux file permissions control who can read, write, and execute files and directories, forming a fundamental access control mechanism. Permissions are displayed as a 10-character string (e.g., -rwxr-xr--) where the first character indicates the file type, and the remaining nine characters represent read (r), write (w), and execute (x) permissions for the owner, group, and others respectively. The numeric representation uses octal values where read=4, write=2, and execute=1, so 755 means the owner has full permissions while group and others have read and execute. Properly configuring file permissions is essential for preventing unauthorized access and follows the principle of least privilege.",
    category: "Linux Security",
    difficulty: "EASY" as const,
    tags: ["file permissions", "chmod", "access control", "Linux"],
  },
  {
    questionText: "What is the difference between the root user and a regular user in Linux?",
    idealAnswer: "The root user (UID 0) is the superuser account with unrestricted access to all commands, files, and system resources on a Linux system. A regular user has limited permissions and can only access files they own or have been granted permission to, and cannot perform system-level operations without elevation. Best security practice is to avoid logging in directly as root and instead use 'sudo' to temporarily elevate privileges for specific commands, which provides accountability through audit logging. Running services and daily operations under regular user accounts minimizes the impact of potential compromises, as an attacker gaining access to a limited user account cannot immediately control the entire system.",
    category: "Linux Security",
    difficulty: "EASY" as const,
    tags: ["root", "sudo", "user management", "privilege"],
  },
  {
    questionText: "What is SSH and how does key-based authentication improve security?",
    idealAnswer: "SSH (Secure Shell) is a cryptographic protocol used for secure remote access to Linux systems, providing encrypted communication over untrusted networks. Key-based authentication uses a public-private key pair where the public key is placed on the server and the private key remains securely on the client. This is more secure than password authentication because keys are cryptographically complex and immune to brute force attacks against the SSH service, they cannot be intercepted during transmission like passwords, and they can be further protected with passphrases. Best practice includes disabling password authentication entirely in sshd_config, using strong key algorithms like Ed25519, and implementing SSH key management policies including rotation and revocation.",
    category: "Linux Security",
    difficulty: "EASY" as const,
    tags: ["SSH", "key-based authentication", "remote access", "encryption"],
  },
  {
    questionText: "Explain SELinux and how it enhances Linux security beyond traditional permissions.",
    idealAnswer: "SELinux (Security-Enhanced Linux) is a mandatory access control (MAC) system that enforces security policies beyond traditional discretionary access control (DAC) file permissions. While DAC allows file owners to set permissions, SELinux uses kernel-level security policies that even the root user cannot override, assigning security contexts to processes, files, and network ports. SELinux operates in three modes: enforcing (blocks policy violations), permissive (logs but allows violations), and disabled. It protects against privilege escalation by confining processes to only the resources they need, so even if an application is compromised, the attacker is restricted to the application's security context. Managing SELinux involves understanding policies, contexts, and using tools like audit2allow and semanage to troubleshoot and customize policies.",
    category: "Linux Security",
    difficulty: "MEDIUM" as const,
    tags: ["SELinux", "MAC", "mandatory access control", "security context"],
  },
  {
    questionText: "How would you harden a Linux server according to CIS benchmarks?",
    idealAnswer: "CIS (Center for Internet Security) benchmarks provide comprehensive hardening guidelines for Linux systems. Key hardening steps include disabling unnecessary services and removing unused packages, configuring firewall rules with iptables or nftables to restrict network access, enabling and configuring audit logging with auditd to track security-relevant events, setting strong password policies in PAM configuration including complexity requirements and lockout thresholds, configuring file system hardening such as mounting /tmp with noexec and nosuid options, disabling root SSH login and enforcing key-based authentication, implementing filesystem integrity monitoring with tools like AIDE, and keeping the system updated with security patches. Automated compliance scanning tools like OpenSCAP can assess systems against CIS benchmarks and generate remediation reports.",
    category: "Linux Security",
    difficulty: "MEDIUM" as const,
    tags: ["CIS benchmarks", "hardening", "compliance", "security baseline"],
  },
  {
    questionText: "What is iptables and how do you configure it for basic network security?",
    idealAnswer: "Iptables is the traditional Linux kernel-level firewall utility that filters network packets using rule chains organized into tables (filter, nat, mangle, raw). The filter table contains INPUT (incoming traffic), OUTPUT (outgoing traffic), and FORWARD (routed traffic) chains. Basic security configuration involves setting the default policy to DROP for INPUT and FORWARD chains, then explicitly allowing necessary traffic such as SSH (port 22), HTTP/HTTPS (ports 80/443), and established connections using stateful tracking with '-m state --state ESTABLISHED,RELATED'. Rules are evaluated sequentially from top to bottom, with the first matching rule being applied. Best practices include logging dropped packets for monitoring, implementing rate limiting to prevent brute force attacks, and saving rules persistently using iptables-save. Modern distributions are transitioning to nftables as the successor to iptables.",
    category: "Linux Security",
    difficulty: "MEDIUM" as const,
    tags: ["iptables", "firewall", "packet filtering", "nftables"],
  },
  {
    questionText: "How does the Linux audit subsystem (auditd) work and what events should be monitored?",
    idealAnswer: "The Linux audit subsystem, managed by the auditd daemon, provides detailed logging of security-relevant system events by hooking into the kernel to track system calls, file access, and user activities. Audit rules are defined in /etc/audit/audit.rules and can monitor specific system calls, file accesses, and changes to critical configuration files. Essential events to monitor include: authentication attempts and failures, privilege escalation (sudo usage), changes to /etc/passwd, /etc/shadow, and /etc/sudoers, modifications to audit configuration itself, kernel module loading, file permission changes on sensitive directories, and network socket creation by unusual processes. Audit logs are stored in /var/log/audit/audit.log and should be forwarded to a centralized SIEM for correlation and long-term retention. Tools like ausearch and aureport help analyze audit data locally.",
    category: "Linux Security",
    difficulty: "MEDIUM" as const,
    tags: ["auditd", "audit logging", "system monitoring", "compliance"],
  },
  {
    questionText: "Explain Linux namespaces and cgroups and their role in container security isolation.",
    idealAnswer: "Linux namespaces provide process-level isolation by creating separate instances of global system resources: PID namespaces isolate process trees, network namespaces provide separate network stacks, mount namespaces isolate filesystem views, user namespaces map container UIDs to unprivileged host UIDs, and UTS namespaces isolate hostname and domain. Cgroups (control groups) complement namespaces by limiting and accounting for resource usage including CPU, memory, disk I/O, and network bandwidth, preventing container escape through resource exhaustion. Together, they form the foundation of container runtimes like Docker and containerd. However, containers share the host kernel, making kernel vulnerabilities a critical concern. Enhanced isolation can be achieved through seccomp profiles to restrict system calls, AppArmor or SELinux policies for mandatory access control, and rootless container configurations to minimize the impact of container breakouts.",
    category: "Linux Security",
    difficulty: "HARD" as const,
    tags: ["namespaces", "cgroups", "containers", "isolation", "kernel"],
  },
  {
    questionText: "How would you investigate and respond to a suspected rootkit infection on a Linux system?",
    idealAnswer: "Investigating a rootkit requires extreme caution since rootkits can subvert system tools to hide their presence. Begin by booting from trusted media or a live CD to avoid using potentially compromised system binaries. Use known-good statically compiled tools from a trusted USB drive for investigation. Run rootkit detection tools like chkrootkit and rkhunter against the mounted filesystem. Check for signs of compromise including hidden processes (compare /proc entries with ps output), unexpected kernel modules (lsmod against a known-good baseline), modified system binaries (compare hashes against package manager databases using rpm -Va or debsums), suspicious network connections, and unauthorized SUID/SGID files. Examine LD_PRELOAD for library injection, check /etc/ld.so.preload, and analyze kernel symbols for hooks. For response, the safest approach is to preserve forensic evidence by imaging the disk, rebuild the system from trusted media, restore data from known-clean backups, and investigate the initial attack vector to prevent reinfection.",
    category: "Linux Security",
    difficulty: "HARD" as const,
    tags: ["rootkit", "forensics", "incident response", "malware"],
  },
  {
    questionText: "Describe how you would implement a comprehensive Linux privilege escalation prevention strategy.",
    idealAnswer: "A comprehensive prevention strategy addresses multiple privilege escalation vectors. For SUID/SGID binaries, audit and minimize them using 'find / -perm -4000' and remove unnecessary setuid bits. Configure sudoers with granular, least-privilege rules avoiding NOPASSWD and using command whitelisting instead of ALL. Implement SELinux or AppArmor in enforcing mode to contain processes even if they gain elevated privileges. Deploy kernel hardening parameters via sysctl, including disabling kernel module loading after boot (kernel.modules_disabled=1), enabling ASLR, restricting ptrace (kernel.yama.ptrace_scope=2), and restricting access to dmesg and kernel pointers. Monitor for common escalation techniques including writable cron jobs, path injection, capabilities abuse (audit with getcap), and Docker socket exposure. Implement filesystem protections including noexec on /tmp, immutable flags on critical binaries, and file integrity monitoring. Regular automated scanning with tools like LinPEAS from an offensive perspective helps identify new escalation paths before attackers do.",
    category: "Linux Security",
    difficulty: "HARD" as const,
    tags: ["privilege escalation", "SUID", "kernel hardening", "sudo", "prevention"],
  },

  // ===== PENETRATION TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What are the main phases of a penetration test?",
    idealAnswer: "A penetration test typically follows five main phases. Reconnaissance involves gathering information about the target through passive (OSINT, DNS lookups, Shodan) and active (port scanning, service enumeration) methods. Scanning and enumeration uses tools like Nmap and vulnerability scanners to map the attack surface and identify potential entry points. Exploitation attempts to gain unauthorized access by leveraging discovered vulnerabilities using tools like Metasploit or manual techniques. Post-exploitation assesses the impact by pivoting through the network, escalating privileges, and accessing sensitive data. Finally, reporting documents all findings, evidence, risk ratings, and remediation recommendations in a comprehensive report for both technical and executive audiences.",
    category: "Penetration Testing",
    difficulty: "EASY" as const,
    tags: ["methodology", "penetration testing", "phases", "security assessment"],
  },
  {
    questionText: "What is the difference between black box, white box, and gray box testing?",
    idealAnswer: "Black box testing simulates a real-world external attacker with no prior knowledge of the target's internal systems, network architecture, or source code, relying entirely on reconnaissance to discover the attack surface. White box testing provides the tester with full knowledge including source code, architecture diagrams, credentials, and network maps, enabling comprehensive assessment of internal security controls. Gray box testing falls between the two, providing the tester with limited information such as user credentials or basic network documentation, simulating an insider threat or a compromised employee scenario. Each approach has trade-offs: black box is most realistic but may miss internal vulnerabilities, white box is most thorough but less realistic, and gray box offers a balanced approach that maximizes coverage within time constraints.",
    category: "Penetration Testing",
    difficulty: "EASY" as const,
    tags: ["black box", "white box", "gray box", "testing methodology"],
  },
  {
    questionText: "What is the OWASP Top 10 and why is it important for web application penetration testing?",
    idealAnswer: "The OWASP Top 10 is a regularly updated list of the ten most critical web application security risks, published by the Open Web Application Security Project. It serves as a standard awareness document and benchmark for web application security testing, covering vulnerabilities such as injection flaws, broken authentication, sensitive data exposure, XML external entities, broken access control, security misconfigurations, cross-site scripting (XSS), insecure deserialization, using components with known vulnerabilities, and insufficient logging and monitoring. Penetration testers use the OWASP Top 10 as a baseline checklist to ensure they test for the most prevalent and impactful web vulnerabilities. It also helps organizations prioritize security investments and meets compliance requirements for frameworks like PCI DSS.",
    category: "Penetration Testing",
    difficulty: "EASY" as const,
    tags: ["OWASP", "web security", "Top 10", "application security"],
  },
  {
    questionText: "Explain how SQL injection works and demonstrate how you would test for it during a penetration test.",
    idealAnswer: "SQL injection occurs when user-supplied input is incorporated into SQL queries without proper sanitization or parameterization, allowing an attacker to manipulate the query logic. During testing, start with manual techniques such as inserting single quotes (') into input fields to trigger SQL errors, using boolean-based payloads like 'OR 1=1--' to test for authentication bypass, and time-based blind injection with payloads like 'OR SLEEP(5)--' to detect vulnerabilities when no error messages are displayed. Automated tools like SQLMap can then be used to enumerate databases, extract data, and test for various injection types including UNION-based, error-based, and out-of-band techniques. Remediation involves using parameterized queries or prepared statements, implementing input validation, applying the principle of least privilege for database accounts, and deploying WAF rules as an additional defense layer.",
    category: "Penetration Testing",
    difficulty: "MEDIUM" as const,
    tags: ["SQL injection", "web exploitation", "SQLMap", "input validation"],
  },
  {
    questionText: "How would you perform privilege escalation on a Windows system during a penetration test?",
    idealAnswer: "Windows privilege escalation begins with thorough enumeration using tools like WinPEAS, PowerUp, or Seatbelt to identify misconfigurations. Common techniques include exploiting unquoted service paths where spaces in the path allow DLL injection, identifying services running with SYSTEM privileges that have weak file permissions allowing binary replacement, abusing AlwaysInstallElevated registry keys to install malicious MSI packages with SYSTEM privileges, token impersonation using tools like Juicy Potato or PrintSpoofer when SeImpersonatePrivilege is available, exploiting scheduled tasks with writable script paths, and harvesting credentials from memory using Mimikatz, registry hives, or saved credential stores. The escalation path chosen depends on the specific misconfigurations found during enumeration, and documentation of each step is critical for the penetration test report.",
    category: "Penetration Testing",
    difficulty: "MEDIUM" as const,
    tags: ["privilege escalation", "Windows", "post-exploitation", "enumeration"],
  },
  {
    questionText: "What is a reverse shell and how does it differ from a bind shell?",
    idealAnswer: "A bind shell opens a listening port on the target machine and waits for the attacker to connect to it, essentially binding a shell to a network port. A reverse shell works in the opposite direction: the target machine initiates an outbound connection back to the attacker's listening machine, providing a shell over that connection. Reverse shells are generally preferred in penetration testing because they can bypass firewalls and NAT configurations that block inbound connections, since most organizations allow outbound traffic. Common reverse shell payloads can be created using tools like msfvenom in various languages (Python, PowerShell, Bash, PHP). Detection of reverse shells involves monitoring for unusual outbound connections, process-network correlation, and behavioral analysis of command-line activity patterns.",
    category: "Penetration Testing",
    difficulty: "MEDIUM" as const,
    tags: ["reverse shell", "bind shell", "post-exploitation", "payload"],
  },
  {
    questionText: "Explain the concept of pivoting in penetration testing and describe common techniques.",
    idealAnswer: "Pivoting is the technique of using a compromised host as a stepping stone to access other systems on internal networks that are not directly reachable from the attacker's position. After compromising a dual-homed machine or a system with access to multiple network segments, the tester routes traffic through it to reach otherwise isolated targets. Common techniques include SSH tunneling (local and dynamic port forwarding), using Metasploit's autoroute and socks proxy modules, deploying tools like Chisel or ligolo-ng for creating SOCKS proxies through the compromised host, and using ProxyChains to route tools through the established tunnel. Pivoting is critical for demonstrating the real impact of a breach, showing how an attacker can move from a DMZ or user workstation deep into the internal network to reach critical assets like domain controllers and database servers.",
    category: "Penetration Testing",
    difficulty: "MEDIUM" as const,
    tags: ["pivoting", "lateral movement", "tunneling", "network penetration"],
  },
  {
    questionText: "Describe how you would conduct an Active Directory attack chain from initial compromise to domain admin.",
    idealAnswer: "An AD attack chain typically begins with initial access through phishing, credential stuffing, or exploiting an exposed service. From the initial foothold, enumerate the AD environment using BloodHound to map trust relationships, group memberships, and attack paths to domain admin. Common escalation techniques include Kerberoasting (requesting service tickets for SPNs and cracking them offline), AS-REP Roasting (targeting accounts without pre-authentication), abusing delegation configurations (unconstrained, constrained, and resource-based constrained delegation), exploiting Group Policy misconfigurations, and leveraging writable shares for credential harvesting. Pass-the-Hash, Pass-the-Ticket, and Overpass-the-Hash techniques allow lateral movement using stolen credentials without knowing plaintext passwords. The final objective typically involves DCSync to extract all domain credentials or Golden Ticket creation for persistent domain-level access. Each step must be carefully documented with timestamps and evidence for the report.",
    category: "Penetration Testing",
    difficulty: "HARD" as const,
    tags: ["Active Directory", "Kerberoasting", "BloodHound", "domain escalation"],
  },
  {
    questionText: "How would you bypass modern endpoint detection and response (EDR) solutions during a penetration test?",
    idealAnswer: "Bypassing modern EDR requires understanding that these solutions monitor process behavior, memory operations, API calls, and network activity using both signatures and behavioral heuristics. Techniques include using custom-compiled or modified tooling that avoids known signatures, employing process injection methods that evade hooking such as direct syscalls to bypass ntdll.dll hooks, using reflective DLL loading or PE manipulation to avoid dropping files to disk, leveraging living-off-the-land binaries (LOLBins) that are trusted system executables, implementing sleep obfuscation and encrypted memory regions to evade memory scanning, and using legitimate communication channels like DNS or HTTPS to trusted domains for C2 traffic. It is critical that EDR bypass testing is explicitly authorized in the rules of engagement, and that techniques used are fully documented. The goal is to demonstrate detection gaps to help the organization improve their EDR configuration and detection capabilities, not to simply avoid detection.",
    category: "Penetration Testing",
    difficulty: "HARD" as const,
    tags: ["EDR bypass", "evasion", "offensive security", "red team"],
  },
  {
    questionText: "Explain how you would test for and exploit Server-Side Request Forgery (SSRF) vulnerabilities in a cloud-hosted application.",
    idealAnswer: "SSRF testing in cloud environments is particularly critical because it can expose cloud metadata services containing sensitive credentials. Start by identifying application features that accept URLs or make server-side HTTP requests (webhooks, URL previews, PDF generators, file importers). Test with internal IP addresses (127.0.0.1, 169.254.169.254 for AWS metadata, 10.x.x.x ranges), DNS rebinding techniques to bypass URL validation, URL schema tricks (gopher://, file://), and redirect chains. In AWS, target the Instance Metadata Service (IMDS) at http://169.254.169.254/latest/meta-data/iam/security-credentials/ to extract IAM role credentials. For Azure, target the Instance Metadata Service at 169.254.169.254 with the Metadata header. Demonstrate impact by using obtained cloud credentials to access S3 buckets, databases, or other cloud services. Recommend mitigations including IMDSv2 enforcement (requiring session tokens), network-level restrictions on metadata endpoint access, URL validation with allowlists, and egress filtering.",
    category: "Penetration Testing",
    difficulty: "HARD" as const,
    tags: ["SSRF", "cloud exploitation", "metadata", "web application"],
  },

  // ===== PYTHON FOR SECURITY (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "Why is Python widely used in cybersecurity and what are its key advantages?",
    idealAnswer: "Python is the most popular programming language in cybersecurity due to its clean syntax, extensive standard library, and vast ecosystem of security-focused third-party packages. It excels at rapid prototyping of security tools, automating repetitive tasks like log analysis and vulnerability scanning, and scripting exploit development. Key libraries include Scapy for packet manipulation, Requests for HTTP interactions, Paramiko for SSH automation, BeautifulSoup for web scraping, and Impacket for network protocol interactions. Python's cross-platform compatibility, large community support, and integration with security tools like Metasploit, Burp Suite, and various APIs make it an essential skill for security analysts, penetration testers, and incident responders alike.",
    category: "Python for Security",
    difficulty: "EASY" as const,
    tags: ["Python", "scripting", "automation", "security tools"],
  },
  {
    questionText: "What Python libraries are commonly used for network scanning and packet analysis?",
    idealAnswer: "Scapy is the most versatile Python library for network analysis, allowing creation, manipulation, sending, and sniffing of network packets at a low level, supporting protocols like TCP, UDP, ICMP, ARP, and DNS. The socket library provides basic network connectivity for creating custom scanners and network tools. Python-nmap serves as a wrapper around the Nmap port scanner, enabling programmatic scanning and result parsing. For packet capture analysis, Pyshark wraps TShark to parse pcap files, while dpkt provides fast packet parsing capabilities. For higher-level HTTP interactions, the Requests library is standard for interacting with web services and APIs. These libraries enable security professionals to build custom network reconnaissance tools, automated vulnerability scanners, and traffic analysis scripts tailored to their specific needs.",
    category: "Python for Security",
    difficulty: "EASY" as const,
    tags: ["Scapy", "networking", "packet analysis", "Python libraries"],
  },
  {
    questionText: "How would you use Python to automate basic security tasks like checking for open ports?",
    idealAnswer: "A basic Python port scanner can be built using the socket library by iterating through a range of ports, attempting TCP connections with socket.connect_ex() which returns 0 for open ports, and handling timeouts to avoid long waits on filtered ports. For better performance, use concurrent.futures.ThreadPoolExecutor to scan multiple ports simultaneously. A more robust approach uses the python-nmap library to leverage Nmap's scanning engine, which provides service detection and OS fingerprinting capabilities. The results can be parsed and formatted into reports, stored in databases, or compared against previous scans to detect changes. Adding error handling, configurable timeouts, and output formatting transforms a simple script into a reusable security tool. This type of automation saves analysts significant time compared to manual scanning and can be scheduled to run periodically.",
    category: "Python for Security",
    difficulty: "EASY" as const,
    tags: ["port scanning", "automation", "socket", "scripting"],
  },
  {
    questionText: "How would you write a Python script to parse and analyze log files for security indicators?",
    idealAnswer: "A log analysis script would use Python's built-in modules like 're' for regex pattern matching, 'collections.Counter' for frequency analysis, and 'datetime' for timestamp parsing. Start by defining patterns for indicators such as failed login attempts, unusual user agents, SQL injection payloads, or access to sensitive paths. Read log files line by line using generators to handle large files efficiently without loading them entirely into memory. Use regex named groups to extract structured data like IP addresses, timestamps, and request paths. Aggregate findings using dictionaries or pandas DataFrames for statistical analysis, identifying anomalies such as IPs with high failure rates or unusual access patterns during off-hours. Output can be formatted as CSV reports, JSON for SIEM ingestion, or visualized using matplotlib. Adding argparse for command-line arguments makes the script flexible and reusable across different log formats.",
    category: "Python for Security",
    difficulty: "MEDIUM" as const,
    tags: ["log analysis", "regex", "parsing", "threat detection"],
  },
  {
    questionText: "Explain how you would use Python to interact with security APIs like VirusTotal or Shodan.",
    idealAnswer: "Interacting with security APIs in Python typically uses the Requests library for HTTP calls and API-specific SDKs when available. For VirusTotal, use the vt-py library or direct API calls to submit file hashes, URLs, or domains for reputation checks, parsing JSON responses to extract detection ratios and threat classifications. For Shodan, the shodan Python library provides methods to search for exposed services, query specific IPs for open ports and vulnerabilities, and monitor network exposure. Implementation best practices include storing API keys securely in environment variables rather than hardcoding them, implementing rate limiting with time.sleep() to respect API quotas, using try-except blocks for error handling, and caching results to avoid redundant queries. These integrations enable automated threat intelligence enrichment, indicator of compromise lookups, and attack surface monitoring as part of security workflows.",
    category: "Python for Security",
    difficulty: "MEDIUM" as const,
    tags: ["API integration", "VirusTotal", "Shodan", "threat intelligence"],
  },
  {
    questionText: "How would you use Python to detect and analyze potential phishing emails?",
    idealAnswer: "A Python phishing detection script would use the 'email' library to parse MIME messages, extracting headers, body content, and attachments. Analyze email headers for spoofing indicators by checking SPF, DKIM, and DMARC alignment using libraries like dkimpy and checkdmarc, and comparing the envelope sender with the display name. Extract URLs from the body and HTML content using BeautifulSoup and regex, then check them against URL reputation services, verify SSL certificates, and analyze for lookalike domains using Levenshtein distance or confusable Unicode character detection. Examine attachments for suspicious file types, embedded macros in Office documents using oletools, and calculate file hashes for VirusTotal lookups. Machine learning models trained on features like header anomalies, URL characteristics, and language patterns can enhance detection accuracy beyond rule-based approaches.",
    category: "Python for Security",
    difficulty: "MEDIUM" as const,
    tags: ["phishing", "email analysis", "detection", "automation"],
  },
  {
    questionText: "Describe how you would build a Python-based password auditing tool that checks for weak credentials.",
    idealAnswer: "A password auditing tool would use the hashlib library to generate hashes in various formats (MD5, SHA-1, SHA-256, bcrypt, NTLM) for comparison against captured password hashes. Implement dictionary attacks by hashing entries from wordlists like rockyou.txt and comparing against target hashes, using multiprocessing to parallelize the cracking across CPU cores. Add rule-based mutations including common substitutions (a->@, e->3), appending numbers, and capitalization variations. For policy compliance checking, verify passwords against rules for minimum length, complexity, common patterns, and known breach databases using the Have I Been Pwned API with k-anonymity. Use the 'zxcvbn' library for password strength estimation that considers patterns, dictionary words, and keyboard sequences. The tool should generate reports identifying weak accounts, policy violations, and recommendations for strengthening password policies, while ensuring all testing is authorized and credential data is handled securely.",
    category: "Python for Security",
    difficulty: "MEDIUM" as const,
    tags: ["password auditing", "hash cracking", "credential security", "compliance"],
  },
  {
    questionText: "How would you develop a Python-based network forensics tool that reconstructs TCP sessions from a pcap file?",
    idealAnswer: "Building a TCP session reconstruction tool requires using Scapy or dpkt to read pcap files and extract individual packets. Implement session tracking by creating a dictionary keyed on the 4-tuple (source IP, source port, destination IP, destination port), assembling packets into sessions by following TCP sequence numbers and handling retransmissions, out-of-order packets, and fragmentation. Track TCP state transitions (SYN, SYN-ACK, ACK, FIN, RST) to identify session boundaries and anomalies like incomplete handshakes or RST floods. Extract application-layer payloads by reassembling the byte stream in sequence-number order, identifying protocols through port numbers and deep packet inspection. For HTTP sessions, parse request/response pairs and extract transferred files. Implement flow statistics including duration, byte counts, packet counts, and inter-arrival times for behavioral analysis. Output structured session data as JSON or CSV for further analysis, and integrate with YARA rules for pattern matching against known malicious content.",
    category: "Python for Security",
    difficulty: "HARD" as const,
    tags: ["network forensics", "pcap", "TCP reconstruction", "Scapy"],
  },
  {
    questionText: "Explain how you would create a Python-based YARA rule engine for automated malware classification.",
    idealAnswer: "Building a YARA-based malware classification system uses the yara-python library to compile and apply YARA rules against suspicious files. Design a modular rule repository organized by malware family, technique, and packer type, with rules matching on byte patterns, strings, file structure characteristics, imported functions, and entropy levels. Implement a scanning pipeline that accepts files from multiple sources (filesystem directories, email attachments, SIEM alerts), extracts metadata including file type, size, hashes, and entropy, then applies the YARA ruleset to classify samples. Add static analysis features such as PE header parsing with pefile, extracting embedded strings, and identifying packed or obfuscated binaries. Integrate with sandbox APIs (Cuckoo, Any.Run) for dynamic analysis when static rules are inconclusive. Store results in a database for tracking malware trends and building classification statistics. Implement automated rule testing against known samples to prevent false positives and maintain rule quality as the repository grows.",
    category: "Python for Security",
    difficulty: "HARD" as const,
    tags: ["YARA", "malware classification", "static analysis", "automation"],
  },
  {
    questionText: "How would you build a Python-based threat hunting framework that automates IOC sweeping across an enterprise environment?",
    idealAnswer: "A threat hunting framework would integrate with multiple enterprise data sources through APIs: EDR platforms (CrowdStrike, Carbon Black) for endpoint telemetry, SIEM for centralized logs, Active Directory for identity context, and network monitoring tools for traffic analysis. Build an IOC ingestion module that accepts indicators in STIX/TAXII format from threat intelligence feeds, normalizing them into a common data model supporting IP addresses, domains, file hashes, email addresses, and behavioral patterns. Implement parallel search workers using asyncio or multiprocessing that simultaneously query across data sources, with configurable time windows and scope filters. Add a correlation engine that links related findings across sources, building attack narratives from individual IOC matches. Generate prioritized hunt reports with context enrichment from MITRE ATT&CK mapping, asset criticality, and historical baseline comparison. Include a feedback loop where confirmed true positives automatically generate detection rules for the SIEM, and false positives refine the IOC quality scoring system.",
    category: "Python for Security",
    difficulty: "HARD" as const,
    tags: ["threat hunting", "IOC", "STIX/TAXII", "enterprise security"],
  },

  // ===== CLOUD SECURITY (AWS/AZURE) (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What is the shared responsibility model in cloud security?",
    idealAnswer: "The shared responsibility model defines the division of security obligations between the cloud service provider (CSP) and the customer. The CSP is responsible for security 'of' the cloud, including physical infrastructure, hypervisors, networking hardware, and the global network backbone. The customer is responsible for security 'in' the cloud, which varies by service model: in IaaS, customers manage OS patching, network configuration, and data encryption; in PaaS, they manage application code and data; in SaaS, they manage access control and data classification. Understanding this model is critical because many cloud security breaches result from customers failing to properly configure their side of the responsibility, such as leaving S3 buckets public or not enabling MFA on IAM accounts.",
    category: "Cloud Security",
    difficulty: "EASY" as const,
    tags: ["shared responsibility", "cloud security", "IaaS", "AWS", "Azure"],
  },
  {
    questionText: "What is IAM in cloud security and why is it important?",
    idealAnswer: "Identity and Access Management (IAM) is the framework for managing digital identities and controlling access to cloud resources. In AWS, IAM allows you to create users, groups, roles, and policies that define granular permissions for accessing services and resources. In Azure, Azure Active Directory (Entra ID) serves a similar function with role-based access control (RBAC). IAM is critically important because misconfigured permissions are among the leading causes of cloud security breaches, with overly permissive policies granting more access than needed. Best practices include following the principle of least privilege, enforcing multi-factor authentication, using service roles instead of long-lived access keys, regularly auditing permissions, and implementing conditions and boundaries on policies to restrict access by IP, time, or resource tags.",
    category: "Cloud Security",
    difficulty: "EASY" as const,
    tags: ["IAM", "access control", "AWS", "Azure", "identity management"],
  },
  {
    questionText: "What are security groups and network ACLs in AWS, and how do they differ?",
    idealAnswer: "Security groups act as virtual firewalls at the instance level, controlling inbound and outbound traffic with stateful rules, meaning if inbound traffic is allowed, the response is automatically permitted. Network ACLs operate at the subnet level and are stateless, requiring explicit rules for both inbound and outbound traffic. Security groups use allow-only rules and evaluate all rules before making a decision, while NACLs support both allow and deny rules and evaluate rules in numerical order, stopping at the first match. Best practice is to use security groups as the primary defense with restrictive rules per instance or application tier, and NACLs as an additional layer for broad subnet-level controls. Both should follow the principle of least privilege, allowing only necessary traffic.",
    category: "Cloud Security",
    difficulty: "EASY" as const,
    tags: ["security groups", "NACL", "AWS", "network security", "VPC"],
  },
  {
    questionText: "How would you secure an S3 bucket to prevent unauthorized data exposure?",
    idealAnswer: "Securing S3 buckets requires a layered approach starting with enabling S3 Block Public Access at the account and bucket level to prevent accidental public exposure. Configure bucket policies and IAM policies following least privilege, granting access only to specific principals and actions needed. Enable server-side encryption (SSE-S3, SSE-KMS, or SSE-C) for data at rest, and enforce encryption in transit by adding a bucket policy condition requiring 'aws:SecureTransport'. Enable S3 access logging and CloudTrail data events to monitor who accesses the bucket. Implement S3 Object Lock for immutable storage where needed, use VPC endpoints to restrict access from within specific VPCs, and enable versioning to protect against accidental deletion. Regularly audit bucket permissions using AWS Config rules, IAM Access Analyzer, and tools like Prowler to detect misconfigurations before they lead to data breaches.",
    category: "Cloud Security",
    difficulty: "MEDIUM" as const,
    tags: ["S3", "data protection", "encryption", "access control", "AWS"],
  },
  {
    questionText: "Explain how AWS CloudTrail and Azure Activity Log support security monitoring.",
    idealAnswer: "AWS CloudTrail records API calls made in your AWS account, capturing who made the call, from which IP address, when it occurred, and what resources were affected, providing a comprehensive audit trail for security investigations. It supports management events (control plane operations), data events (resource-level operations like S3 object access), and insight events for detecting unusual activity. Azure Activity Log similarly captures subscription-level events including resource modifications, RBAC changes, and service health events. Both should be configured to send logs to centralized storage (S3 bucket or Azure Storage Account) with appropriate retention policies, integrated with SIEM systems for real-time alerting, and protected with immutable logging configurations to prevent tampering. Key security use cases include detecting unauthorized IAM changes, identifying credential abuse, tracking resource creation for shadow IT detection, and supporting compliance audit requirements.",
    category: "Cloud Security",
    difficulty: "MEDIUM" as const,
    tags: ["CloudTrail", "Azure", "audit logging", "monitoring", "compliance"],
  },
  {
    questionText: "What are the key security considerations when deploying containerized applications in cloud environments?",
    idealAnswer: "Container security in the cloud spans the entire lifecycle from build to runtime. During build, scan container images for vulnerabilities using tools like Trivy, Snyk, or ECR/ACR native scanning, use minimal base images (distroless or Alpine), avoid running as root, and never embed secrets in images. In the registry, enable image signing and enforce signature verification, implement access controls, and enable vulnerability scanning on push. For orchestration (EKS, AKS, ECS), implement network policies to restrict pod-to-pod communication, use pod security standards to prevent privileged containers, enable RBAC with least-privilege service accounts, and manage secrets through cloud-native services like AWS Secrets Manager or Azure Key Vault. At runtime, implement runtime security monitoring with tools like Falco or cloud-native solutions, configure resource limits to prevent denial-of-service, and enable audit logging. Regularly update both the container runtime and the orchestration platform to patch known vulnerabilities.",
    category: "Cloud Security",
    difficulty: "MEDIUM" as const,
    tags: ["containers", "Kubernetes", "EKS", "AKS", "image security"],
  },
  {
    questionText: "How do you prevent and detect credential theft in cloud environments?",
    idealAnswer: "Preventing cloud credential theft starts with eliminating long-lived credentials wherever possible by using IAM roles for EC2 instances, Lambda functions, and ECS tasks, and implementing identity federation with SAML or OIDC for human users. Enforce MFA on all IAM accounts, especially privileged ones, and implement SCPs (Service Control Policies) to restrict sensitive actions. Use AWS Secrets Manager or Azure Key Vault for application credentials with automatic rotation. For detection, monitor CloudTrail for anomalous API calls such as unusual services being accessed, calls from unexpected geographic locations, or API calls outside normal business hours. Enable GuardDuty (AWS) or Microsoft Defender for Cloud to detect compromised credentials through behavioral analysis. Implement credential exposure scanning in code repositories using tools like git-secrets or GitHub Advanced Security, and monitor public paste sites and dark web sources for leaked credentials. Response procedures should include immediate credential rotation, session invalidation, and forensic analysis of actions performed with compromised credentials.",
    category: "Cloud Security",
    difficulty: "MEDIUM" as const,
    tags: ["credential theft", "IAM", "detection", "secrets management"],
  },
  {
    questionText: "Describe how you would architect a secure multi-account AWS environment using AWS Organizations.",
    idealAnswer: "A secure multi-account architecture uses AWS Organizations with a well-defined organizational unit (OU) structure separating environments (production, staging, development), workloads, and security functions. Create dedicated accounts for security tooling (centralized logging, GuardDuty delegated admin, Security Hub), networking (Transit Gateway, shared VPCs), and identity (SSO via IAM Identity Center). Implement Service Control Policies (SCPs) at each OU level to enforce guardrails such as preventing disabling of CloudTrail, restricting region usage, and blocking public S3 buckets. Enable centralized logging by configuring all accounts to forward CloudTrail, VPC Flow Logs, and DNS logs to a dedicated log archive account with immutable storage. Deploy AWS Config across all accounts with conformance packs for continuous compliance monitoring. Use Infrastructure as Code (CloudFormation StackSets or Terraform) for consistent security baseline deployment across accounts. Implement cross-account access using IAM roles with external IDs rather than sharing credentials, and use AWS RAM for controlled resource sharing between accounts.",
    category: "Cloud Security",
    difficulty: "HARD" as const,
    tags: ["AWS Organizations", "multi-account", "SCP", "architecture"],
  },
  {
    questionText: "How would you respond to a compromised AWS access key that was accidentally committed to a public GitHub repository?",
    idealAnswer: "Immediate response must be rapid since exposed keys are typically exploited within minutes by automated scanners. First, immediately disable or delete the exposed access key in the IAM console without waiting for investigation. Then revoke any active sessions associated with the IAM user or role by applying an inline deny policy with a condition on token issuance time. Review CloudTrail logs to determine all actions performed with the compromised key, focusing on resource creation (crypto mining instances), data access (S3 downloads), privilege escalation (new IAM users or policies), and persistence mechanisms (new access keys, Lambda functions, or backdoor accounts). Assess the blast radius by checking what permissions the key had and which resources may have been accessed or modified. Remediate any changes made by the attacker, including terminating unauthorized resources and revoking any new credentials created. Post-incident, implement preventive measures including pre-commit hooks with git-secrets, GitHub push protection, AWS credential rotation policies, and reducing the scope of IAM policies to follow least privilege.",
    category: "Cloud Security",
    difficulty: "HARD" as const,
    tags: ["incident response", "credential exposure", "AWS", "GitHub"],
  },
  {
    questionText: "Explain how to implement a comprehensive data protection strategy across a multi-cloud environment spanning AWS and Azure.",
    idealAnswer: "A multi-cloud data protection strategy begins with data classification and discovery using tools like AWS Macie and Azure Purview to identify and categorize sensitive data across both environments. Implement encryption at rest using cloud-native KMS services (AWS KMS, Azure Key Vault) with customer-managed keys and cross-cloud key management policies that enforce separation of duties. Enforce encryption in transit through TLS policies on all endpoints, certificate management via ACM and Azure App Service Certificates, and mutual TLS for service-to-service communication. Implement data loss prevention (DLP) policies at the network edge, application layer, and endpoint level, with consistent rules across both clouds. For data residency and sovereignty, configure cloud resources in compliant regions with policy enforcement preventing data movement to restricted regions. Deploy cross-cloud monitoring using a unified SIEM that ingests logs from both environments, with alerting on unauthorized data access, exfiltration attempts, and encryption policy violations. Implement backup and recovery strategies with cross-region and cross-cloud replication where compliance allows, and regularly test data recovery procedures.",
    category: "Cloud Security",
    difficulty: "HARD" as const,
    tags: ["multi-cloud", "data protection", "encryption", "classification"],
  },

  // ===== COMPLIANCE (SOC2/GDPR/HIPAA) (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What is SOC 2 compliance and what are its five Trust Service Criteria?",
    idealAnswer: "SOC 2 (System and Organization Controls 2) is an auditing framework developed by the AICPA that evaluates an organization's information systems based on five Trust Service Criteria. Security (the common criteria) requires protection of system resources against unauthorized access. Availability ensures the system is accessible as committed in service agreements. Processing Integrity verifies that system processing is complete, valid, accurate, and timely. Confidentiality ensures information designated as confidential is protected as committed. Privacy addresses the collection, use, retention, and disposal of personal information. SOC 2 reports come in two types: Type I evaluates the design of controls at a point in time, while Type II evaluates the operating effectiveness of controls over a period, typically 6-12 months.",
    category: "Compliance",
    difficulty: "EASY" as const,
    tags: ["SOC 2", "Trust Service Criteria", "audit", "compliance"],
  },
  {
    questionText: "What is GDPR and what are the key rights it grants to data subjects?",
    idealAnswer: "The General Data Protection Regulation (GDPR) is the European Union's comprehensive data privacy regulation that governs how organizations collect, process, store, and transfer personal data of EU residents. Key rights granted to data subjects include the right to access their personal data, the right to rectification of inaccurate data, the right to erasure (right to be forgotten), the right to restrict processing, the right to data portability in machine-readable format, the right to object to processing, and rights related to automated decision-making and profiling. Organizations must have a lawful basis for processing personal data (consent, contract, legal obligation, vital interests, public task, or legitimate interests), implement data protection by design and default, and report data breaches to supervisory authorities within 72 hours. Non-compliance can result in fines up to 20 million euros or 4% of global annual turnover.",
    category: "Compliance",
    difficulty: "EASY" as const,
    tags: ["GDPR", "data privacy", "data subject rights", "EU regulation"],
  },
  {
    questionText: "What is HIPAA and what types of data does it protect?",
    idealAnswer: "HIPAA (Health Insurance Portability and Accountability Act) is a US federal law that establishes national standards for protecting sensitive patient health information. It protects Protected Health Information (PHI), which includes any individually identifiable health information relating to a person's past, present, or future physical or mental health condition, healthcare services provided, or payment for healthcare. PHI encompasses names, addresses, dates, Social Security numbers, medical record numbers, and any other data that could identify a patient when combined with health information. Electronic PHI (ePHI) is specifically addressed by the Security Rule, which requires administrative, physical, and technical safeguards. HIPAA applies to covered entities (healthcare providers, health plans, clearinghouses) and their business associates, with violations potentially resulting in fines ranging from $100 to $50,000 per violation.",
    category: "Compliance",
    difficulty: "EASY" as const,
    tags: ["HIPAA", "PHI", "healthcare", "data protection"],
  },
  {
    questionText: "How do you implement access controls to satisfy both SOC 2 and HIPAA requirements?",
    idealAnswer: "Access control implementation that satisfies both frameworks requires role-based access control (RBAC) that enforces the principle of least privilege, ensuring users only have access to systems and data necessary for their job functions. Implement multi-factor authentication for all privileged access and remote connections, with regular access reviews conducted quarterly to verify that permissions remain appropriate. Maintain detailed audit logs of all access events including login attempts, privilege escalations, and data access, with logs retained for the required period (typically 6 years for HIPAA). Implement automated provisioning and de-provisioning tied to HR processes to ensure timely access removal for terminated employees. For HIPAA specifically, implement break-glass procedures for emergency access with post-access review, and ensure that access to ePHI is logged and monitored. Document all access control policies and procedures, conduct annual risk assessments, and include access controls in security awareness training.",
    category: "Compliance",
    difficulty: "MEDIUM" as const,
    tags: ["access control", "RBAC", "SOC 2", "HIPAA", "audit"],
  },
  {
    questionText: "What is a Data Protection Impact Assessment (DPIA) and when is it required under GDPR?",
    idealAnswer: "A Data Protection Impact Assessment (DPIA) is a process required by GDPR Article 35 to identify and minimize the data protection risks of a project or processing activity. It is mandatory when processing is likely to result in high risk to individuals' rights and freedoms, specifically when implementing new technologies, conducting large-scale profiling or automated decision-making, processing special category data (health, biometric, genetic) at scale, or conducting systematic monitoring of public areas. The DPIA must describe the processing operations and their purposes, assess the necessity and proportionality of the processing, identify risks to data subjects, and detail the measures planned to mitigate those risks. If the assessment reveals high residual risk that cannot be mitigated, the organization must consult with the supervisory authority before proceeding. DPIAs should be reviewed regularly and updated when the processing activity changes.",
    category: "Compliance",
    difficulty: "MEDIUM" as const,
    tags: ["DPIA", "GDPR", "risk assessment", "data protection"],
  },
  {
    questionText: "Explain the concept of Business Associate Agreements (BAAs) in HIPAA compliance.",
    idealAnswer: "A Business Associate Agreement (BAA) is a legally binding contract required by HIPAA between a covered entity and any business associate that creates, receives, maintains, or transmits Protected Health Information on the covered entity's behalf. The BAA must specify the permitted uses and disclosures of PHI, require the business associate to implement appropriate safeguards, mandate breach notification procedures, ensure return or destruction of PHI upon contract termination, and make the business associate's internal practices and records available for compliance audits. Common business associates include cloud service providers, IT managed service providers, billing companies, and consultants who access PHI. Failure to have a BAA in place when one is required constitutes a HIPAA violation for both parties. Cloud providers like AWS and Azure offer HIPAA-eligible services and standard BAAs, but organizations must still configure these services appropriately to maintain compliance.",
    category: "Compliance",
    difficulty: "MEDIUM" as const,
    tags: ["BAA", "HIPAA", "business associate", "covered entity"],
  },
  {
    questionText: "How do you handle a data breach notification process that satisfies GDPR, HIPAA, and SOC 2 requirements simultaneously?",
    idealAnswer: "A unified breach notification process must accommodate the strictest requirements from each framework. GDPR requires notification to the supervisory authority within 72 hours and to affected data subjects without undue delay when there is high risk. HIPAA requires notification to affected individuals within 60 days, to HHS, and to media for breaches affecting 500 or more individuals. SOC 2 requires documented incident response procedures and communication to affected parties. The process should include immediate containment and assessment, classification of the breach by type and scope, determination of which regulations apply based on the data involved and affected individuals, parallel notification workflows for each regulatory body with appropriate templates, documentation of the root cause analysis and remediation steps, and post-incident review. Maintain pre-approved notification templates for each jurisdiction, establish clear roles and responsibilities including legal counsel involvement, and regularly test the process through tabletop exercises.",
    category: "Compliance",
    difficulty: "MEDIUM" as const,
    tags: ["breach notification", "GDPR", "HIPAA", "incident response"],
  },
  {
    questionText: "How would you architect a compliance-as-code framework that continuously monitors and enforces regulatory requirements across the technology stack?",
    idealAnswer: "A compliance-as-code framework translates regulatory requirements into automated, version-controlled policies that continuously monitor the environment. Map each regulatory control (SOC 2 CC6.1, HIPAA 164.312(a), GDPR Article 32) to specific technical controls and codify them as policy rules using tools like Open Policy Agent (OPA), AWS Config Rules, Azure Policy, or HashiCorp Sentinel. Implement a policy pipeline where changes to compliance rules follow the same CI/CD practices as application code, with peer review, testing, and staged rollout. Deploy continuous monitoring agents that evaluate infrastructure, application configurations, and access patterns against the codified policies, generating real-time compliance dashboards and automated alerts for violations. Integrate with IaC tools (Terraform, CloudFormation) to prevent non-compliant resources from being deployed through pre-deployment policy checks. Maintain evidence collection automation that continuously gathers audit artifacts like access reviews, configuration snapshots, and vulnerability scan results, mapping them to specific control requirements for auditor review.",
    category: "Compliance",
    difficulty: "HARD" as const,
    tags: ["compliance-as-code", "automation", "OPA", "continuous compliance"],
  },
  {
    questionText: "Describe how you would implement cross-border data transfer mechanisms that comply with GDPR requirements post-Schrems II.",
    idealAnswer: "Following the Schrems II ruling that invalidated the EU-US Privacy Shield, organizations must rely on alternative transfer mechanisms with supplementary measures. Standard Contractual Clauses (SCCs) adopted by the European Commission remain the primary mechanism, but must be supplemented with a Transfer Impact Assessment (TIA) that evaluates whether the destination country's laws provide adequate protection. Supplementary technical measures include end-to-end encryption where the data importer cannot access decryption keys, pseudonymization before transfer where the re-identification key remains in the EU, and split or multi-party processing where no single entity outside the EU has access to complete personal data. Organizational measures include binding corporate rules (BCRs) for intra-group transfers, contractual commitments from sub-processors, and transparency reporting on government access requests. For US transfers, the EU-US Data Privacy Framework provides an adequacy basis for certified organizations, but its long-term viability remains subject to legal challenges. Maintain a comprehensive Record of Processing Activities (ROPA) that documents all cross-border transfers with their legal basis and safeguards.",
    category: "Compliance",
    difficulty: "HARD" as const,
    tags: ["Schrems II", "data transfer", "SCCs", "GDPR", "international"],
  },
  {
    questionText: "How would you prepare for and manage a SOC 2 Type II audit while simultaneously maintaining HIPAA and GDPR compliance?",
    idealAnswer: "Managing multi-framework compliance requires establishing a unified controls framework that maps common controls across SOC 2, HIPAA, and GDPR to eliminate redundancy. Use a GRC (Governance, Risk, and Compliance) platform to maintain a control matrix that maps each implemented control to its applicable regulatory requirements, with evidence tagging that satisfies multiple frameworks simultaneously. Prepare for SOC 2 Type II by ensuring controls have been operating effectively throughout the audit period with continuous evidence collection, including access reviews, change management records, incident response documentation, and vulnerability management reports. Assign control owners who understand their responsibilities across all frameworks and conduct quarterly internal audits to identify gaps before external auditors find them. Maintain a risk register that addresses risks across all three frameworks, with documented risk treatment decisions. For the audit itself, prepare a readiness assessment with the auditor to align expectations, organize evidence in a centralized repository mapped to control objectives, and schedule personnel availability for auditor interviews. Post-audit, implement a remediation tracking process for any identified gaps with clear timelines and ownership.",
    category: "Compliance",
    difficulty: "HARD" as const,
    tags: ["SOC 2 audit", "multi-framework", "GRC", "audit readiness"],
  },

  // ===== INCIDENT RESPONSE (10 questions: 3 EASY, 4 MEDIUM, 3 HARD) =====
  {
    questionText: "What are the six phases of the NIST incident response lifecycle?",
    idealAnswer: "The NIST incident response lifecycle consists of six phases. Preparation involves establishing the incident response team, developing policies and procedures, deploying detection tools, and conducting training and exercises. Detection and Analysis focuses on monitoring systems for indicators of compromise, validating alerts, determining the scope and impact of incidents, and documenting findings. Containment involves short-term measures to stop the incident from spreading and long-term measures to maintain system stability while preparing for eradication. Eradication removes the root cause of the incident, including malware, compromised accounts, and exploited vulnerabilities. Recovery restores systems to normal operations, verifies functionality, and implements enhanced monitoring. Post-Incident Activity (Lessons Learned) involves a formal review of what happened, what worked, what could be improved, and updating procedures and controls accordingly.",
    category: "Incident Response",
    difficulty: "EASY" as const,
    tags: ["NIST", "incident response", "lifecycle", "methodology"],
  },
  {
    questionText: "What is an Indicator of Compromise (IOC) and what are common examples?",
    idealAnswer: "An Indicator of Compromise (IOC) is a piece of forensic evidence that suggests a system or network has been breached or is under attack. Common examples include unusual outbound network traffic to known malicious IP addresses or domains, unexpected changes to system files or registry keys, anomalous login patterns such as logins from unusual geographic locations or at odd hours, presence of known malware signatures or file hashes, unexpected privileged account usage, and large volumes of database queries indicating data exfiltration. IOCs are shared between organizations through threat intelligence platforms using standardized formats like STIX (Structured Threat Information eXpression) and can be automatically ingested by security tools for detection. They differ from Indicators of Attack (IOAs), which focus on detecting adversary behavior and intent rather than specific artifacts.",
    category: "Incident Response",
    difficulty: "EASY" as const,
    tags: ["IOC", "indicators", "threat detection", "forensics"],
  },
  {
    questionText: "What is the role of an incident response team and who should be on it?",
    idealAnswer: "An incident response team (IRT) is responsible for detecting, analyzing, containing, eradicating, and recovering from security incidents. The team typically includes a designated incident commander who coordinates the response, security analysts who perform technical investigation and containment, system administrators who implement technical remediation, a communications lead who manages internal and external messaging, legal counsel who advises on regulatory obligations and liability, and management representatives who authorize decisions with business impact. Depending on the incident, additional members may include HR (for insider threats), public relations (for public-facing breaches), and third-party forensics specialists. The team should have clearly defined roles and responsibilities documented in an incident response plan, conduct regular tabletop exercises and simulations, and maintain updated contact information and escalation procedures for after-hours incidents.",
    category: "Incident Response",
    difficulty: "EASY" as const,
    tags: ["incident response team", "roles", "IRT", "coordination"],
  },
  {
    questionText: "How do you perform proper digital evidence collection and chain of custody during an incident?",
    idealAnswer: "Digital evidence collection must follow forensically sound procedures to maintain admissibility and integrity. Begin by documenting the scene with photographs and notes about the state of systems. Create forensic images of affected drives using write-blockers and tools like FTK Imager or dd, generating hash values (SHA-256) of the original and the copy to verify integrity. Capture volatile evidence first (order of volatility): system memory using tools like WinPmem or LiME, running processes and network connections, then disk contents. Maintain a detailed chain of custody log documenting who handled the evidence, when, where, and why, with each transfer signed by both parties. Store evidence in tamper-evident containers and secure storage with restricted access. For cloud environments, preserve cloud-specific evidence including API logs, instance metadata, and snapshots before they are overwritten. All collection methods, tools used, and personnel involved should be documented to withstand legal scrutiny.",
    category: "Incident Response",
    difficulty: "MEDIUM" as const,
    tags: ["digital forensics", "evidence collection", "chain of custody", "imaging"],
  },
  {
    questionText: "Explain the difference between containment strategies and when to use each.",
    idealAnswer: "Containment strategies are categorized as short-term and long-term, with the choice depending on the incident type, severity, and business impact. Short-term containment aims to stop the immediate damage and includes isolating affected systems from the network (pulling network cables, disabling switch ports, or modifying firewall rules), blocking malicious IP addresses or domains at the perimeter, disabling compromised user accounts, and redirecting DNS to prevent C2 communication. Long-term containment involves more sustained measures such as implementing temporary firewall rules, setting up clean systems alongside compromised ones, applying emergency patches, and enhancing monitoring while preparing for full eradication. The decision to fully isolate versus partially contain depends on whether evidence preservation is needed (keeping systems running for forensics), the business criticality of affected systems, and whether the attacker's presence is fully mapped to avoid alerting them and triggering destructive actions.",
    category: "Incident Response",
    difficulty: "MEDIUM" as const,
    tags: ["containment", "isolation", "incident management", "strategy"],
  },
  {
    questionText: "How would you conduct a post-incident review and what should the resulting report include?",
    idealAnswer: "A post-incident review (lessons learned meeting) should be conducted within 1-2 weeks of incident closure while details are fresh, involving all team members who participated in the response plus relevant management. The meeting should objectively walk through the incident timeline, examining what happened, how it was detected, what actions were taken, and their effectiveness, in a blameless culture that focuses on process improvement. The resulting report should include an executive summary with business impact quantification, a detailed timeline of events from initial compromise through resolution, root cause analysis identifying the vulnerability or weakness exploited, an assessment of what worked well and what needs improvement in detection, response, and communication, specific actionable recommendations with assigned owners and deadlines, updates needed to incident response procedures, and metrics such as time to detect, time to contain, and time to remediate. The report should be distributed to appropriate stakeholders and recommendations tracked to completion.",
    category: "Incident Response",
    difficulty: "MEDIUM" as const,
    tags: ["lessons learned", "post-incident", "review", "improvement"],
  },
  {
    questionText: "What is a SOAR platform and how does it improve incident response efficiency?",
    idealAnswer: "SOAR (Security Orchestration, Automation, and Response) platforms integrate with security tools to automate and streamline incident response workflows. Orchestration connects disparate security tools (SIEM, EDR, firewalls, threat intelligence platforms, ticketing systems) through APIs, enabling coordinated actions across the security stack. Automation handles repetitive, time-consuming tasks such as enriching alerts with threat intelligence lookups, quarantining endpoints, blocking indicators across security controls, creating tickets, and sending notifications, reducing response time from hours to minutes. Response playbooks codify standard operating procedures for common incident types (phishing, malware, data breach) into executable workflows that guide analysts through investigation and remediation steps. SOAR platforms significantly reduce mean time to respond (MTTR), ensure consistent response quality regardless of analyst experience level, and free senior analysts to focus on complex investigations that require human judgment.",
    category: "Incident Response",
    difficulty: "MEDIUM" as const,
    tags: ["SOAR", "automation", "orchestration", "playbooks"],
  },
  {
    questionText: "Describe how you would handle a ransomware incident from detection through full recovery.",
    idealAnswer: "Upon ransomware detection, immediately isolate affected systems by disconnecting them from the network while preserving their state for forensics, and activate the incident response plan. Assess the scope by identifying all encrypted systems, the ransomware variant (using tools like ID Ransomware), and the infection vector. Preserve evidence by creating forensic images and memory captures before any remediation. Contain the spread by blocking the ransomware's C2 infrastructure, disabling compromised accounts, and segmenting unaffected network segments. During eradication, identify and remove all persistence mechanisms, backdoors, and the initial access point used by the attackers, recognizing that many ransomware operators maintain access for weeks before deploying encryption. For recovery, restore systems from verified clean backups that predate the compromise, rebuilding from scratch where possible rather than attempting to decrypt. Before reconnecting restored systems, ensure all vulnerabilities are patched and enhanced monitoring is in place. The decision to pay ransom should involve executive leadership, legal counsel, and law enforcement, with the understanding that payment does not guarantee data recovery and may fund criminal activity. Post-incident, implement measures to prevent recurrence including improved backup strategies, network segmentation, and EDR deployment.",
    category: "Incident Response",
    difficulty: "HARD" as const,
    tags: ["ransomware", "recovery", "containment", "business continuity"],
  },
  {
    questionText: "How would you investigate a suspected insider threat involving a privileged administrator exfiltrating sensitive data?",
    idealAnswer: "Insider threat investigations require extreme discretion and coordination with legal and HR before taking any technical actions that might alert the subject. Begin by quietly gathering evidence from multiple data sources: DLP alerts showing unusual data transfers, SIEM logs revealing off-hours access to sensitive databases or file shares, email gateway logs for large attachments or external forwarding rules, endpoint monitoring showing USB device connections or cloud storage uploads, VPN and badge access logs corroborating physical presence during suspicious activity, and database audit logs showing query patterns beyond normal job duties. Use UEBA analytics to establish the administrator's baseline behavior and quantify deviations. Correlate financial or HR indicators (upcoming departure, denied promotion) with technical anomalies. Implement enhanced covert monitoring with legal authorization, ensuring compliance with employee privacy laws and company policies. Preserve evidence chain of custody for potential legal proceedings. The response plan should be coordinated with HR and legal, with options ranging from supervised access restriction to immediate termination and potential law enforcement referral, depending on the severity and nature of the exfiltration.",
    category: "Incident Response",
    difficulty: "HARD" as const,
    tags: ["insider threat", "data exfiltration", "investigation", "DLP"],
  },
  {
    questionText: "Design an incident response automation strategy for a Security Operations Center handling 10,000+ alerts per day.",
    idealAnswer: "At scale, effective incident response requires a tiered automation architecture. Tier 0 (fully automated) handles the highest volume of alerts through SOAR playbooks that automatically enrich alerts with threat intelligence, correlate related events, and close confirmed false positives with documented reasoning, resolving approximately 70-80% of alerts without human intervention. Tier 1 (semi-automated) presents pre-enriched, contextualized alerts to junior analysts with guided investigation playbooks, pre-populated response actions, and one-click containment options, handling the next 15-20% of alerts. Tier 2 (analyst-driven) escalates complex, multi-stage incidents to senior analysts with automated timeline reconstruction, full forensic data collection, and cross-team collaboration tools. The architecture requires robust alert prioritization using risk scoring that considers asset criticality, threat intelligence confidence, vulnerability context, and user behavior baselines. Implement feedback loops where analyst decisions on alert disposition continuously improve automated classification models. Monitor SOC metrics including MTTR, alert-to-incident ratio, false positive rate, and automation coverage percentage, with regular playbook reviews to expand automation coverage. Deploy machine learning models for anomaly detection and alert clustering to identify campaigns that span multiple individual alerts.",
    category: "Incident Response",
    difficulty: "HARD" as const,
    tags: ["SOC", "automation", "SOAR", "scalability", "alert management"],
  },

  // ===== CRYPTOGRAPHY (10 questions: 3 EASY, 3 MEDIUM, 4 HARD) =====
  {
    questionText: "What is the difference between symmetric and asymmetric encryption?",
    idealAnswer: "Symmetric encryption uses the same key for both encryption and decryption, making it fast and efficient for encrypting large volumes of data, with common algorithms including AES, ChaCha20, and 3DES. Asymmetric encryption uses a mathematically related key pair: a public key for encryption and a private key for decryption, enabling secure communication without prior key exchange, with RSA and ECC being widely used algorithms. Symmetric encryption is significantly faster but requires a secure method to share the secret key between parties. In practice, most secure communication systems use a hybrid approach where asymmetric encryption securely exchanges a symmetric session key, which is then used for the actual data encryption, combining the security of asymmetric key exchange with the performance of symmetric encryption.",
    category: "Cryptography",
    difficulty: "EASY" as const,
    tags: ["symmetric", "asymmetric", "encryption", "AES", "RSA"],
  },
  {
    questionText: "What is hashing and how does it differ from encryption?",
    idealAnswer: "Hashing is a one-way mathematical function that converts input data of any size into a fixed-length output (hash or digest), and it is computationally infeasible to reverse the process to recover the original input. Encryption, by contrast, is a two-way process designed to be reversible using the appropriate key. Hashing is used for data integrity verification (ensuring files have not been tampered with), password storage (storing hashes rather than plaintext passwords), and digital signatures. Common hashing algorithms include SHA-256, SHA-3, and BLAKE2. A good hash function produces unique outputs for different inputs (collision resistance), generates vastly different outputs for similar inputs (avalanche effect), and is deterministic (same input always produces the same output). For password hashing, specialized algorithms like bcrypt, scrypt, or Argon2 include salting and computational cost parameters to resist brute force attacks.",
    category: "Cryptography",
    difficulty: "EASY" as const,
    tags: ["hashing", "SHA-256", "integrity", "password storage"],
  },
  {
    questionText: "What is a digital certificate and how does PKI work?",
    idealAnswer: "A digital certificate is an electronic document issued by a Certificate Authority (CA) that binds a public key to the identity of its owner, enabling trusted communication over the internet. Public Key Infrastructure (PKI) is the framework of policies, procedures, and technology that manages digital certificates and public key encryption. PKI works through a hierarchy of trust: root CAs sign intermediate CA certificates, which in turn sign end-entity certificates for servers and users. When a browser connects to an HTTPS website, it verifies the server's certificate by checking the digital signature chain back to a trusted root CA, confirming that the certificate has not expired or been revoked (via CRL or OCSP). PKI enables secure web browsing (HTTPS), email encryption (S/MIME), code signing, and mutual authentication in enterprise environments.",
    category: "Cryptography",
    difficulty: "EASY" as const,
    tags: ["PKI", "certificates", "CA", "digital certificates", "trust"],
  },
  {
    questionText: "Explain how key exchange protocols like Diffie-Hellman work and why they are important.",
    idealAnswer: "The Diffie-Hellman (DH) key exchange protocol allows two parties to establish a shared secret key over an insecure channel without transmitting the key itself. Each party generates a private value and computes a public value using modular exponentiation with agreed-upon parameters (a prime number and generator). They exchange public values and each independently computes the shared secret by combining the other's public value with their own private value, which is mathematically equivalent due to the properties of modular arithmetic. DH is important because it solves the key distribution problem that plagued symmetric cryptography. Modern implementations use Elliptic Curve Diffie-Hellman (ECDH) for better performance and smaller key sizes with equivalent security. Ephemeral DH (DHE/ECDHE) generates new key pairs for each session, providing forward secrecy, which ensures that compromise of long-term keys cannot decrypt previously captured traffic.",
    category: "Cryptography",
    difficulty: "MEDIUM" as const,
    tags: ["Diffie-Hellman", "key exchange", "ECDH", "forward secrecy"],
  },
  {
    questionText: "What are the common modes of operation for block ciphers and their security implications?",
    idealAnswer: "Block cipher modes define how a cipher processes data larger than its block size. ECB (Electronic Codebook) encrypts each block independently, which is insecure because identical plaintext blocks produce identical ciphertext blocks, revealing patterns. CBC (Cipher Block Chaining) XORs each plaintext block with the previous ciphertext block before encryption, requiring an IV and providing better security but being vulnerable to padding oracle attacks and not parallelizable for encryption. CTR (Counter) mode turns the block cipher into a stream cipher by encrypting sequential counter values and XORing with plaintext, allowing parallel processing and random access. GCM (Galois/Counter Mode) combines CTR mode encryption with Galois field authentication, providing both confidentiality and integrity (authenticated encryption) and is the recommended mode for AES in modern applications. The choice of mode significantly impacts security: always prefer authenticated encryption modes like GCM or CCM to prevent chosen-ciphertext attacks.",
    category: "Cryptography",
    difficulty: "MEDIUM" as const,
    tags: ["block cipher", "AES", "GCM", "CBC", "encryption modes"],
  },
  {
    questionText: "How do digital signatures work and what security properties do they provide?",
    idealAnswer: "Digital signatures use asymmetric cryptography to provide authentication, integrity, and non-repudiation for digital messages and documents. The signer creates a hash of the message and encrypts (signs) the hash with their private key to produce the signature. The recipient verifies the signature by decrypting it with the signer's public key and comparing the resulting hash with an independently computed hash of the received message. If they match, the signature is valid, confirming that the message was signed by the private key holder (authentication), has not been altered since signing (integrity), and the signer cannot deny having signed it (non-repudiation). Common signature algorithms include RSA-PSS, ECDSA, and EdDSA (Ed25519). Digital signatures are foundational to PKI, code signing, document signing, blockchain transactions, and secure software updates, with the security depending on the protection of the private key and the strength of the underlying hash function.",
    category: "Cryptography",
    difficulty: "MEDIUM" as const,
    tags: ["digital signatures", "non-repudiation", "ECDSA", "authentication"],
  },
  {
    questionText: "Explain the concept of post-quantum cryptography and why organizations should start preparing for it now.",
    idealAnswer: "Post-quantum cryptography (PQC) refers to cryptographic algorithms designed to be secure against attacks by both classical and quantum computers. Current asymmetric algorithms like RSA and ECC rely on mathematical problems (integer factorization, discrete logarithm) that quantum computers could efficiently solve using Shor's algorithm, potentially breaking all current public key cryptography. NIST has standardized several PQC algorithms including CRYSTALS-Kyber (ML-KEM) for key encapsulation and CRYSTALS-Dilithium (ML-DSA) for digital signatures, based on lattice problems believed to be quantum-resistant. Organizations should prepare now because of the 'harvest now, decrypt later' threat where adversaries capture encrypted data today for future quantum decryption, the long lead time required for cryptographic transitions in complex systems, and the need to maintain crypto-agility in system architectures. The transition involves inventorying cryptographic usage across all systems, testing PQC algorithm compatibility, implementing hybrid approaches that combine classical and PQC algorithms during the transition period, and updating protocols like TLS to support PQC key exchange.",
    category: "Cryptography",
    difficulty: "HARD" as const,
    tags: ["post-quantum", "PQC", "quantum computing", "NIST", "lattice"],
  },
  {
    questionText: "Describe how a side-channel attack on a cryptographic implementation works and how to mitigate it.",
    idealAnswer: "Side-channel attacks exploit information leaked through the physical implementation of a cryptographic system rather than mathematical weaknesses in the algorithm itself. Timing attacks measure how long cryptographic operations take, as execution time may vary based on the secret key value due to conditional branches or cache behavior. Power analysis attacks (SPA and DPA) monitor the electrical power consumption of a device during cryptographic operations to extract key material. Electromagnetic emanation attacks capture radiation patterns that correlate with internal processing. Cache timing attacks exploit CPU cache behavior in shared environments, as demonstrated by the Spectre and Meltdown vulnerabilities. Mitigation strategies include implementing constant-time algorithms that execute in the same time regardless of input values, using power-balanced logic gates in hardware implementations, adding random delays and dummy operations to mask power signatures, employing cache-partitioning techniques, and using dedicated cryptographic hardware modules (HSMs) that provide physical tamper resistance. Software mitigations should be validated through automated tools that detect timing variations in cryptographic code.",
    category: "Cryptography",
    difficulty: "HARD" as const,
    tags: ["side-channel", "timing attack", "power analysis", "implementation security"],
  },
  {
    questionText: "Explain how homomorphic encryption works and its practical applications in cybersecurity.",
    idealAnswer: "Homomorphic encryption (HE) allows computations to be performed directly on encrypted data without decrypting it first, with the result of the computation being the encrypted form of what the result would be if the operation were performed on the plaintext. Partially Homomorphic Encryption supports either addition (Paillier) or multiplication (RSA) on ciphertexts. Somewhat Homomorphic Encryption supports a limited number of both operations. Fully Homomorphic Encryption (FHE), pioneered by Craig Gentry in 2009, supports arbitrary computations on encrypted data. Practical cybersecurity applications include privacy-preserving cloud computing where sensitive data can be processed without the cloud provider seeing plaintext, secure multi-party computation for collaborative threat intelligence sharing without revealing raw data, encrypted database queries that return results without exposing the query or data, and privacy-compliant analytics on regulated data (healthcare, financial). Current limitations include significant computational overhead (orders of magnitude slower than plaintext operations), large ciphertext expansion, and complex key management, though recent advances with libraries like Microsoft SEAL and TFHE are making FHE increasingly practical for specific use cases.",
    category: "Cryptography",
    difficulty: "HARD" as const,
    tags: ["homomorphic encryption", "FHE", "privacy-preserving", "computation"],
  },
  {
    questionText: "How would you design a secure key management architecture for an enterprise handling multiple encryption use cases?",
    idealAnswer: "An enterprise key management architecture must address the complete key lifecycle across all use cases including data at rest, data in transit, application-level encryption, code signing, and certificate management. Deploy a centralized Key Management System (KMS) using hardware security modules (HSMs) that are FIPS 140-2 Level 3 certified for generating, storing, and managing master keys. Implement a key hierarchy with master keys in HSMs that encrypt data encryption keys (DEKs), which in turn encrypt the actual data, limiting exposure through envelope encryption. Define key lifecycle policies covering generation with sufficient entropy, distribution through secure channels, storage with access controls and audit logging, rotation on regular schedules and after suspected compromise, and destruction with crypto-shredding capabilities. Integrate with applications through standardized APIs (PKCS#11, KMIP) and cloud-native KMS services (AWS KMS, Azure Key Vault) for workloads in those environments. Implement separation of duties ensuring no single administrator can access both encrypted data and decryption keys, maintain comprehensive audit trails of all key operations, and establish disaster recovery procedures including secure key backup and escrow mechanisms with tested recovery procedures.",
    category: "Cryptography",
    difficulty: "HARD" as const,
    tags: ["key management", "HSM", "KMS", "encryption architecture", "lifecycle"],
  },
];
