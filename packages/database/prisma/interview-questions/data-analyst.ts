export const dataAnalystQuestions = [
  // ===== EASY (30 questions) =====
  {
    questionText: "What is the difference between a data analyst and a data scientist?",
    idealAnswer: "Data analysts focus on interpreting existing data using SQL, spreadsheets, and visualization tools to answer business questions and generate reports. Data scientists build predictive models using machine learning, programming, and statistical modeling. Analysts describe what happened; scientists predict what will happen. Both need SQL and analytical thinking, but scientists need deeper programming and ML skills.",
    category: "Career",
    difficulty: "EASY" as const,
    tags: ["career", "roles", "data-analysis"],
  },
  {
    questionText: "What is SQL and why is it essential for data analysts?",
    idealAnswer: "SQL (Structured Query Language) queries relational databases to extract, filter, aggregate, and join data. It's the most important skill for data analysts because most business data lives in relational databases. Key operations: SELECT, WHERE, JOIN, GROUP BY, ORDER BY, HAVING. Analysts use SQL daily for ad-hoc queries, building reports, and feeding data into visualization tools.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "databases", "fundamentals"],
  },
  {
    questionText: "What are the different types of SQL JOINs?",
    idealAnswer: "INNER JOIN returns matching rows from both tables. LEFT JOIN returns all rows from the left table plus matching rows from the right (NULL for non-matches). RIGHT JOIN is the reverse. FULL OUTER JOIN returns all rows from both tables. CROSS JOIN produces the Cartesian product. Use INNER for required relationships, LEFT for optional relationships (e.g., users who may or may not have orders).",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "joins", "databases"],
  },
  {
    questionText: "What is a KPI and how do you choose the right ones?",
    idealAnswer: "A KPI (Key Performance Indicator) is a measurable metric that reflects how effectively an organization is achieving its objectives. Good KPIs are: Specific, Measurable, Achievable, Relevant, Time-bound (SMART). Examples: monthly revenue growth, customer acquisition cost, churn rate, net promoter score. Choose KPIs that directly align with business goals and can be acted upon — vanity metrics (page views alone) are less useful than actionable ones (conversion rate).",
    category: "Business",
    difficulty: "EASY" as const,
    tags: ["kpi", "metrics", "business"],
  },
  {
    questionText: "What is data cleaning and why is it important?",
    idealAnswer: "Data cleaning fixes or removes inaccurate, incomplete, duplicate, or inconsistent data. Common tasks: handling missing values, removing duplicates, fixing data types, standardizing formats (dates, names), correcting typos, handling outliers. It's critical because analysis on dirty data produces unreliable results — garbage in, garbage out. Data cleaning typically takes 60-80% of an analyst's time.",
    category: "Data Preprocessing",
    difficulty: "EASY" as const,
    tags: ["data-cleaning", "quality", "preprocessing"],
  },
  {
    questionText: "What is the difference between COUNT, COUNT(*), and COUNT(DISTINCT)?",
    idealAnswer: "COUNT(*) counts all rows including NULLs. COUNT(column) counts non-NULL values in that column. COUNT(DISTINCT column) counts unique non-NULL values. Example: a table with 100 rows where 'email' has 5 NULLs and 10 duplicates — COUNT(*) = 100, COUNT(email) = 95, COUNT(DISTINCT email) = 85. Use DISTINCT when calculating unique customers, products, etc.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "aggregation", "count"],
  },
  {
    questionText: "What is a pivot table and when would you use one?",
    idealAnswer: "A pivot table summarizes data by reorganizing rows into columns, applying aggregations (sum, count, average). Use cases: sales by region and product, monthly revenue by department, survey responses by category. In Excel: Insert → PivotTable. In SQL: use CASE WHEN with GROUP BY or PIVOT function. In pandas: `df.pivot_table(values, index, columns, aggfunc)`. They're essential for cross-tabulation and quick exploratory analysis.",
    category: "Tools",
    difficulty: "EASY" as const,
    tags: ["pivot-table", "excel", "aggregation"],
  },
  {
    questionText: "What is a dashboard and what makes a good one?",
    idealAnswer: "A dashboard is a visual display of key metrics and data points. Good dashboards: focus on actionable KPIs, have clear hierarchy (most important metrics prominent), use appropriate chart types, update automatically, have consistent formatting, include filters for drilling down, and tell a story. Tools: Tableau, Power BI, Looker, Metabase. Avoid: too many metrics, decorative charts, misleading scales, lack of context.",
    category: "Visualization",
    difficulty: "EASY" as const,
    tags: ["dashboard", "visualization", "reporting"],
  },
  {
    questionText: "What is the GROUP BY clause in SQL?",
    idealAnswer: "GROUP BY groups rows sharing values in specified columns, then applies aggregate functions (COUNT, SUM, AVG, MIN, MAX) to each group. Example: `SELECT department, AVG(salary) FROM employees GROUP BY department` — returns average salary per department. Use HAVING to filter groups (like WHERE but for aggregates): `HAVING AVG(salary) > 50000`. Every non-aggregated column in SELECT must be in GROUP BY.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "group-by", "aggregation"],
  },
  {
    questionText: "What is the difference between a bar chart and a histogram?",
    idealAnswer: "A bar chart displays categorical data with discrete bars (gaps between them) — e.g., sales by product category. A histogram displays the distribution of continuous numerical data using adjacent bins (no gaps) — e.g., distribution of customer ages. Bar chart: categorical x-axis, bars can be reordered. Histogram: numerical x-axis, order matters, bin width affects interpretation.",
    category: "Visualization",
    difficulty: "EASY" as const,
    tags: ["visualization", "charts", "statistics"],
  },
  {
    questionText: "What is Excel VLOOKUP and when would you use it?",
    idealAnswer: "VLOOKUP searches for a value in the first column of a range and returns a value from a specified column. Syntax: `VLOOKUP(lookup_value, table_array, col_index, [range_lookup])`. Use for: looking up product prices by ID, matching employee names to departments. Limitations: only searches rightward, returns first match. Modern alternatives: XLOOKUP (bidirectional, handles errors), INDEX-MATCH (more flexible). In analysis: joining data from different sheets.",
    category: "Excel",
    difficulty: "EASY" as const,
    tags: ["excel", "vlookup", "functions"],
  },
  {
    questionText: "What is a primary key and a foreign key in a database?",
    idealAnswer: "A primary key uniquely identifies each row in a table — it's unique and not NULL (e.g., user_id). A foreign key references a primary key in another table, creating relationships (e.g., orders.user_id references users.id). Understanding keys is essential for writing JOINs. A composite key uses multiple columns together as the primary key. Foreign keys enforce referential integrity — you can't reference a non-existent record.",
    category: "Databases",
    difficulty: "EASY" as const,
    tags: ["databases", "keys", "relationships"],
  },
  {
    questionText: "What is the difference between WHERE and HAVING in SQL?",
    idealAnswer: "WHERE filters individual rows before grouping — it cannot use aggregate functions. HAVING filters groups after GROUP BY — it can use aggregates. Example: `SELECT dept, COUNT(*) FROM employees WHERE status = 'active' GROUP BY dept HAVING COUNT(*) > 5` — WHERE filters active employees first, then HAVING keeps only departments with more than 5 active employees.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "filtering", "having"],
  },
  {
    questionText: "What is a correlation coefficient and how do you interpret it?",
    idealAnswer: "The Pearson correlation coefficient (r) measures linear relationship strength between two variables, ranging from -1 to +1. r = 1: perfect positive correlation, r = -1: perfect negative, r = 0: no linear relationship. Interpretation: |r| > 0.7 strong, 0.3-0.7 moderate, < 0.3 weak. Important: correlation measures linear relationships only and doesn't imply causation. Spearman correlation handles non-linear monotonic relationships.",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["statistics", "correlation", "analysis"],
  },
  {
    questionText: "What are the common data types in SQL?",
    idealAnswer: "Numeric: INT (whole numbers), DECIMAL/NUMERIC (exact decimals for money), FLOAT (approximate decimals). Text: VARCHAR (variable-length text), TEXT (long text), CHAR (fixed-length). Date/Time: DATE, TIMESTAMP, TIME. Boolean: BOOLEAN (true/false). Other: JSON, UUID, ARRAY. Choose the right type: use INT for IDs, DECIMAL for currency (not FLOAT — precision issues), VARCHAR for names, TIMESTAMP for event times.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "data-types", "databases"],
  },
  {
    questionText: "What is data visualization and why is it important?",
    idealAnswer: "Data visualization represents data graphically to reveal patterns, trends, outliers, and relationships that are hard to see in raw numbers. It's important because humans process visual information faster than tables. Best practices: choose the right chart type, label axes clearly, avoid misleading scales, use color purposefully, keep it simple. Tools: Tableau, Power BI, matplotlib, Excel. Visualization is the primary way analysts communicate findings.",
    category: "Visualization",
    difficulty: "EASY" as const,
    tags: ["visualization", "communication", "charts"],
  },
  {
    questionText: "What is a subquery in SQL? Give an example.",
    idealAnswer: "A subquery is a query nested inside another query. Types: scalar (returns one value), row (one row), table (multiple rows). Example: `SELECT name FROM employees WHERE salary > (SELECT AVG(salary) FROM employees)` — finds employees earning above average. Subqueries can appear in WHERE, FROM (derived tables), and SELECT clauses. CTEs (WITH clause) are often clearer alternatives for complex subqueries.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "subquery", "queries"],
  },
  {
    questionText: "What is the difference between a line chart and an area chart?",
    idealAnswer: "Both show trends over time with connected data points. A line chart uses lines only — best for comparing multiple trends without visual clutter. An area chart fills the space below the line — emphasizes volume/magnitude. Stacked area charts show composition over time (parts of a whole). Use line charts for comparing trends, area charts when total volume matters. Avoid area charts with many overlapping series.",
    category: "Visualization",
    difficulty: "EASY" as const,
    tags: ["visualization", "charts", "trends"],
  },
  {
    questionText: "What is ETL and why is it important for data analysis?",
    idealAnswer: "ETL stands for Extract, Transform, Load: Extract data from sources (databases, APIs, files), Transform it (clean, aggregate, join, format), Load into a destination (data warehouse, dashboard). It's how raw data becomes analysis-ready. Analysts often do lightweight ETL with SQL or Python. Data engineers build automated ETL pipelines. Modern variant: ELT (load raw data first, transform in the warehouse).",
    category: "Data Engineering",
    difficulty: "EASY" as const,
    tags: ["etl", "data-pipeline", "data-engineering"],
  },
  {
    questionText: "What is a NULL value in SQL and how do you handle it?",
    idealAnswer: "NULL represents missing or unknown data — it's not zero or empty string. NULL comparisons are tricky: `NULL = NULL` is NOT true (use `IS NULL`). NULLs propagate: `5 + NULL = NULL`. Handle with: COALESCE(column, default_value), IS NULL/IS NOT NULL in WHERE, NULLIF(), CASE WHEN. Aggregates ignore NULLs (except COUNT(*)). Always consider NULLs when writing queries — they're a common source of bugs.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "null", "data-quality"],
  },
  {
    questionText: "What is A/B testing from a data analyst's perspective?",
    idealAnswer: "A/B testing compares two versions (control vs treatment) to measure impact of a change. Analyst's role: define success metrics, calculate required sample size, monitor test execution, analyze results (statistical significance, practical significance), and present findings. Key: ensure random assignment, sufficient sample size, no peeking during the test, and check for segment-level effects. Report confidence intervals, not just p-values.",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["ab-testing", "statistics", "experimentation"],
  },
  {
    questionText: "What is the ORDER BY clause and how does it work with multiple columns?",
    idealAnswer: "ORDER BY sorts query results. Default is ascending (ASC); use DESC for descending. Multiple columns: `ORDER BY department ASC, salary DESC` — sorts by department first, then by salary within each department (highest first). NULLs sort first in ASC (PostgreSQL default). Use with LIMIT for top-N queries: `ORDER BY revenue DESC LIMIT 10` for top 10 by revenue.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "sorting", "order-by"],
  },
  {
    questionText: "What is a data warehouse and how does it differ from a regular database?",
    idealAnswer: "A data warehouse is optimized for analytical queries (OLAP) — reads, aggregations, historical analysis. Regular databases (OLTP) are optimized for transactional operations — reads/writes for applications. Warehouses: denormalized (star/snowflake schema), columnar storage, batch-loaded. Databases: normalized, row-based, real-time. Tools: Snowflake, BigQuery, Redshift. Analysts query the warehouse; applications use the database.",
    category: "Data Engineering",
    difficulty: "EASY" as const,
    tags: ["data-warehouse", "olap", "analytics"],
  },
  {
    questionText: "What is the difference between UNION and UNION ALL?",
    idealAnswer: "UNION combines results from two SELECT statements, removing duplicates (slower). UNION ALL combines all results including duplicates (faster). Use UNION ALL when: you know there are no duplicates, duplicates are acceptable, or performance matters. Both require the same number of columns with compatible data types. Example: combining sales from two regions: `SELECT * FROM sales_east UNION ALL SELECT * FROM sales_west`.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "union", "set-operations"],
  },
  {
    questionText: "What are common Excel functions every analyst should know?",
    idealAnswer: "Essential functions: SUMIFS/COUNTIFS (conditional aggregation), VLOOKUP/XLOOKUP (data lookup), INDEX-MATCH (flexible lookup), IF/IFS (conditional logic), TEXT/DATE functions (formatting), LEFT/RIGHT/MID (text extraction), CONCATENATE/TEXTJOIN (combining text), IFERROR (error handling), UNIQUE/FILTER (dynamic arrays). Also: pivot tables, conditional formatting, data validation, named ranges. Excel is often the first tool for quick analysis before SQL.",
    category: "Excel",
    difficulty: "EASY" as const,
    tags: ["excel", "functions", "tools"],
  },
  {
    questionText: "What is a box plot and what information does it convey?",
    idealAnswer: "A box plot displays the distribution of numerical data. The box spans Q1 (25th percentile) to Q3 (75th percentile) with a line at the median (Q2). Whiskers extend to 1.5×IQR from the box. Points beyond whiskers are outliers. It shows: central tendency (median), spread (IQR), skewness (median position within box), and outliers. Best for comparing distributions across categories.",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["statistics", "visualization", "distribution"],
  },
  {
    questionText: "What is a CTE (Common Table Expression) in SQL?",
    idealAnswer: "A CTE is a temporary named result set defined with the WITH clause, making complex queries more readable. Example: `WITH monthly_sales AS (SELECT month, SUM(amount) AS total FROM sales GROUP BY month) SELECT * FROM monthly_sales WHERE total > 10000`. CTEs can reference each other and be recursive. They improve readability over nested subqueries and are evaluated once per query. Most modern databases support CTEs.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "cte", "queries"],
  },
  {
    questionText: "What is the difference between qualitative and quantitative data?",
    idealAnswer: "Quantitative data is numerical and measurable — can be discrete (counts: number of orders) or continuous (measurements: revenue, temperature). Analyzed with statistical methods (mean, regression). Qualitative data is categorical/descriptive — nominal (no order: colors, categories) or ordinal (ordered: ratings, education level). Analyzed with frequency counts, mode, chi-squared tests. Understanding the type determines which analysis methods and visualizations to use.",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["data-types", "statistics", "fundamentals"],
  },
  {
    questionText: "What is a scatter plot and when should you use it?",
    idealAnswer: "A scatter plot displays the relationship between two numerical variables as points on x-y axes. Use it to: identify correlations, spot clusters, detect outliers, and assess the nature of relationships (linear, non-linear). Add a trend line for regression. Color or size points by a third variable for additional dimensions. Not suitable for: categorical data, large datasets (overplotting — use density plots instead).",
    category: "Visualization",
    difficulty: "EASY" as const,
    tags: ["visualization", "scatter-plot", "correlation"],
  },
  {
    questionText: "What is data integrity and how do you ensure it?",
    idealAnswer: "Data integrity means data is accurate, consistent, and reliable throughout its lifecycle. Ensure it through: database constraints (primary keys, foreign keys, NOT NULL, UNIQUE, CHECK), input validation, automated data quality checks, audit trails, access controls, regular data profiling, referential integrity enforcement. Types: entity integrity (unique PKs), referential integrity (valid FKs), domain integrity (valid values). Compromised integrity leads to wrong decisions.",
    category: "Data Quality",
    difficulty: "EASY" as const,
    tags: ["data-integrity", "quality", "databases"],
  },

  // ===== MEDIUM (40 questions) =====
  {
    questionText: "Explain SQL window functions and give three practical examples.",
    idealAnswer: "Window functions compute values across a set of rows related to the current row without collapsing groups. Examples: (1) `ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC)` — rank employees by salary within each department, (2) `SUM(revenue) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)` — running total, (3) `LAG(sales, 1) OVER (PARTITION BY product ORDER BY month)` — previous month's sales for calculating month-over-month change. Window functions are essential for rankings, running aggregates, and period-over-period comparisons.",
    category: "SQL",
    difficulty: "MEDIUM" as const,
    tags: ["sql", "window-functions", "analytics"],
  },
  {
    questionText: "How would you investigate a sudden drop in a key business metric?",
    idealAnswer: "Framework: (1) Validate data: check for data pipeline issues, missing data, schema changes, (2) Determine scope: is it all segments or specific ones? (segment by: region, platform, user type, product), (3) Timeline: when exactly did it start? Did it correlate with deployments, marketing changes, or external events? (4) Decompose: break the metric into components (revenue = users × conversion × AOV), find which component dropped, (5) Check related metrics for context, (6) Form hypotheses, validate with data, (7) Present findings with clear evidence and recommended actions.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["root-cause-analysis", "debugging", "metrics"],
  },
  {
    questionText: "What is cohort analysis and how do you perform one?",
    idealAnswer: "Cohort analysis groups users by a shared characteristic (usually sign-up date) and tracks their behavior over time. Steps: (1) Define cohorts (January signups, February signups), (2) Define metric (retention, revenue, engagement), (3) Build a matrix: cohorts as rows, time periods as columns, (4) Visualize as a retention triangle/heatmap. SQL: join users by signup month with activity table, pivot by period offset. Reveals: are newer cohorts performing better? When do users typically churn? It separates growth effects from retention improvements.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["cohort-analysis", "retention", "analytics"],
  },
  {
    questionText: "How do you handle duplicate data in a dataset?",
    idealAnswer: "Steps: (1) Identify duplicates: exact duplicates (all columns match) or fuzzy duplicates (similar but not identical), (2) Understand why: data entry errors, system bugs, multiple data sources, (3) Define duplicate criteria (which columns determine uniqueness), (4) Choose strategy: keep first/last occurrence, keep the most complete record, merge fields. SQL: `ROW_NUMBER() OVER (PARTITION BY email ORDER BY updated_at DESC)` then keep row_number = 1. In pandas: `df.drop_duplicates(subset=['email'], keep='last')`. Document the deduplication logic.",
    category: "Data Preprocessing",
    difficulty: "MEDIUM" as const,
    tags: ["deduplication", "data-cleaning", "sql"],
  },
  {
    questionText: "Explain the concept of funnel analysis. How would you build one?",
    idealAnswer: "Funnel analysis tracks users through a sequence of steps to identify where they drop off. Example e-commerce funnel: visit → product view → add to cart → checkout → purchase. Build: (1) Define steps and their order, (2) Query each step's count, (3) Calculate conversion rates between steps, (4) Segment by dimensions (device, source, user type) to find insights. SQL: count distinct users at each step, join on user_id. Key insights: biggest drop-off points, segment differences, time between steps. Visualization: funnel chart or bar chart with conversion percentages.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["funnel-analysis", "conversion", "analytics"],
  },
  {
    questionText: "What is the difference between a star schema and a snowflake schema?",
    idealAnswer: "Both are data warehouse models. Star schema: central fact table (transactions, events) surrounded by dimension tables (customer, product, date) — denormalized dimensions for faster queries. Snowflake schema: dimension tables are further normalized into sub-dimensions (product → product category → department). Star is simpler, faster for queries, and most common. Snowflake saves storage and avoids redundancy but requires more JOINs. Most modern warehouses prefer star schema.",
    category: "Data Modeling",
    difficulty: "MEDIUM" as const,
    tags: ["star-schema", "data-warehouse", "modeling"],
  },
  {
    questionText: "How do you calculate and interpret year-over-year (YoY) growth?",
    idealAnswer: "YoY growth = (Current Period - Same Period Last Year) / Same Period Last Year × 100%. In SQL: use LAG with PARTITION BY month or self-join. Example: `LAG(revenue, 12) OVER (ORDER BY month)` for monthly data. Interpretation: accounts for seasonality (comparing same month/quarter). Caveats: one-off events can distort, consider using a rolling average. Also calculate: month-over-month (short-term trends), compound annual growth rate (CAGR) for multi-year performance.",
    category: "Metrics",
    difficulty: "MEDIUM" as const,
    tags: ["metrics", "growth", "analytics"],
  },
  {
    questionText: "How would you design a customer segmentation analysis?",
    idealAnswer: "Approaches: (1) RFM analysis: Recency (last purchase), Frequency (number of purchases), Monetary (total spend) — score and segment into groups (champions, at-risk, lost), (2) Behavioral clustering: K-means on normalized features, (3) Demographic segmentation: age, location, industry. Steps: define features, handle outliers, normalize, apply method, profile segments, validate with stakeholders. SQL: calculate RFM scores with NTILE() window function. Actionable segments enable targeted marketing, pricing, and product decisions.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["segmentation", "rfm", "clustering"],
  },
  {
    questionText: "What are the best practices for writing efficient SQL queries?",
    idealAnswer: "Practices: (1) SELECT only needed columns (not SELECT *), (2) Use WHERE to filter early, (3) Index columns used in WHERE, JOIN, ORDER BY, (4) Avoid functions on indexed columns in WHERE (prevents index use), (5) Use EXISTS instead of IN for large subqueries, (6) LIMIT results during development, (7) Use EXPLAIN to check query plans, (8) Avoid correlated subqueries (use JOINs or CTEs), (9) Use appropriate data types, (10) Pre-aggregate in CTEs when joining to reduce row counts. Profile before optimizing.",
    category: "SQL",
    difficulty: "MEDIUM" as const,
    tags: ["sql", "optimization", "performance"],
  },
  {
    questionText: "How do you present data findings to non-technical stakeholders?",
    idealAnswer: "Framework: (1) Start with the bottom line — lead with insights and recommendations, not methodology, (2) Use clear visualizations with titles that state the finding (not just 'Revenue Chart'), (3) Translate metrics to business language (revenue impact, customer impact), (4) Show comparisons (vs last year, vs target, vs benchmark), (5) Tell a story: context → problem → finding → action, (6) Anticipate questions, (7) Include confidence levels and caveats. Avoid: jargon, excessive detail, burying conclusions in appendices.",
    category: "Communication",
    difficulty: "MEDIUM" as const,
    tags: ["communication", "presentation", "stakeholders"],
  },
  {
    questionText: "What is statistical significance and how do you determine it?",
    idealAnswer: "Statistical significance means the observed result is unlikely due to random chance. Determination: (1) Set significance level α (typically 0.05), (2) Choose appropriate test (t-test, chi-squared, z-test), (3) Calculate test statistic and p-value, (4) If p < α, the result is statistically significant. Important: statistical significance ≠ practical significance. A tiny effect can be significant with a large sample. Always report effect size and confidence intervals alongside p-values. Consider multiple testing corrections (Bonferroni) when running many tests.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["statistics", "significance", "hypothesis-testing"],
  },
  {
    questionText: "Explain the concept of customer lifetime value (CLV) and how to calculate it.",
    idealAnswer: "CLV is the total revenue a customer generates over their entire relationship. Simple: CLV = Average Purchase Value × Purchase Frequency × Customer Lifespan. More sophisticated: sum of discounted future cash flows. Cohort-based: track actual revenue per cohort over time. Predictive: use BG/NBD model for purchase frequency + Gamma-Gamma for monetary value. CLV informs: acquisition budget (spend up to CLV), segmentation (high-value vs low-value), retention investment decisions.",
    category: "Metrics",
    difficulty: "MEDIUM" as const,
    tags: ["clv", "metrics", "customer-analytics"],
  },
  {
    questionText: "How do you handle messy date data in analysis?",
    idealAnswer: "Common issues and fixes: (1) Inconsistent formats (MM/DD vs DD/MM): standardize to ISO 8601 (YYYY-MM-DD), (2) Timezone mismatches: convert to UTC or the business's timezone, (3) Invalid dates: validate ranges, check for default values (1970-01-01, 9999-12-31), (4) Date vs datetime confusion: extract date component when time doesn't matter, (5) Missing dates: interpolation or forward-fill for time series. SQL functions: CAST, DATE_TRUNC, EXTRACT, DATE_DIFF. Always document timezone assumptions.",
    category: "Data Preprocessing",
    difficulty: "MEDIUM" as const,
    tags: ["dates", "data-cleaning", "sql"],
  },
  {
    questionText: "What is the difference between OLAP and OLTP databases?",
    idealAnswer: "OLTP (Online Transaction Processing): handles real-time transactions — inserts, updates, lookups. Normalized schema, row-oriented, optimized for write speed. Examples: PostgreSQL, MySQL. OLAP (Online Analytical Processing): handles complex queries over large datasets — aggregations, historical analysis. Denormalized, often columnar, optimized for read speed. Examples: Snowflake, BigQuery, Redshift. Analysts query OLAP (data warehouse); applications use OLTP (operational database).",
    category: "Data Engineering",
    difficulty: "MEDIUM" as const,
    tags: ["olap", "oltp", "databases"],
  },
  {
    questionText: "How would you analyze the effectiveness of a marketing campaign?",
    idealAnswer: "Framework: (1) Define success metrics before the campaign (conversion rate, ROI, customer acquisition cost), (2) Establish a baseline (pre-campaign performance), (3) Set up tracking (UTM parameters, attribution), (4) Compare treatment vs control (A/B test if possible), (5) Measure: impressions → clicks → conversions → revenue, (6) Calculate ROI = (Revenue - Cost) / Cost × 100%, (7) Segment results by channel, audience, creative, (8) Account for external factors (seasonality, competitors). Attribution models: first-touch, last-touch, multi-touch.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["marketing-analytics", "campaign", "roi"],
  },
  {
    questionText: "What is Tableau and how does it compare to Power BI?",
    idealAnswer: "Both are leading BI visualization tools. Tableau: more flexible visualizations, stronger in exploratory analysis, larger community, better for complex custom charts. Power BI: tighter Microsoft integration (Excel, Azure), more affordable, built-in DAX for calculations, better for enterprise reporting. Both support: drag-and-drop dashboards, live data connections, calculated fields, sharing. Choose based on existing tech stack and budget. Alternatives: Looker (SQL-based), Metabase (open-source).",
    category: "Tools",
    difficulty: "MEDIUM" as const,
    tags: ["tableau", "power-bi", "visualization"],
  },
  {
    questionText: "How do you perform a retention analysis?",
    idealAnswer: "Steps: (1) Define retention event (login, purchase, feature use), (2) Define time periods (day, week, month), (3) Group users by cohort (signup date), (4) For each cohort, calculate % who performed the event in subsequent periods. SQL: join users by signup_month with events, count distinct users per period offset, divide by cohort size. Visualize as a retention curve (% vs time) or retention matrix (heatmap). Key metrics: Day 1/7/30 retention, median lifetime. Compare cohorts to measure product improvements.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["retention", "cohort", "analytics"],
  },
  {
    questionText: "Explain the difference between RANK, DENSE_RANK, and ROW_NUMBER in SQL.",
    idealAnswer: "For values [100, 100, 90, 80]: ROW_NUMBER assigns unique sequential numbers: 1, 2, 3, 4. RANK assigns same rank to ties with gaps: 1, 1, 3, 4. DENSE_RANK assigns same rank to ties without gaps: 1, 1, 2, 3. Use ROW_NUMBER for deduplication (pick one per group), RANK for competition-style ranking, DENSE_RANK when you need consecutive ranks. All use OVER (ORDER BY ...) clause.",
    category: "SQL",
    difficulty: "MEDIUM" as const,
    tags: ["sql", "window-functions", "ranking"],
  },
  {
    questionText: "How would you build a churn analysis?",
    idealAnswer: "Steps: (1) Define churn (no activity in X days, subscription cancelled), (2) Calculate churn rate by period and segment, (3) Identify leading indicators: decreased usage, support tickets, missed payments, (4) Build a churn prediction model (logistic regression with usage features), (5) Segment churned users to understand patterns, (6) Calculate revenue impact of churn, (7) Recommend retention strategies per segment. SQL: flag users whose last activity was > 30 days ago. Track: churn rate over time, churn by cohort, reasons for churn.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["churn", "retention", "analytics"],
  },
  {
    questionText: "What is a confidence interval and how do you interpret it?",
    idealAnswer: "A confidence interval is a range of values that likely contains the true population parameter. A 95% CI means if you repeated the study many times, 95% of the intervals would contain the true value. Example: 'Average order value is $45 with a 95% CI of [$42, $48].' Narrower CI = more precise (larger sample or less variance). Report CIs alongside point estimates for honest analysis. CIs on differences help assess practical significance.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["confidence-interval", "statistics", "inference"],
  },
  {
    questionText: "How do you create an effective data model for a reporting database?",
    idealAnswer: "Steps: (1) Identify business processes to model (sales, marketing, support), (2) Determine the grain (one row per transaction/event/day), (3) Design star schema: fact tables (measurable events with foreign keys) and dimension tables (descriptive attributes), (4) Add slowly changing dimensions for historical accuracy (Type 2: new row per change), (5) Pre-aggregate for common queries, (6) Document with a data dictionary. Principles: model for query patterns, denormalize for performance, use consistent naming, include audit columns (loaded_at).",
    category: "Data Modeling",
    difficulty: "MEDIUM" as const,
    tags: ["data-modeling", "star-schema", "warehouse"],
  },
  {
    questionText: "What is regression analysis and when would you use it?",
    idealAnswer: "Regression models the relationship between a dependent variable and one or more independent variables. Linear regression: Y = β₀ + β₁X₁ + ... + ε. Use cases: predicting sales from ad spend, understanding factor impacts (how much does price affect demand?), forecasting. Evaluate with: R² (explained variance), p-values for coefficients, residual analysis. Assumptions: linearity, independence, normality of residuals, homoscedasticity. For non-linear relationships, consider polynomial or log transforms.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["regression", "statistics", "modeling"],
  },
  {
    questionText: "How do you handle time zones in data analysis?",
    idealAnswer: "Best practices: (1) Store all timestamps in UTC in the database, (2) Convert to business timezone for reporting, (3) Be explicit about timezone in column names (created_at_utc), (4) Use timezone-aware datetime types, (5) Handle daylight saving time transitions carefully, (6) When joining data from multiple sources, verify they're in the same timezone. SQL: `AT TIME ZONE 'America/New_York'`. Common bugs: comparing timestamps in different timezones, incorrect day boundaries for daily aggregations.",
    category: "Data Preprocessing",
    difficulty: "MEDIUM" as const,
    tags: ["timezones", "data-cleaning", "dates"],
  },
  {
    questionText: "Explain the concept of sampling and when an analyst might need it.",
    idealAnswer: "Sampling selects a subset of data that represents the population. When needed: very large datasets (too slow to query full data), quick exploratory analysis, statistical testing. Types: simple random (equal chance for each row), stratified (proportional from subgroups), systematic (every nth record). Ensure sample is large enough (sample size calculations) and representative. In SQL: `ORDER BY RANDOM() LIMIT 10000` or `TABLESAMPLE` (PostgreSQL). Always note when analysis was done on a sample.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["sampling", "statistics", "methodology"],
  },
  {
    questionText: "How would you build a product usage analytics dashboard?",
    idealAnswer: "Key metrics: (1) Engagement: DAU/MAU ratio, session duration, feature adoption, (2) Growth: new users, activation rate, referrals, (3) Retention: D1/D7/D30 retention, churn rate, (4) Conversion: trial-to-paid, upgrade rate. Dashboard design: summary KPIs at top, trend lines for key metrics, cohort retention chart, feature usage heatmap, user segmentation breakdown. Add filters: date range, user segment, platform. Data source: event tracking system (Mixpanel, Amplitude) or warehouse. Update daily, review weekly.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["product-analytics", "dashboard", "metrics"],
  },
  {
    questionText: "What is data profiling and how do you perform it?",
    idealAnswer: "Data profiling examines data to understand its structure, quality, and content. Steps: (1) Row count and column types, (2) Null percentage per column, (3) Unique value counts and cardinality, (4) Min, max, mean, median, standard deviation for numerics, (5) Value distribution and top values for categoricals, (6) Pattern validation (email formats, phone numbers), (7) Referential integrity checks. Tools: pandas `df.describe()`, `df.info()`, Great Expectations. Profile data before any analysis to understand what you're working with.",
    category: "Data Quality",
    difficulty: "MEDIUM" as const,
    tags: ["data-profiling", "quality", "eda"],
  },
  {
    questionText: "How do you design an effective A/B test for a product feature?",
    idealAnswer: "Steps: (1) Define hypothesis and primary metric (conversion rate, engagement), (2) Determine minimum detectable effect based on business significance, (3) Calculate sample size (power analysis: 80% power, 5% significance), (4) Decide on randomization unit (user, session, page), (5) Run for predetermined duration (at least 1-2 business cycles), (6) Don't peek at results during the test, (7) Analyze: check for sample ratio mismatch, compute p-value and confidence interval, check guardrail metrics, segment analysis. Report practical significance alongside statistical significance.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["ab-testing", "experimentation", "statistics"],
  },
  {
    questionText: "What are the different types of data visualizations and when to use each?",
    idealAnswer: "Comparison: bar chart (categories), grouped bar (compare across groups). Trend: line chart (over time), area chart (magnitude over time). Distribution: histogram (single variable), box plot (compare distributions). Relationship: scatter plot (two variables), bubble chart (three variables). Composition: pie chart (simple parts-of-whole), stacked bar (composition over categories), treemap (hierarchical). Avoid: 3D charts, dual-axis charts (misleading), pie charts with many slices. Match chart type to the question being answered.",
    category: "Visualization",
    difficulty: "MEDIUM" as const,
    tags: ["visualization", "chart-types", "best-practices"],
  },
  {
    questionText: "How would you analyze pricing data to recommend optimal pricing?",
    idealAnswer: "Approaches: (1) Price sensitivity analysis: survey-based (Van Westendorp, Gabor-Granger) or behavioral data, (2) Price elasticity: measure demand change as price changes (elasticity = % change in demand / % change in price), (3) Competitor benchmarking: compare pricing tiers and features, (4) Segmented pricing analysis: willingness to pay by segment, (5) Revenue optimization: model revenue = price × quantity at various price points. SQL: analyze conversion rates at different price points, A/B test pricing changes, consider LTV impact not just initial conversion.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["pricing", "analytics", "optimization"],
  },
  {
    questionText: "What is the CASE WHEN statement in SQL and how is it used?",
    idealAnswer: "CASE WHEN applies conditional logic in SQL queries. Syntax: `CASE WHEN condition THEN result WHEN condition2 THEN result2 ELSE default END`. Use cases: (1) Creating categories: `CASE WHEN age < 18 THEN 'Minor' WHEN age < 65 THEN 'Adult' ELSE 'Senior' END`, (2) Conditional aggregation: `SUM(CASE WHEN status = 'completed' THEN amount ELSE 0 END)`, (3) Pivoting data, (4) Custom sorting. It's one of the most versatile SQL constructs for data transformation and analysis.",
    category: "SQL",
    difficulty: "MEDIUM" as const,
    tags: ["sql", "case-when", "conditional"],
  },
  {
    questionText: "How do you ensure data quality in your analyses?",
    idealAnswer: "Checklist: (1) Profile data first (nulls, ranges, distributions), (2) Validate against known totals (reconciliation), (3) Check for duplicates, (4) Verify joins don't fan out (row count before/after), (5) Sanity check results against expectations, (6) Cross-reference with other data sources, (7) Document assumptions and limitations, (8) Peer review queries and findings, (9) Automate data quality tests for recurring reports, (10) Version control SQL queries. A wrong analysis is worse than no analysis.",
    category: "Data Quality",
    difficulty: "MEDIUM" as const,
    tags: ["data-quality", "validation", "best-practices"],
  },
  {
    questionText: "How would you perform market basket analysis?",
    idealAnswer: "Market basket analysis identifies products frequently bought together. Method: (1) Association rules mining (Apriori algorithm), (2) Calculate metrics: Support (% of transactions containing both items), Confidence (P(B|A) — given A was bought, probability of buying B), Lift (confidence / expected probability — lift > 1 means positive association), (3) Filter by minimum support and confidence thresholds, (4) Actionable outputs: cross-selling recommendations, store layout, bundle promotions. SQL: self-join transactions on transaction_id to find co-occurring items.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["market-basket", "association-rules", "analytics"],
  },
  {
    questionText: "What is Python pandas and how does it complement SQL for analysis?",
    idealAnswer: "Pandas provides DataFrames for data manipulation in Python. Complements SQL by: handling data SQL can't (web scraping results, API responses, Excel files), complex transformations (string manipulation, custom functions), statistical modeling, visualization integration (matplotlib/seaborn). Workflow: extract with SQL, transform and analyze in pandas. Key operations: merge, groupby, pivot_table, apply, rolling windows. Use SQL for heavy data extraction and aggregation; pandas for detailed analysis, modeling, and visualization.",
    category: "Python",
    difficulty: "MEDIUM" as const,
    tags: ["pandas", "python", "sql"],
  },
  {
    questionText: "How do you analyze user engagement metrics?",
    idealAnswer: "Key metrics: (1) DAU/WAU/MAU (daily/weekly/monthly active users), (2) DAU/MAU ratio (stickiness), (3) Session frequency and duration, (4) Feature adoption (% users using feature X), (5) Engagement depth (actions per session). Analysis: trend over time, segment by user type, cohort by signup date, compare across platforms. Advanced: engagement scoring (combine metrics into a single score), identify power users and their behaviors, predict churn from declining engagement. Visualize with time series charts and user segmentation breakdowns.",
    category: "Analysis",
    difficulty: "MEDIUM" as const,
    tags: ["engagement", "product-analytics", "metrics"],
  },
  {
    questionText: "What is the difference between correlation and regression?",
    idealAnswer: "Correlation measures the strength and direction of the linear relationship between two variables (r value, symmetric — correlation of X with Y = correlation of Y with X). Regression models the dependency of one variable on another(s) to make predictions (Y = f(X), asymmetric — predicting Y from X ≠ predicting X from Y). Correlation tells you 'how related'; regression tells you 'how much Y changes per unit X' and enables prediction. Both assume linear relationships by default.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["correlation", "regression", "statistics"],
  },
  {
    questionText: "How would you build an automated reporting system?",
    idealAnswer: "Components: (1) Data source: connect to data warehouse with SQL queries, (2) Scheduling: cron jobs, Airflow, or BI tool scheduler, (3) Transformation: SQL scripts or dbt models for metric calculations, (4) Visualization: Tableau/Power BI dashboards or programmatic reports (Python + Jinja templates), (5) Distribution: email, Slack, or BI tool sharing, (6) Monitoring: alerts when metrics breach thresholds, (7) Documentation: metric definitions and data sources. Best practices: parameterize date ranges, log run status, handle failures gracefully, version control queries.",
    category: "Tools",
    difficulty: "MEDIUM" as const,
    tags: ["reporting", "automation", "analytics"],
  },

  // ===== HARD (30 questions) =====
  {
    questionText: "Design an analytics framework for measuring the health of a two-sided marketplace.",
    idealAnswer: "Framework: Supply side (sellers): active listings, listing quality scores, supply growth rate, seller churn. Demand side (buyers): search-to-purchase rate, buyer acquisition cost, repeat purchase rate. Matching: search relevance, time-to-purchase, match rate. Liquidity: % of listings that sell, % of searches that result in purchase. Balance: supply/demand ratio by category/region. Financial: GMV, take rate, unit economics. Monitor: cross-side network effects (more sellers → more buyers → more sellers). Segment all metrics by category, region, user type. Key: both sides must be healthy for the marketplace to thrive.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["marketplace", "metrics", "framework"],
  },
  {
    questionText: "How would you design an attribution model for marketing spend across channels?",
    idealAnswer: "Models: (1) Last-touch: all credit to final interaction (simple, biased toward lower-funnel), (2) First-touch: credit to awareness channel, (3) Linear: equal credit to all touchpoints, (4) Time-decay: more credit to recent interactions, (5) Position-based: 40% first, 40% last, 20% middle, (6) Data-driven (Markov chains or Shapley values): calculate each channel's incremental contribution. Implementation: collect touchpoint data with UTM parameters, build user journeys, apply model. Evaluate with incrementality tests (holdout experiments). Most important: use attribution to optimize channel spend, not as absolute truth.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["attribution", "marketing-analytics", "modeling"],
  },
  {
    questionText: "How would you analyze the impact of a pricing change on revenue and customer behavior?",
    idealAnswer: "Analysis: (1) Pre/post comparison with control group (A/B test if possible), (2) Measure: conversion rate, ARPU, churn rate, LTV, upgrade/downgrade patterns, (3) Price elasticity: % change in quantity / % change in price, (4) Revenue decomposition: revenue = users × conversion × price, identify which changed, (5) Segment impact: new vs existing, by tier, by geography, (6) Long-term effects: monitor churn over 3-6 months (delayed impact), (7) Competitive response. Causal inference: difference-in-differences if no A/B test. Present: short-term revenue impact, projected long-term LTV impact, and recommendations.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["pricing", "causal-analysis", "revenue"],
  },
  {
    questionText: "Design a comprehensive data quality monitoring system for a data warehouse.",
    idealAnswer: "Components: (1) Freshness: data arrives on schedule (alert on delay), (2) Volume: row counts within expected range (±20% of historical), (3) Schema: no unexpected column changes, (4) Completeness: null rates per column vs baseline, (5) Distribution: statistical tests (KS test) against rolling baseline, (6) Business rules: domain-specific assertions (revenue > 0, dates in range), (7) Referential integrity: all FK references valid, (8) Cross-source reconciliation: totals match between systems. Implementation: Great Expectations or dbt tests. Run after each pipeline stage. Dashboard for monitoring, PagerDuty for critical failures. Classify checks as critical (block downstream) or warning (alert only).",
    category: "Data Engineering",
    difficulty: "HARD" as const,
    tags: ["data-quality", "monitoring", "warehouse"],
  },
  {
    questionText: "How would you build a customer health score for a B2B SaaS product?",
    idealAnswer: "Components: (1) Product engagement: DAU/MAU, feature adoption, depth of usage, (2) Relationship: NPS/CSAT scores, support ticket frequency, executive sponsor engagement, (3) Financial: payment timeliness, expansion/contraction signals, contract status, (4) Onboarding: time-to-value, milestone completion. Methodology: (1) Normalize each component to 0-100, (2) Weight by importance (validated against historical churn), (3) Combine into composite score, (4) Validate: score should predict churn and expansion. Segment into red/yellow/green for CSM action. Monitor score trends, not just absolute values. Regularly recalibrate weights.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["health-score", "saas", "customer-analytics"],
  },
  {
    questionText: "How would you design a self-service analytics platform for business users?",
    idealAnswer: "Architecture: (1) Semantic layer (dbt metrics, Looker LookML): business-friendly metric definitions on top of raw data, (2) Governed dataset catalog: documented, curated datasets with descriptions and owners, (3) BI tool (Tableau, Looker, Metabase): drag-and-drop exploration with guardrails, (4) Pre-built dashboards for common questions, (5) Training program for SQL/tool literacy, (6) Data dictionary with metric definitions and ownership, (7) Access control: row-level security, role-based permissions, (8) Query performance optimization: materialized views, caching. Balance: flexibility for exploration vs governance to prevent misinterpretation.",
    category: "Architecture",
    difficulty: "HARD" as const,
    tags: ["self-service", "analytics-platform", "governance"],
  },
  {
    questionText: "Explain how you would perform a root cause analysis for a 15% drop in weekly revenue.",
    idealAnswer: "Systematic approach: (1) Verify data: check pipeline, recent deployments, tracking changes, (2) Timeline: pinpoint when the drop started (hourly/daily granularity), (3) Decompose revenue = traffic × conversion rate × average order value — which component dropped?, (4) Segment: by channel, geography, device, user type, product — is it widespread or localized?, (5) Check external factors: competitor actions, seasonality, economic events, (6) Cross-reference: did other metrics change? (site speed, error rates, inventory), (7) Correlation with changes: deployments, marketing changes, pricing, (8) Quantify impact per root cause, (9) Recommend fixes prioritized by impact. Present: waterfall chart showing contribution of each factor to the 15% drop.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["root-cause-analysis", "revenue", "debugging"],
  },
  {
    questionText: "How would you design experiments when A/B testing isn't possible?",
    idealAnswer: "Quasi-experimental methods: (1) Difference-in-differences: compare treatment vs control groups' change over time (requires parallel trends assumption), (2) Regression discontinuity: exploit a threshold (users above/below a cutoff), (3) Interrupted time series: compare trend before/after intervention, (4) Propensity score matching: create comparable groups from observational data, (5) Instrumental variables: find a variable affecting treatment but not outcome directly, (6) Synthetic control: construct a weighted combination of control units to match the treated unit. Always: document assumptions, perform sensitivity analyses, acknowledge limitations. These are weaker than RCTs but often the only option.",
    category: "Statistics",
    difficulty: "HARD" as const,
    tags: ["causal-inference", "quasi-experimental", "statistics"],
  },
  {
    questionText: "Design a metric framework for a subscription-based product.",
    idealAnswer: "Framework (pirate metrics AARRR): Acquisition: signups, source attribution, CAC. Activation: trial-to-active conversion, time-to-value. Revenue: MRR, ARPU, expansion revenue, contraction. Retention: logo retention, net revenue retention (NRR), DAU/MAU. Referral: viral coefficient, referral conversion. Financial: LTV, LTV:CAC ratio (>3x healthy), payback period, gross margin. Cohort-level: track each metric by signup cohort. Leading indicators: engagement drop → churn risk. Segment: by plan tier, company size, industry. North star: Net Revenue Retention (captures growth + retention).",
    category: "Metrics",
    difficulty: "HARD" as const,
    tags: ["metrics", "saas", "subscription"],
  },
  {
    questionText: "How would you build a forecasting model for business planning?",
    idealAnswer: "Approach: (1) Understand the business cycle (seasonality, growth patterns), (2) Collect historical data (2+ years for seasonal patterns), (3) EDA: trend, seasonality, outliers, external factors, (4) Methods: simple (moving averages, exponential smoothing), statistical (ARIMA, Prophet), ML-based (XGBoost with lag features), (5) Incorporate known future events (holidays, promotions, launches), (6) Ensemble multiple models for robustness, (7) Backtesting: test on historical holdout periods, measure MAPE/RMSE, (8) Scenario planning: best/base/worst case. Present forecast with confidence intervals. Update regularly as actuals come in.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["forecasting", "time-series", "planning"],
  },
  {
    questionText: "How do you analyze network effects in a platform business?",
    idealAnswer: "Analysis: (1) Direct network effects: measure value increase with user count — engagement per user vs total users, Metcalfe's law validation, (2) Cross-side effects: how supply growth affects demand and vice versa (correlation analysis, Granger causality), (3) Critical mass: identify the tipping point where organic growth exceeds churn, (4) Geographic density: local network effects measured by engagement vs local user density, (5) Engagement loops: map and measure viral loops (invite → signup → engage → invite), (6) Defensibility: switching costs analysis, multi-tenanting rate. Metrics: viral coefficient (k-factor), organic vs paid growth ratio, engagement by network size.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["network-effects", "platform", "growth"],
  },
  {
    questionText: "Design a comprehensive SQL-based analytics workflow for a large e-commerce platform.",
    idealAnswer: "Architecture: (1) Raw layer: event streams (clicks, purchases, page views) in partitioned tables, (2) Staging: cleaned, deduplicated, standardized (dbt models), (3) Dimensional: star schema — fact_orders, fact_events, dim_users, dim_products, dim_dates, (4) Aggregated: pre-computed daily/weekly metrics per entity, (5) Reporting: views/tables optimized for dashboard queries. Key models: user LTV calculation, funnel conversion, cohort retention matrix, RFM segmentation, product affinity. Performance: partition by date, cluster by common filter columns, materialized views for heavy aggregations. Use dbt for transformation orchestration with documentation and testing.",
    category: "Data Engineering",
    difficulty: "HARD" as const,
    tags: ["sql", "analytics-engineering", "dbt"],
  },
  {
    questionText: "How would you measure and optimize the onboarding experience?",
    idealAnswer: "Framework: (1) Define 'aha moment' — the action that correlates with long-term retention (find via correlation analysis of Day 1 actions vs D30 retention), (2) Map onboarding funnel: signup → first action → aha moment → habit formation, (3) Measure: time-to-aha, step completion rates, drop-off points, (4) Segment by: acquisition channel, user type, device, (5) Analyze: which onboarding paths lead to highest activation? (path analysis), (6) Experiment: A/B test changes to onboarding flow, tooltips, emails, (7) Optimize: reduce friction at highest drop-off points, personalize by segment, progressive disclosure. Track: activation rate by cohort over time.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["onboarding", "activation", "product-analytics"],
  },
  {
    questionText: "How would you use SQL to detect anomalies in daily business metrics?",
    idealAnswer: "SQL approach: (1) Calculate rolling statistics: `AVG(metric) OVER (ORDER BY date ROWS BETWEEN 28 PRECEDING AND 1 PRECEDING)` for 28-day rolling average, similarly for STDDEV, (2) Z-score: `(current_value - rolling_avg) / rolling_stddev`, (3) Flag anomalies: `|z_score| > 3`, (4) Account for day-of-week seasonality: separate baselines per weekday, (5) Percentile-based: flag values below 5th or above 95th percentile of recent history, (6) Year-over-year: compare to same day last year. Schedule this as a daily job, alert via email/Slack when anomalies detected. Include context: related metrics, recent changes, external events.",
    category: "SQL",
    difficulty: "HARD" as const,
    tags: ["anomaly-detection", "sql", "monitoring"],
  },
  {
    questionText: "How would you analyze the incremental impact of a loyalty program?",
    idealAnswer: "Approach: (1) Define treatment (loyalty members) and outcome metrics (purchase frequency, AOV, LTV, churn), (2) Challenge: self-selection bias — loyal customers join loyalty programs, they don't become loyal because of it, (3) Methods: propensity score matching (match members to non-members by pre-program behavior), diff-in-diff (compare behavior change for members vs similar non-members), RDD if enrollment has a threshold, (4) Control for: tenure, pre-program spending, demographics, (5) Measure incremental revenue vs program costs, (6) Tier analysis: does higher tier drive more incremental behavior? Always distinguish correlation from causation.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["incrementality", "causal-analysis", "loyalty"],
  },
  {
    questionText: "Design a data governance framework for an analytics team.",
    idealAnswer: "Framework: (1) Data catalog: central inventory of all datasets with descriptions, owners, freshness, lineage (dbt docs, DataHub), (2) Metric definitions: single source of truth for business metrics — prevent conflicting numbers, (3) Access control: role-based permissions, PII masking, audit logs, (4) Data quality: automated tests at each pipeline stage, SLAs for freshness, (5) Documentation standards: every query/dashboard documented with methodology and caveats, (6) Change management: review process for metric definition changes, (7) Training: analyst onboarding, SQL style guide, dashboard design standards, (8) Privacy: GDPR/CCPA compliance, data retention policies. Review and update quarterly.",
    category: "Governance",
    difficulty: "HARD" as const,
    tags: ["governance", "data-management", "standards"],
  },
  {
    questionText: "How would you perform a market sizing analysis?",
    idealAnswer: "Two approaches: (1) Top-down: total market → relevant segment → our addressable share. Example: total retail spend → online share → our category → our geography → realistic capture rate, (2) Bottom-up: unit economics × addressable units. Example: average customer value × number of potential customers in target segments. Triangulate both methods. Data sources: government statistics, industry reports, competitor analysis, survey data. Present: TAM (Total Addressable Market), SAM (Serviceable), SOM (Obtainable). Include assumptions and sensitivity analysis for key inputs.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["market-sizing", "strategy", "estimation"],
  },
  {
    questionText: "How would you build a predictive model for customer support ticket volume?",
    idealAnswer: "Approach: (1) Feature engineering: day of week, month, holidays, recent product launches/changes, marketing campaigns, user growth rate, app version releases, seasonal patterns, (2) Target: daily/hourly ticket count, (3) Models: SARIMA for time-series baseline, Prophet for handling holidays, XGBoost with lag features and external regressors for incorporating multiple signals, (4) Evaluation: MAPE on holdout period, backtest across multiple periods, (5) Operationalize: daily forecast for staffing, weekly forecast for hiring, (6) Monitor: forecast vs actual, retrain when accuracy degrades. Output: expected volume with confidence intervals to plan staffing levels.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["forecasting", "predictive", "support"],
  },
  {
    questionText: "How do you evaluate the effectiveness of personalization features?",
    idealAnswer: "Framework: (1) A/B test: personalized vs non-personalized experience, (2) Metrics: CTR on recommendations, conversion rate, revenue per user, engagement depth, (3) Beyond averages: measure by user segment (new vs returning, active vs dormant), (4) Serendipity: are users discovering new things or just seeing more of the same?, (5) Diversity metrics: are recommendations diverse or filtered-bubble?, (6) Long-term impact: measure retention and LTV, not just session metrics, (7) Coverage: what % of catalog is surfaced? (popularity bias check), (8) Revenue attribution: incremental revenue from personalized items vs what users would have bought anyway. Challenge: users may have bought popular items regardless.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["personalization", "experimentation", "analytics"],
  },
  {
    questionText: "Design an approach for analyzing the impact of a product redesign.",
    idealAnswer: "Analysis plan: (1) Pre/post metrics comparison with A/B test (phased rollout), (2) Guardrail metrics: ensure no harm (page load time, error rate, support tickets), (3) Primary metrics: task completion rate, time-on-task, conversion rate, engagement, (4) Qualitative: user feedback, NPS change, heatmap/session recording analysis, (5) Learning curve: track metrics over time post-launch (initial dip expected), (6) Segment analysis: does the redesign help some users more than others?, (7) Feature-level: which specific changes drive the most impact? (multi-variate testing), (8) Long-term: track 30/60/90 day retention. Present: impact summary table with statistical significance, qualitative themes, and user quotes.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["product-analytics", "redesign", "impact-analysis"],
  },
  {
    questionText: "How would you build a complex SQL query to calculate net revenue retention (NRR)?",
    idealAnswer: "NRR = (Starting MRR + Expansion - Contraction - Churn) / Starting MRR × 100%. SQL: (1) CTE for monthly user MRR: join subscriptions with plan prices, calculate MRR per user per month, (2) Self-join current month to previous month by user: `LEFT JOIN mrr_prev ON mrr_current.user_id = mrr_prev.user_id`, (3) Classify: new (no prev), expansion (current > prev), contraction (current < prev, current > 0), churned (current is null, prev > 0), (4) Aggregate each category, (5) Calculate NRR. Handle: mid-month changes (prorate), annual plans (convert to monthly), free trials, paused subscriptions. Good NRR: >100% means existing customers grow enough to offset churn.",
    category: "SQL",
    difficulty: "HARD" as const,
    tags: ["nrr", "sql", "saas-metrics"],
  },
  {
    questionText: "How do you design a multi-touch attribution model using data analysis?",
    idealAnswer: "Implementation: (1) Collect user journey data: all touchpoints with timestamps (UTM parameters, cookie tracking), (2) Build user paths: sequence of channels before conversion, (3) Markov chain model: build state transition matrix between channels, calculate removal effect (how much would conversions drop if a channel were removed?), (4) Shapley value approach: calculate each channel's marginal contribution across all possible coalitions, (5) Compare model results with last-touch baseline, (6) Validate: holdout experiments (turn off a channel, measure actual impact). SQL: sessionize user events, build path sequences, calculate transition probabilities. Key insight: attribution is a model, not ground truth.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["attribution", "markov-chain", "marketing"],
  },
  {
    questionText: "How would you perform a geographic expansion analysis?",
    idealAnswer: "Framework: (1) Market sizing per region: TAM estimation using demographic data, internet penetration, category spending, (2) Demand signals: organic traffic, waitlist signups, app store searches by region, (3) Competition: competitor presence and market share, (4) Operational feasibility: payment infrastructure, legal requirements, language, support costs, (5) Unit economics: CAC, LTV, and margin projections per market, (6) Proxy analysis: find markets most similar to successful existing markets (clustering by economic/demographic features), (7) Scoring model: weight and rank markets on all factors, (8) Phased approach: recommend pilot markets with highest score/lowest risk.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["expansion", "market-analysis", "strategy"],
  },
  {
    questionText: "Design an approach for detecting and analyzing cannibalization between products.",
    idealAnswer: "Analysis: (1) Define cannibalization: new product sales that substitute rather than add to existing product sales, (2) Time series: plot old product sales before/after new product launch, control for seasonality and trend, (3) Customer level: do customers who buy the new product reduce purchases of the old? (purchase frequency analysis), (4) Market basket: are the products ever bought together? (low co-purchase = substitution), (5) Price sensitivity: cross-price elasticity (positive = substitutes), (6) A/B test: show/hide new product to different users, measure total revenue, (7) Customer surveys for purchase intent. Quantify: total revenue impact = new product revenue - cannibalized revenue from old products. Net positive = good.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["cannibalization", "product-analytics", "revenue"],
  },
  {
    questionText: "How would you analyze the effectiveness of a recommendation algorithm?",
    idealAnswer: "Offline metrics: (1) Precision@k and Recall@k (are recommendations relevant?), (2) nDCG (are better items ranked higher?), (3) Coverage (what % of catalog is recommended?), (4) Diversity (how varied are recommendations?). Online metrics (A/B test): (5) CTR on recommendations, (6) Conversion rate from recommendation clicks, (7) Revenue attributed to recommendations, (8) User engagement (session length, return visits), (9) Serendipity (are users discovering new items?), (10) Long-term retention impact. Analysis: segment by user type (new vs power users), cold-start performance, compare algorithms with interleaving experiments. Watch for popularity bias and feedback loops.",
    category: "Analysis",
    difficulty: "HARD" as const,
    tags: ["recommendations", "evaluation", "analytics"],
  },
  {
    questionText: "How would you structure a data analysis project from start to finish?",
    idealAnswer: "Structure: (1) Problem definition: translate business question to analytical question, agree on success criteria, (2) Scoping: what data is needed, is it available, timeline estimate, (3) Data collection: identify sources, extract with SQL, validate quality, (4) EDA: understand distributions, relationships, anomalies, (5) Analysis: apply appropriate methods (descriptive, diagnostic, predictive), (6) Validation: sanity check results, peer review, sensitivity analysis, (7) Communication: deck with executive summary, key findings, visualizations, recommendations, (8) Handoff: documented queries, reproducible analysis, dashboard if ongoing. Throughout: document decisions, assumptions, and limitations. Iterate with stakeholders — don't disappear for weeks.",
    category: "Methodology",
    difficulty: "HARD" as const,
    tags: ["methodology", "project-management", "process"],
  },
  {
    questionText: "How do you handle multiple comparison problems in analysis?",
    idealAnswer: "Problem: when testing many hypotheses simultaneously, the probability of at least one false positive increases (at α=0.05, testing 20 metrics → ~64% chance of at least one false positive). Solutions: (1) Bonferroni correction: α/n per test (conservative), (2) Holm-Bonferroni: stepwise correction (less conservative), (3) Benjamini-Hochberg: controls false discovery rate (FDR) instead of family-wise error (more powerful), (4) Pre-register primary metric: designate one primary metric, others are exploratory, (5) Reduce the number of tests: focus analysis. Always report how many tests were conducted. In A/B testing: have one primary metric, treat others as directional.",
    category: "Statistics",
    difficulty: "HARD" as const,
    tags: ["multiple-comparisons", "statistics", "testing"],
  },
  {
    questionText: "How would you build an executive metrics dashboard for a CEO?",
    idealAnswer: "Design: (1) Top-level KPIs with trend sparklines: MRR, revenue growth, runway, headcount, (2) Growth: user acquisition trend, conversion funnel, CAC by channel, (3) Product: DAU/MAU ratio, activation rate, feature adoption, (4) Retention: monthly cohort retention, NRR, churn rate, (5) Financial: revenue vs forecast, burn rate, unit economics, (6) Comparison: vs target, vs last year, vs plan. Principles: no more than 10-15 metrics, traffic-light indicators (red/yellow/green vs target), drill-down capability, weekly auto-refresh, mobile-friendly. Include commentary section for context. Avoid: vanity metrics, excessive detail, metrics without targets.",
    category: "Visualization",
    difficulty: "HARD" as const,
    tags: ["dashboard", "executive", "kpis"],
  },
  {
    questionText: "How would you quantify the ROI of a data analytics team?",
    idealAnswer: "Approach: (1) Direct revenue impact: analyses that led to decisions with measurable outcomes (pricing optimization → revenue increase, churn prediction → saved customers × LTV), (2) Cost savings: automated reporting replacing manual work (hours saved × analyst cost), fraud detection savings, optimized marketing spend, (3) Speed: faster decision-making from dashboards vs ad-hoc requests (time-to-insight), (4) Decision quality: A/B test win rates, forecast accuracy, (5) Enablement: self-service analytics usage, data literacy improvement. Framework: maintain a decision log linking analyses to business outcomes. Present: aggregate impact vs team cost. Caveats: attribution is imperfect — the team enables decisions, doesn't make them alone.",
    category: "Business",
    difficulty: "HARD" as const,
    tags: ["roi", "analytics", "business-value"],
  },
];
