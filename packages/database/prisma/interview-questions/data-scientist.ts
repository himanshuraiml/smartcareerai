export const dataScientistQuestions = [
  // ===== EASY (30 questions) =====
  {
    questionText: "What is the difference between supervised and unsupervised learning?",
    idealAnswer: "Supervised learning trains on labeled data (input-output pairs) to predict outcomes — classification (categories) and regression (continuous values). Unsupervised learning finds patterns in unlabeled data — clustering (grouping), dimensionality reduction (PCA), and anomaly detection. Semi-supervised learning combines both. Choose supervised when you have labels; unsupervised for exploratory analysis.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["ml", "supervised", "unsupervised"],
  },
  {
    questionText: "What is the difference between classification and regression?",
    idealAnswer: "Classification predicts discrete categories (spam/not spam, cat/dog). Regression predicts continuous values (house price, temperature). Classification metrics: accuracy, precision, recall, F1, AUC-ROC. Regression metrics: MSE, RMSE, MAE, R². Some algorithms do both: decision trees, neural networks. Logistic regression despite its name is a classifier.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["classification", "regression", "ml"],
  },
  {
    questionText: "What is overfitting and how do you prevent it?",
    idealAnswer: "Overfitting is when a model learns noise in training data and performs poorly on unseen data (high training accuracy, low test accuracy). Prevention: more training data, regularization (L1/L2), cross-validation, early stopping, dropout (neural networks), simpler models, feature selection, data augmentation. Detect by comparing train vs validation performance.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["overfitting", "regularization", "ml"],
  },
  {
    questionText: "What is a pandas DataFrame and how do you use it?",
    idealAnswer: "A DataFrame is a 2D labeled data structure in pandas (like a spreadsheet). Create from CSV, JSON, dict, or SQL. Key operations: `df.head()`, `df.describe()`, `df.info()`, filtering (`df[df['age'] > 30]`), grouping (`df.groupby('city').mean()`), merging (`pd.merge()`), handling missing values (`df.fillna()`, `df.dropna()`). It's the core data manipulation tool in Python data science.",
    category: "Python",
    difficulty: "EASY" as const,
    tags: ["pandas", "python", "data-manipulation"],
  },
  {
    questionText: "What is the difference between mean, median, and mode?",
    idealAnswer: "Mean is the arithmetic average — sensitive to outliers. Median is the middle value when sorted — robust to outliers, better for skewed distributions. Mode is the most frequent value — useful for categorical data. For normally distributed data, all three are equal. For salary data (right-skewed), median is more representative than mean.",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["statistics", "descriptive", "central-tendency"],
  },
  {
    questionText: "What is a p-value and what does statistical significance mean?",
    idealAnswer: "A p-value is the probability of observing results as extreme as the data, assuming the null hypothesis is true. Statistical significance (typically p < 0.05) means the result is unlikely due to chance alone. It does NOT mean the effect is large or practically important. Common misinterpretation: p-value is not the probability the null hypothesis is true. Always consider effect size alongside p-values.",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["statistics", "p-value", "hypothesis-testing"],
  },
  {
    questionText: "What is feature engineering and why is it important?",
    idealAnswer: "Feature engineering transforms raw data into features that better represent the underlying problem to the model. Techniques: encoding categoricals (one-hot, label), scaling numerics (normalization, standardization), creating interaction features, binning, polynomial features, text vectorization (TF-IDF), date extraction (day, month, weekday). Good features often matter more than algorithm choice.",
    category: "Data Preprocessing",
    difficulty: "EASY" as const,
    tags: ["feature-engineering", "preprocessing", "ml"],
  },
  {
    questionText: "What is cross-validation and why use it?",
    idealAnswer: "Cross-validation evaluates model performance by splitting data into k folds: train on k-1 folds, validate on the remaining, rotate k times. Benefits: uses all data for both training and validation, more reliable performance estimate than a single train/test split, detects overfitting. k-Fold (k=5 or 10) is standard. Stratified k-fold preserves class distribution in classification.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["cross-validation", "evaluation", "ml"],
  },
  {
    questionText: "What is SQL and why is it important for data science?",
    idealAnswer: "SQL queries relational databases to extract, transform, and analyze data. Essential operations: SELECT, WHERE, JOIN, GROUP BY, HAVING, ORDER BY, subqueries, window functions. Data scientists use SQL to: extract datasets from production databases, perform initial analysis, build data pipelines. Most data lives in SQL databases. Proficiency in SQL is often the most-used skill in day-to-day data science work.",
    category: "SQL",
    difficulty: "EASY" as const,
    tags: ["sql", "databases", "data-extraction"],
  },
  {
    questionText: "What is the difference between correlation and causation?",
    idealAnswer: "Correlation measures the statistical relationship between two variables (how they move together). Causation means one variable directly influences the other. Correlation does not imply causation — confounding variables or coincidence can create correlations. To establish causation: randomized controlled experiments, natural experiments, or causal inference methods (instrumental variables, difference-in-differences).",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["statistics", "correlation", "causation"],
  },
  {
    questionText: "What is a confusion matrix and what metrics can you derive from it?",
    idealAnswer: "A confusion matrix shows True Positives, False Positives, True Negatives, and False Negatives for a classifier. Derived metrics: Accuracy = (TP+TN)/total, Precision = TP/(TP+FP) (of predicted positives, how many correct), Recall = TP/(TP+FN) (of actual positives, how many caught), F1 = harmonic mean of precision and recall. Choose based on costs: precision when false positives are costly, recall when false negatives are costly.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["evaluation", "confusion-matrix", "metrics"],
  },
  {
    questionText: "What is NumPy and why is it fundamental to data science?",
    idealAnswer: "NumPy provides n-dimensional arrays and vectorized operations in Python. It's the foundation for pandas, scikit-learn, TensorFlow, and most scientific Python libraries. Key features: fast array operations (C-implemented), broadcasting, linear algebra, random number generation. Arrays are more memory-efficient and faster than Python lists for numerical operations due to contiguous memory and SIMD instructions.",
    category: "Python",
    difficulty: "EASY" as const,
    tags: ["numpy", "python", "arrays"],
  },
  {
    questionText: "What is data normalization and when should you use it?",
    idealAnswer: "Normalization scales features to a common range. Min-max scaling: [0, 1] range. Standardization (Z-score): mean=0, std=1. Normalize when: features have different scales (age vs income), using distance-based algorithms (KNN, SVM, K-means), or gradient descent. Don't normalize: tree-based models (decision trees, random forest) are scale-invariant. Always fit scaler on training data only, transform both train and test.",
    category: "Data Preprocessing",
    difficulty: "EASY" as const,
    tags: ["normalization", "scaling", "preprocessing"],
  },
  {
    questionText: "What is a decision tree and how does it work?",
    idealAnswer: "A decision tree splits data recursively based on feature thresholds to make predictions. At each node, it chooses the split that maximizes information gain (entropy reduction) or minimizes Gini impurity. Leaf nodes contain predictions. Advantages: interpretable, handles mixed data types, no scaling needed. Disadvantages: prone to overfitting, unstable (small data changes = different tree). Mitigate with pruning, or use ensembles (Random Forest, XGBoost).",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["decision-tree", "ml", "algorithms"],
  },
  {
    questionText: "What is EDA (Exploratory Data Analysis)?",
    idealAnswer: "EDA is the initial investigation of data to discover patterns, anomalies, and relationships. Steps: (1) Check shape, types, missing values (`df.info()`), (2) Summary statistics (`df.describe()`), (3) Distribution plots (histograms, box plots), (4) Correlation matrix/heatmap, (5) Outlier detection, (6) Categorical value counts, (7) Feature relationships (scatter plots, pair plots). EDA guides feature engineering, model selection, and data cleaning decisions.",
    category: "Data Analysis",
    difficulty: "EASY" as const,
    tags: ["eda", "visualization", "data-analysis"],
  },
  {
    questionText: "What is the difference between bagging and boosting?",
    idealAnswer: "Bagging (Bootstrap Aggregating) trains multiple models on random subsets of data in parallel, then averages predictions — reduces variance. Example: Random Forest. Boosting trains models sequentially, each correcting the previous model's errors — reduces bias. Example: XGBoost, AdaBoost. Bagging works well with high-variance models (deep trees), boosting with high-bias models (shallow trees). Boosting can overfit if not tuned.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["bagging", "boosting", "ensemble"],
  },
  {
    questionText: "What is a hypothesis test? Explain the steps.",
    idealAnswer: "Steps: (1) Define null hypothesis H₀ (no effect) and alternative H₁ (effect exists), (2) Choose significance level α (usually 0.05), (3) Select appropriate test (t-test, chi-squared, ANOVA), (4) Calculate test statistic from data, (5) Compute p-value, (6) Compare p-value to α: reject H₀ if p < α. Common tests: t-test (means comparison), chi-squared (categorical independence), ANOVA (multiple group means).",
    category: "Statistics",
    difficulty: "EASY" as const,
    tags: ["hypothesis-testing", "statistics", "inference"],
  },
  {
    questionText: "What is the difference between L1 and L2 regularization?",
    idealAnswer: "L1 (Lasso) adds absolute value of weights to the loss — can shrink weights to exactly zero, performing feature selection. L2 (Ridge) adds squared weights — shrinks weights toward zero but rarely to zero. L1 produces sparse models; L2 produces stable models. Elastic Net combines both. Use L1 when you suspect many irrelevant features; L2 when all features contribute. Both prevent overfitting.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["regularization", "lasso", "ridge"],
  },
  {
    questionText: "What is a data pipeline and why is it important?",
    idealAnswer: "A data pipeline automates the flow of data from source to destination: Extract (from databases, APIs, files), Transform (clean, aggregate, feature engineer), Load (into data warehouse or model). Importance: reproducibility, automation, scalability. Tools: Apache Airflow for orchestration, pandas/Spark for transformation, dbt for SQL transformations. A reliable pipeline is essential for model retraining and consistent analysis.",
    category: "Data Engineering",
    difficulty: "EASY" as const,
    tags: ["data-pipeline", "etl", "data-engineering"],
  },
  {
    questionText: "What are common data visualization libraries in Python?",
    idealAnswer: "Matplotlib: low-level, highly customizable, foundation for others. Seaborn: statistical visualizations built on matplotlib (heatmaps, pair plots, violin plots). Plotly: interactive plots, dashboards, web-ready. Use matplotlib for publication figures, seaborn for quick statistical plots, plotly for interactive dashboards. Additional: Altair (declarative), bokeh (web apps). Always label axes, use appropriate chart types, and consider colorblind-friendly palettes.",
    category: "Visualization",
    difficulty: "EASY" as const,
    tags: ["visualization", "matplotlib", "seaborn"],
  },
  {
    questionText: "What is the bias-variance tradeoff?",
    idealAnswer: "Bias is error from oversimplified models (underfitting) — model misses patterns. Variance is error from overly complex models (overfitting) — model captures noise. Total error = bias² + variance + irreducible noise. Simple models have high bias, low variance. Complex models have low bias, high variance. The goal is to find the sweet spot that minimizes total error. Regularization, cross-validation, and ensemble methods help manage this tradeoff.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["bias-variance", "ml", "theory"],
  },
  {
    questionText: "How do you handle missing data?",
    idealAnswer: "Strategies: (1) Remove rows/columns if missing data is small and random (MCAR), (2) Impute with mean/median (numerical) or mode (categorical), (3) Advanced imputation: KNN imputer, iterative imputer (MICE), (4) Create a missing indicator feature, (5) Use algorithms that handle missing values natively (XGBoost). First understand WHY data is missing: MCAR (random), MAR (depends on observed data), MNAR (depends on the missing value itself). Document imputation choices.",
    category: "Data Preprocessing",
    difficulty: "EASY" as const,
    tags: ["missing-data", "imputation", "preprocessing"],
  },
  {
    questionText: "What is a random forest and why is it popular?",
    idealAnswer: "Random Forest is an ensemble of decision trees trained on random subsets of data and features. It averages predictions (regression) or uses majority voting (classification). Benefits: handles non-linear relationships, robust to outliers, feature importance ranking, minimal tuning needed, handles mixed data types. Drawbacks: less interpretable than single trees, slower for large datasets. Often a strong baseline before trying complex models.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["random-forest", "ensemble", "ml"],
  },
  {
    questionText: "What is the difference between a data scientist and a data analyst?",
    idealAnswer: "Data analysts focus on describing what happened using SQL, Excel, dashboards, and descriptive statistics. Data scientists build predictive models, design experiments, and use machine learning. Analysts answer business questions with existing data; scientists build systems that make predictions. Both need SQL and statistics. Data scientists additionally need programming (Python), ML algorithms, and experimental design. There's significant overlap in practice.",
    category: "Career",
    difficulty: "EASY" as const,
    tags: ["career", "roles", "data-science"],
  },
  {
    questionText: "What is Jupyter Notebook and why is it used in data science?",
    idealAnswer: "Jupyter Notebook is an interactive environment combining code, visualizations, and markdown text in cells. Benefits: iterative exploration, inline plots, documentation alongside code, easy sharing of analysis. Used for: EDA, prototyping models, presenting results. Alternatives: JupyterLab (IDE-like), Google Colab (cloud + GPU), VS Code notebooks. For production: convert notebooks to Python scripts with proper testing and version control.",
    category: "Tooling",
    difficulty: "EASY" as const,
    tags: ["jupyter", "tooling", "python"],
  },
  {
    questionText: "What is gradient descent and how does it work?",
    idealAnswer: "Gradient descent is an optimization algorithm that minimizes a loss function by iteratively moving in the direction of steepest descent (negative gradient). Steps: initialize weights, compute loss, calculate gradients, update weights: w = w - learning_rate × gradient. Variants: batch (full dataset), stochastic (one sample), mini-batch (subset). Learning rate controls step size — too high diverges, too low is slow. Adam optimizer adapts learning rates per parameter.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["gradient-descent", "optimization", "ml"],
  },
  {
    questionText: "What is the difference between precision and recall?",
    idealAnswer: "Precision = TP / (TP + FP) — of all predicted positives, what fraction is correct. Recall = TP / (TP + FN) — of all actual positives, what fraction was detected. There's a tradeoff: increasing one often decreases the other. High precision: few false alarms (spam filter — don't lose real email). High recall: catch most positives (cancer screening — don't miss cases). F1 score balances both.",
    category: "Machine Learning",
    difficulty: "EASY" as const,
    tags: ["precision", "recall", "evaluation"],
  },
  {
    questionText: "What is Git and why should data scientists use it?",
    idealAnswer: "Git tracks code changes, enables collaboration, and provides version history. Data scientists should use it for: reproducible analysis (track code versions), collaboration (branches, pull requests), experiment tracking (branch per experiment), code review, CI/CD integration. Use `.gitignore` for large data files and model artifacts. For large files: Git LFS or DVC (Data Version Control). Never commit credentials or API keys.",
    category: "Tooling",
    difficulty: "EASY" as const,
    tags: ["git", "version-control", "tooling"],
  },
  {
    questionText: "What is an outlier and how do you handle them?",
    idealAnswer: "An outlier is a data point significantly different from others. Detection: Z-score (> 3 standard deviations), IQR method (below Q1-1.5×IQR or above Q3+1.5×IQR), visualization (box plots, scatter plots). Handling: investigate root cause first — could be data errors, genuine extremes, or different populations. Options: remove (if errors), cap/winsorize, transform (log), or use robust algorithms. Always document the decision.",
    category: "Data Preprocessing",
    difficulty: "EASY" as const,
    tags: ["outliers", "data-cleaning", "statistics"],
  },
  {
    questionText: "What are the main Python libraries used in data science?",
    idealAnswer: "Core: NumPy (arrays), pandas (DataFrames), matplotlib/seaborn (visualization), scikit-learn (ML algorithms). Deep learning: TensorFlow, PyTorch. NLP: spaCy, Hugging Face transformers. Data engineering: Apache Spark (PySpark), Airflow. Statistics: SciPy, statsmodels. Deployment: Flask/FastAPI for model serving. The pandas + scikit-learn + matplotlib trio covers most day-to-day data science tasks.",
    category: "Python",
    difficulty: "EASY" as const,
    tags: ["python", "libraries", "tooling"],
  },

  // ===== MEDIUM (40 questions) =====
  {
    questionText: "Explain the ROC curve and AUC metric. When would you prefer AUC over accuracy?",
    idealAnswer: "ROC plots True Positive Rate vs False Positive Rate at various classification thresholds. AUC (Area Under Curve) measures overall discrimination ability — 1.0 is perfect, 0.5 is random. Prefer AUC over accuracy when: classes are imbalanced (99% negative class → 99% accuracy by predicting all negative), you need threshold-independent evaluation, or you want to compare models. AUC works with probability outputs, not just hard predictions.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["roc", "auc", "evaluation"],
  },
  {
    questionText: "How do you handle imbalanced datasets in classification?",
    idealAnswer: "Techniques: (1) Resampling: oversample minority (SMOTE), undersample majority, (2) Class weights: assign higher weight to minority class in loss function, (3) Threshold tuning: adjust classification threshold from 0.5, (4) Metrics: use F1, precision-recall AUC, not accuracy, (5) Ensemble methods: balanced random forest, (6) Stratified cross-validation, (7) Collect more data for minority class. The right approach depends on the use case and how severe the imbalance is.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["imbalanced-data", "smote", "classification"],
  },
  {
    questionText: "Explain XGBoost and why it's widely used in competitions and industry.",
    idealAnswer: "XGBoost is gradient boosted decision trees with optimizations: second-order gradients for better convergence, regularization (L1/L2), column subsampling, built-in handling of missing values, parallel tree construction. Hyperparameters: learning_rate, max_depth, n_estimators, subsample, colsample_bytree. Popular because: strong out-of-box performance, handles tabular data well, feature importance, scalable. LightGBM and CatBoost are alternatives with specific advantages.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["xgboost", "boosting", "ml"],
  },
  {
    questionText: "What is PCA and when would you use it?",
    idealAnswer: "PCA (Principal Component Analysis) reduces dimensionality by projecting data onto orthogonal axes of maximum variance. The first component captures most variance, each subsequent captures less. Use when: many correlated features (multicollinearity), visualization of high-dimensional data, reducing noise, speeding up training. Choose components retaining 95% of variance. Limitation: linear only, loses interpretability. Alternatives: t-SNE/UMAP for visualization.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["pca", "dimensionality-reduction", "ml"],
  },
  {
    questionText: "How do you design and evaluate an A/B test?",
    idealAnswer: "Steps: (1) Define hypothesis and metric (conversion rate), (2) Calculate sample size based on minimum detectable effect and power (80%), (3) Randomize users into control/treatment groups, (4) Run test for predetermined duration (don't peek), (5) Check for sample ratio mismatch, (6) Analyze: t-test or chi-squared, check p-value and confidence interval, (7) Consider practical significance, not just statistical. Pitfalls: multiple comparisons, peeking, novelty effects, selection bias.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["ab-testing", "statistics", "experimentation"],
  },
  {
    questionText: "What is feature selection and what methods exist?",
    idealAnswer: "Feature selection reduces input features to improve model performance and interpretability. Methods: (1) Filter: statistical tests (chi-squared, correlation, mutual information) — fast, model-independent, (2) Wrapper: recursive feature elimination, forward/backward selection — evaluates subsets with a model, (3) Embedded: L1 regularization, tree-based importance — feature selection during training. Also: variance threshold (remove near-zero variance), domain knowledge. Fewer features = less overfitting, faster training, easier interpretation.",
    category: "Data Preprocessing",
    difficulty: "MEDIUM" as const,
    tags: ["feature-selection", "preprocessing", "ml"],
  },
  {
    questionText: "Explain the concept of the curse of dimensionality.",
    idealAnswer: "As feature dimensions increase: (1) Data becomes sparse — distance between points converges, making distance-based methods ineffective, (2) Volume of feature space grows exponentially, requiring exponentially more data, (3) Overfitting risk increases, (4) Computation becomes expensive. Solutions: dimensionality reduction (PCA, autoencoders), feature selection, regularization. Rule of thumb: need ~10× samples per feature. Affects: KNN, K-means, SVM more than tree-based methods.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["curse-of-dimensionality", "ml", "theory"],
  },
  {
    questionText: "How would you build a recommendation system?",
    idealAnswer: "Approaches: (1) Collaborative filtering: user-based (similar users) or item-based (similar items) using cosine similarity or matrix factorization (SVD), (2) Content-based: recommend items similar to user's history using item features, (3) Hybrid: combine both. Implementation: user-item interaction matrix, handle cold start with popularity or content-based. Evaluation: RMSE for ratings, precision@k and nDCG for ranking. Deep learning: two-tower models, autoencoders. Scale with approximate nearest neighbors (FAISS).",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["recommender-systems", "collaborative-filtering", "ml"],
  },
  {
    questionText: "What is regularization and why is it important?",
    idealAnswer: "Regularization adds a penalty term to the loss function to prevent overfitting by constraining model complexity. L1 (Lasso) = |w|, promotes sparsity. L2 (Ridge) = w², shrinks weights. Elastic Net = both. In neural networks: dropout (randomly zero out neurons), batch normalization, weight decay. Regularization reduces variance at the cost of slightly higher bias, improving generalization. The regularization strength (λ/α) is a key hyperparameter.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["regularization", "overfitting", "ml"],
  },
  {
    questionText: "What is time series analysis? Name key concepts and models.",
    idealAnswer: "Time series is data indexed by time. Key concepts: trend (long-term direction), seasonality (repeating patterns), stationarity (constant mean/variance), autocorrelation. Models: ARIMA (autoregressive integrated moving average), SARIMA (seasonal), Prophet (by Meta, handles holidays), exponential smoothing. ML approaches: LSTM, Transformer, XGBoost with lag features. Always test stationarity (ADF test), split temporally (no random splits), and watch for data leakage from future values.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["time-series", "arima", "forecasting"],
  },
  {
    questionText: "How do you handle categorical variables in machine learning?",
    idealAnswer: "Methods: (1) One-hot encoding: binary columns per category — for low cardinality (< 20 values), (2) Label encoding: ordinal integers — for tree-based models or ordinal categories, (3) Target encoding: replace with mean target value — for high cardinality (zip codes), risk of leakage, (4) Embedding: learned representations — for deep learning. Avoid one-hot for high cardinality (creates many features). Use scikit-learn's encoders or category_encoders library.",
    category: "Data Preprocessing",
    difficulty: "MEDIUM" as const,
    tags: ["encoding", "categorical", "preprocessing"],
  },
  {
    questionText: "Explain the difference between batch and online learning.",
    idealAnswer: "Batch learning trains on the entire dataset at once, then deploys a static model. Retraining requires processing all data again. Online (incremental) learning updates the model with each new data point or mini-batch. Use batch for: stable data, sufficient compute, high accuracy needs. Use online for: streaming data, memory constraints, concept drift adaptation. SGD and neural networks support online learning. Some models (KNN, trees) don't.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["batch-learning", "online-learning", "ml"],
  },
  {
    questionText: "What is the difference between parametric and non-parametric models?",
    idealAnswer: "Parametric models assume a fixed functional form and have a fixed number of parameters regardless of data size — linear regression, logistic regression, naive Bayes. Faster, need less data, but may underfit if assumptions are wrong. Non-parametric models have flexible structure that grows with data — KNN, decision trees, SVMs with kernels. More flexible but need more data, risk overfitting, and are slower. Choose based on data size and assumed relationships.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["parametric", "non-parametric", "ml"],
  },
  {
    questionText: "How do you evaluate a regression model?",
    idealAnswer: "Metrics: (1) MSE (Mean Squared Error): penalizes large errors heavily, (2) RMSE: square root of MSE, same units as target, (3) MAE: mean absolute error, robust to outliers, (4) R² (coefficient of determination): proportion of variance explained (1.0 is perfect), (5) Adjusted R²: accounts for number of features. Always plot: residuals vs predicted (check for patterns), actual vs predicted (check linearity). Use cross-validation for reliable estimates. Compare to a baseline (mean prediction).",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["regression", "evaluation", "metrics"],
  },
  {
    questionText: "What is data leakage and how do you prevent it?",
    idealAnswer: "Data leakage occurs when training data includes information that wouldn't be available at prediction time, leading to unrealistically high performance. Types: (1) Target leakage: feature derived from target variable, (2) Train-test contamination: fitting transformations on entire dataset before splitting. Prevention: split data first then preprocess, use pipelines, temporal splits for time series, be suspicious of near-perfect accuracy. Leakage is one of the most common and costly ML mistakes.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["data-leakage", "ml", "evaluation"],
  },
  {
    questionText: "Explain k-means clustering and its limitations.",
    idealAnswer: "K-means partitions data into k clusters by minimizing within-cluster variance. Steps: initialize k centroids (k-means++), assign points to nearest centroid, update centroids, repeat until convergence. Limitations: must specify k (use elbow method or silhouette score), assumes spherical clusters, sensitive to outliers and initialization, doesn't handle varying cluster sizes/densities. Alternatives: DBSCAN (density-based, no k needed), hierarchical clustering, Gaussian mixture models.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["clustering", "k-means", "unsupervised"],
  },
  {
    questionText: "How do you handle multicollinearity in a dataset?",
    idealAnswer: "Multicollinearity is high correlation between features, causing unstable coefficients in linear models. Detection: correlation matrix, VIF (Variance Inflation Factor) > 5-10 indicates issues. Solutions: (1) Remove one of correlated features, (2) PCA to combine into uncorrelated components, (3) Regularization (Ridge regression), (4) Domain knowledge to select the most relevant feature. Tree-based models are less affected by multicollinearity.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["multicollinearity", "statistics", "regression"],
  },
  {
    questionText: "What is natural language processing (NLP)? Name key techniques.",
    idealAnswer: "NLP processes and analyzes human language computationally. Techniques: tokenization, stemming/lemmatization, TF-IDF vectorization, word embeddings (Word2Vec, GloVe), named entity recognition, sentiment analysis, text classification. Modern: transformer models (BERT, GPT) with transfer learning — fine-tune pretrained models for specific tasks. Libraries: spaCy (production), Hugging Face (transformers), NLTK (educational). Applications: chatbots, search, translation, summarization.",
    category: "NLP",
    difficulty: "MEDIUM" as const,
    tags: ["nlp", "text-processing", "transformers"],
  },
  {
    questionText: "Explain the concepts of Type I and Type II errors.",
    idealAnswer: "Type I error (false positive): rejecting a true null hypothesis — concluding there's an effect when there isn't. Probability = α (significance level, typically 0.05). Type II error (false negative): failing to reject a false null hypothesis — missing a real effect. Probability = β. Power = 1 - β (typically 0.80). Tradeoff: reducing Type I increases Type II. Example: medical test — Type I = healthy person tested positive, Type II = sick person tested negative.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["statistics", "hypothesis-testing", "errors"],
  },
  {
    questionText: "How do you perform hyperparameter tuning?",
    idealAnswer: "Methods: (1) Grid search: exhaustive search over parameter combinations — thorough but slow, (2) Random search: sample random combinations — often more efficient than grid for many parameters, (3) Bayesian optimization (Optuna): uses past results to guide search — most efficient, (4) Cross-validation for evaluation at each combination. Key: use validation set or CV, not test set. Common params to tune: learning rate, regularization strength, tree depth, number of estimators. Start broad, then narrow the range.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["hyperparameter-tuning", "optimization", "ml"],
  },
  {
    questionText: "What is the difference between generative and discriminative models?",
    idealAnswer: "Discriminative models learn the decision boundary P(y|x) directly — logistic regression, SVM, neural networks. They map inputs to outputs without modeling the input distribution. Generative models learn the joint probability P(x,y) = P(x|y)P(y) — Naive Bayes, GMMs, GANs. They can generate new data samples. Discriminative models typically perform better for classification when you have enough labeled data. Generative models are useful for data augmentation and understanding data distribution.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["generative", "discriminative", "ml"],
  },
  {
    questionText: "Explain how you would build a fraud detection system.",
    idealAnswer: "Approach: (1) Highly imbalanced data — use SMOTE, class weights, anomaly detection, (2) Features: transaction amount, frequency, time patterns, location, device, (3) Models: isolation forest for anomaly detection, XGBoost/LightGBM for supervised classification, (4) Real-time scoring with low latency, (5) Optimize for recall (catch fraud) with acceptable precision (not too many false alarms), (6) Threshold tuning based on business cost of fraud vs investigation cost, (7) Model monitoring for concept drift as fraud patterns evolve.",
    category: "Applied ML",
    difficulty: "MEDIUM" as const,
    tags: ["fraud-detection", "anomaly-detection", "applied-ml"],
  },
  {
    questionText: "What is Bayesian statistics and how does it differ from frequentist?",
    idealAnswer: "Frequentist: probability is long-run frequency, parameters are fixed, inference via p-values and confidence intervals. Bayesian: probability represents belief, parameters have distributions, inference via posterior = prior × likelihood / evidence (Bayes' theorem). Bayesian advantages: incorporates prior knowledge, natural uncertainty quantification, works with small samples. Disadvantages: prior selection can be subjective, computationally expensive (MCMC). Use PyMC or Stan for Bayesian modeling.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["bayesian", "statistics", "inference"],
  },
  {
    questionText: "How do you deploy a machine learning model to production?",
    idealAnswer: "Steps: (1) Export model (pickle, ONNX, or framework-specific format), (2) Wrap in REST API (FastAPI/Flask), (3) Containerize with Docker, (4) Deploy to cloud (AWS SageMaker, GCP Vertex AI, or Kubernetes), (5) Input validation and preprocessing pipeline, (6) Monitoring: prediction distribution drift, feature drift, model performance, (7) A/B testing new vs old model, (8) CI/CD for model updates, (9) Versioning: track model versions with MLflow. Consider latency requirements: batch prediction vs real-time serving.",
    category: "MLOps",
    difficulty: "MEDIUM" as const,
    tags: ["deployment", "mlops", "production"],
  },
  {
    questionText: "What is transfer learning and when is it useful?",
    idealAnswer: "Transfer learning reuses a model trained on a large dataset for a new, related task. Common in deep learning: take a pretrained model (ImageNet for vision, BERT for NLP), freeze early layers (general features), fine-tune later layers for your task. Useful when: limited labeled data, similar domains, expensive to train from scratch. Examples: fine-tune BERT for sentiment analysis, use ResNet features for medical imaging. Dramatically reduces training time and data requirements.",
    category: "Deep Learning",
    difficulty: "MEDIUM" as const,
    tags: ["transfer-learning", "deep-learning", "nlp"],
  },
  {
    questionText: "Explain the Central Limit Theorem and its importance.",
    idealAnswer: "CLT states that the sampling distribution of the mean approaches a normal distribution as sample size increases, regardless of the population distribution. With n ≥ 30, the sample mean is approximately normal with mean μ and standard error σ/√n. Importance: enables confidence intervals and hypothesis tests even for non-normal data, justifies using t-tests and z-tests, underlies most of inferential statistics. It's why many statistical methods assume normality.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["clt", "statistics", "probability"],
  },
  {
    questionText: "What is model interpretability and what tools exist?",
    idealAnswer: "Interpretability explains how a model makes predictions. Methods: (1) Intrinsically interpretable: linear regression coefficients, decision tree rules, (2) SHAP values: game-theoretic feature importance per prediction, (3) LIME: local surrogate models for individual predictions, (4) Partial dependence plots: feature effect on prediction, (5) Feature importance: permutation or tree-based. Critical for: regulated industries (finance, healthcare), debugging, stakeholder trust. Trade-off: complex models are often more accurate but less interpretable.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["interpretability", "shap", "explainability"],
  },
  {
    questionText: "How do you handle text data for machine learning?",
    idealAnswer: "Pipeline: (1) Preprocessing: lowercase, remove punctuation/stopwords, tokenization, lemmatization, (2) Vectorization: bag-of-words, TF-IDF (traditional), word embeddings (Word2Vec, GloVe), transformer embeddings (BERT), (3) Feature engineering: n-grams, text length, sentiment scores, (4) Modeling: Naive Bayes, logistic regression with TF-IDF for simple tasks; fine-tuned transformers for complex tasks. For production: use spaCy for preprocessing, Hugging Face for transformers.",
    category: "NLP",
    difficulty: "MEDIUM" as const,
    tags: ["nlp", "text-processing", "vectorization"],
  },
  {
    questionText: "What is ensemble learning and what types exist?",
    idealAnswer: "Ensemble learning combines multiple models for better performance. Types: (1) Bagging: parallel models on random subsets, average predictions (Random Forest), (2) Boosting: sequential models correcting previous errors (XGBoost, LightGBM), (3) Stacking: train a meta-model on base model predictions, (4) Voting: combine predictions by majority vote or averaging. Ensembles reduce variance (bagging) or bias (boosting). They almost always outperform individual models but at the cost of complexity and interpretability.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["ensemble", "bagging", "boosting"],
  },
  {
    questionText: "How do you perform feature importance analysis?",
    idealAnswer: "Methods: (1) Tree-based importance: built into Random Forest/XGBoost (Gini importance or gain), (2) Permutation importance: shuffle each feature, measure performance drop — model-agnostic, (3) SHAP values: consistent, theoretically grounded importance per feature and prediction, (4) Correlation with target, (5) Lasso coefficients (L1 zero out unimportant features). SHAP is the gold standard — provides both global and local importance. Always use test set for importance to avoid overfitting bias.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["feature-importance", "shap", "ml"],
  },
  {
    questionText: "What is SQL window functions? Give examples of their use in data analysis.",
    idealAnswer: "Window functions compute values across a set of rows related to the current row without collapsing them (unlike GROUP BY). Examples: `ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC)` for ranking within groups, `LAG(sales, 1) OVER (ORDER BY date)` for previous period comparison, `SUM(revenue) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)` for running totals. Essential for: rankings, running aggregates, year-over-year comparisons, sessionization.",
    category: "SQL",
    difficulty: "MEDIUM" as const,
    tags: ["sql", "window-functions", "analytics"],
  },
  {
    questionText: "Explain the concept of model drift and how to monitor it.",
    idealAnswer: "Model drift: model performance degrades over time as data patterns change. Types: (1) Data drift: input feature distributions change, (2) Concept drift: relationship between features and target changes, (3) Prediction drift: output distribution shifts. Monitoring: track prediction distributions, feature statistics, accuracy on recent labeled data. Detection: statistical tests (KS test, PSI), dashboards. Response: retrain on recent data, investigate root cause. Set up automated alerts for drift thresholds.",
    category: "MLOps",
    difficulty: "MEDIUM" as const,
    tags: ["model-drift", "monitoring", "mlops"],
  },
  {
    questionText: "What is dimensionality reduction and what techniques exist beyond PCA?",
    idealAnswer: "Dimensionality reduction projects high-dimensional data to fewer dimensions. Linear: PCA (variance maximization), LDA (class separation). Non-linear: t-SNE (preserves local structure, good for visualization), UMAP (faster, preserves global structure better), autoencoders (neural network based). Use cases: visualization (t-SNE/UMAP), noise reduction (PCA), feature extraction. Choose PCA for linear relationships, t-SNE/UMAP for visualization, autoencoders for complex non-linear mappings.",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["dimensionality-reduction", "tsne", "umap"],
  },
  {
    questionText: "How do you communicate data science results to non-technical stakeholders?",
    idealAnswer: "Principles: (1) Lead with business impact, not methodology, (2) Visualize findings — clear charts with labels and annotations, (3) Translate metrics to business terms (model improves revenue by X%, not 'AUC is 0.92'), (4) Tell a story: context → problem → finding → recommendation, (5) Address uncertainty honestly, (6) Provide actionable next steps, (7) Use executive summaries. Avoid: jargon, excessive detail, burying conclusions. A dashboard is often worth more than a 50-page report.",
    category: "Communication",
    difficulty: "MEDIUM" as const,
    tags: ["communication", "storytelling", "stakeholders"],
  },
  {
    questionText: "What is sampling bias and how do you address it?",
    idealAnswer: "Sampling bias occurs when the sample doesn't represent the population — some groups are over/under-represented. Types: selection bias, survivorship bias, non-response bias, convenience sampling. Address by: random sampling, stratified sampling (ensure subgroup representation), weighting to match population demographics, awareness in experimental design. In ML: stratified train/test splits, demographic-aware evaluation. Always question whether your data represents the target population.",
    category: "Statistics",
    difficulty: "MEDIUM" as const,
    tags: ["sampling", "bias", "statistics"],
  },
  {
    questionText: "How would you approach a churn prediction problem?",
    idealAnswer: "Steps: (1) Define churn (no purchase in 90 days, subscription cancellation), (2) Feature engineering: recency, frequency, monetary value (RFM), engagement metrics, support tickets, tenure, (3) Handle class imbalance (churners are minority), (4) Models: logistic regression (interpretable), XGBoost (performance), survival analysis (time-to-churn), (5) Evaluate: precision-recall curve, lift chart, (6) Segment high-risk customers for targeted interventions, (7) Monitor and retrain as patterns change. Business impact: predict early enough to act.",
    category: "Applied ML",
    difficulty: "MEDIUM" as const,
    tags: ["churn", "classification", "applied-ml"],
  },
  {
    questionText: "What is the difference between SVM and logistic regression?",
    idealAnswer: "Logistic regression models probability of class membership using a sigmoid function — provides calibrated probabilities, interpretable coefficients. SVM finds the maximum-margin hyperplane separating classes — focuses on support vectors (boundary points). SVM with kernels handles non-linear boundaries. Logistic regression is better when: probabilities are needed, features are informative, interpretability matters. SVM is better with: high-dimensional data, clear margins, small datasets. Both are linear classifiers (without kernels).",
    category: "Machine Learning",
    difficulty: "MEDIUM" as const,
    tags: ["svm", "logistic-regression", "classification"],
  },

  // ===== HARD (30 questions) =====
  {
    questionText: "How would you design an ML system for real-time personalized product recommendations?",
    idealAnswer: "Architecture: (1) Offline: train collaborative filtering (matrix factorization) and content-based models on interaction history, (2) Near-real-time: update user embeddings with recent clicks via streaming pipeline (Kafka → Flink), (3) Online: two-stage retrieval — candidate generation (ANN search with FAISS/Milvus, 1000→100 items) then ranking (neural network scoring, 100→10), (4) Feature store for real-time features (Feast/Redis), (5) A/B testing framework, (6) Handle cold start: popular items, content-based for new users, (7) Diversity injection to avoid filter bubbles. Evaluate: CTR, engagement, revenue per session.",
    category: "System Design",
    difficulty: "HARD" as const,
    tags: ["recommendations", "system-design", "real-time"],
  },
  {
    questionText: "Explain the mathematics behind logistic regression and how it relates to maximum likelihood estimation.",
    idealAnswer: "Logistic regression models P(y=1|x) = σ(wᵀx + b) where σ is sigmoid. The likelihood for N samples: L = Π P(yᵢ|xᵢ). Log-likelihood: ℓ = Σ [yᵢ log(pᵢ) + (1-yᵢ) log(1-pᵢ)] — this is binary cross-entropy. MLE maximizes this (equivalent to minimizing cross-entropy loss). Gradient: ∂ℓ/∂w = Σ (yᵢ - pᵢ)xᵢ. Solved iteratively via gradient descent or Newton's method (IRLS). Convex optimization guarantees global optimum. Regularization adds -λ||w||² term for L2.",
    category: "Machine Learning",
    difficulty: "HARD" as const,
    tags: ["logistic-regression", "mle", "mathematics"],
  },
  {
    questionText: "How would you build a real-time anomaly detection system for time series data?",
    idealAnswer: "Architecture: (1) Feature extraction: rolling statistics (mean, std, percentiles), seasonal decomposition, spectral features, (2) Models: statistical (Z-score on residuals after decomposition), isolation forest (for multivariate), LSTM autoencoder (learns normal patterns, anomalies have high reconstruction error), (3) Streaming: Kafka → feature computation → model scoring → alert system, (4) Adaptive thresholds based on time-of-day/seasonality, (5) Feedback loop: human-in-the-loop to label false positives, retrain periodically, (6) Multi-scale detection: point anomalies, contextual, collective. Handle concept drift with sliding windows.",
    category: "Applied ML",
    difficulty: "HARD" as const,
    tags: ["anomaly-detection", "time-series", "real-time"],
  },
  {
    questionText: "Explain the attention mechanism in transformers and why it revolutionized NLP.",
    idealAnswer: "Self-attention computes weighted relationships between all positions in a sequence. For each token, compute Query, Key, Value vectors (learned linear projections). Attention(Q,K,V) = softmax(QKᵀ/√d_k)V. Multi-head attention runs this in parallel with different learned projections. Revolutionary because: (1) captures long-range dependencies without recurrence, (2) fully parallelizable (unlike RNNs), (3) position-agnostic (use positional encoding), (4) enables transfer learning (pretrain on large corpora, fine-tune on tasks). Foundation for BERT, GPT, and modern LLMs.",
    category: "Deep Learning",
    difficulty: "HARD" as const,
    tags: ["transformers", "attention", "nlp"],
  },
  {
    questionText: "How do you handle concept drift in a production ML system?",
    idealAnswer: "Detection: (1) Monitor prediction distribution (PSI > 0.2 = significant), (2) Track performance on recent labeled data, (3) Feature distribution monitoring (KS test, Jensen-Shannon divergence), (4) Residual analysis over time windows. Response: (1) Scheduled retraining (weekly/monthly), (2) Triggered retraining when drift detected, (3) Online learning for gradual adaptation, (4) Ensemble of models from different time periods, (5) Human-in-the-loop labeling for new patterns. Infrastructure: MLflow for model versioning, automated pipelines, rollback capability. Distinguish gradual drift from sudden shifts.",
    category: "MLOps",
    difficulty: "HARD" as const,
    tags: ["concept-drift", "mlops", "monitoring"],
  },
  {
    questionText: "Design an end-to-end ML pipeline for a credit scoring model with fairness constraints.",
    idealAnswer: "Pipeline: (1) Data: financial history, employment, demographics — audit for bias in historical decisions, (2) Feature engineering: credit utilization, payment history, income stability — exclude protected attributes directly but check for proxies, (3) Model: XGBoost with calibrated probabilities, (4) Fairness: equalized odds or demographic parity constraints, adversarial debiasing, (5) Interpretability: SHAP values for individual decisions (regulatory requirement), (6) Validation: stratified evaluation across demographic groups, (7) Monitoring: disparate impact ratio, performance by subgroup, (8) Documentation: model card with intended use, limitations, fairness metrics.",
    category: "Applied ML",
    difficulty: "HARD" as const,
    tags: ["credit-scoring", "fairness", "responsible-ai"],
  },
  {
    questionText: "Explain variational autoencoders (VAEs) and how they differ from regular autoencoders.",
    idealAnswer: "Regular autoencoders learn a deterministic encoding z = f(x) and reconstruct x̂ = g(z). VAEs learn a probabilistic encoding: encoder outputs μ and σ of a distribution q(z|x), sample z using reparameterization trick (z = μ + σ·ε, ε~N(0,1) for backprop). Loss = reconstruction loss + KL divergence between q(z|x) and prior p(z)=N(0,1). This creates a smooth, continuous latent space that enables generation of new samples by sampling from p(z). VAEs are generative models; regular AEs are not. Trade-off: VAEs produce blurrier but more diverse outputs than GANs.",
    category: "Deep Learning",
    difficulty: "HARD" as const,
    tags: ["vae", "generative", "deep-learning"],
  },
  {
    questionText: "How would you design a large-scale data processing pipeline using Apache Spark?",
    idealAnswer: "Architecture: (1) Ingest from diverse sources (S3, Kafka, databases) using Spark structured streaming or batch, (2) Schema validation and data quality checks (Great Expectations), (3) Transform with PySpark: joins, aggregations, UDFs — use DataFrame API over RDDs for optimization, (4) Feature engineering at scale, (5) Store in data lakehouse (Delta Lake/Iceberg) for ACID transactions, (6) Serve to ML models and BI tools. Optimization: partition by key columns, cache intermediate results, avoid UDFs when possible (Catalyst optimizer can't optimize them), right-size executors. Monitor with Spark UI for stage analysis.",
    category: "Data Engineering",
    difficulty: "HARD" as const,
    tags: ["spark", "data-engineering", "big-data"],
  },
  {
    questionText: "Explain the mathematics behind gradient boosting machines.",
    idealAnswer: "GBM fits sequential models to the negative gradient (pseudo-residuals) of the loss function. For MSE loss, pseudo-residual = y - F(x). At step m: (1) Compute residuals rᵢ = -∂L/∂F(xᵢ), (2) Fit a base learner hₘ to residuals, (3) Line search for optimal step size γₘ = argmin Σ L(yᵢ, Fₘ₋₁(xᵢ) + γhₘ(xᵢ)), (4) Update: Fₘ = Fₘ₋₁ + η·γₘ·hₘ. XGBoost adds second-order Taylor expansion of loss (uses both gradient and Hessian) for better approximation, plus L1/L2 regularization on tree complexity. This generalized framework works with any differentiable loss function.",
    category: "Machine Learning",
    difficulty: "HARD" as const,
    tags: ["gradient-boosting", "mathematics", "xgboost"],
  },
  {
    questionText: "How do you design an experiment to measure the causal impact of a product change?",
    idealAnswer: "Methods by rigor: (1) A/B test (gold standard): random assignment, large sample, measure treatment effect with t-test/bootstrap, control for multiple comparisons (Bonferroni), check for novelty/primacy effects, (2) Difference-in-differences: when randomization isn't possible, compare treatment vs control groups before and after, (3) Regression discontinuity: exploit a threshold for quasi-random assignment, (4) Instrumental variables: find a variable that affects treatment but not outcome directly, (5) Propensity score matching: create comparable groups from observational data. Always: define metrics upfront, calculate required sample size, pre-register analysis plan.",
    category: "Statistics",
    difficulty: "HARD" as const,
    tags: ["causal-inference", "experimentation", "ab-testing"],
  },
  {
    questionText: "How would you build a multi-task learning system for related prediction tasks?",
    idealAnswer: "Architecture: shared layers learn common representations, task-specific heads branch for each task. Benefits: tasks regularize each other, shared features improve data efficiency, one model serves multiple needs. Design: (1) Identify related tasks with shared features, (2) Hard parameter sharing (shared base) or soft sharing (constrained separate networks), (3) Loss weighting: manual, uncertainty-based (Kendall), or GradNorm for dynamic balancing, (4) Task sampling strategy for training. Examples: predict click AND conversion, classify sentiment AND detect topic. Challenges: negative transfer when tasks conflict.",
    category: "Deep Learning",
    difficulty: "HARD" as const,
    tags: ["multi-task", "deep-learning", "architecture"],
  },
  {
    questionText: "Explain how BERT works and how to fine-tune it for a specific NLP task.",
    idealAnswer: "BERT (Bidirectional Encoder Representations from Transformers): pretrained on masked language modeling (predict masked words using both left and right context) and next sentence prediction. Architecture: 12 transformer encoder layers, 768 hidden size, 12 attention heads (BERT-base). Fine-tuning: (1) Add task-specific head (classification: [CLS] token → linear layer, NER: each token → linear), (2) Use pretrained weights as initialization, (3) Train on task data with small learning rate (2e-5), (4) Few epochs (3-5) to avoid catastrophic forgetting. Use tokenizer from the same pretrained model.",
    category: "NLP",
    difficulty: "HARD" as const,
    tags: ["bert", "transformers", "fine-tuning"],
  },
  {
    questionText: "How would you design a feature store for a machine learning platform?",
    idealAnswer: "A feature store centralizes feature computation, storage, and serving. Design: (1) Feature registry: metadata, owners, lineage, (2) Offline store (data warehouse): batch-computed features for training, (3) Online store (Redis/DynamoDB): low-latency features for serving, (4) Feature pipelines: batch (Spark) and streaming (Flink) computation, (5) Point-in-time correct joins to prevent leakage in training, (6) Feature versioning, (7) Monitoring: feature freshness, distribution drift. Tools: Feast (open source), Tecton (managed). Benefits: reuse features across models, consistency between training and serving.",
    category: "MLOps",
    difficulty: "HARD" as const,
    tags: ["feature-store", "mlops", "infrastructure"],
  },
  {
    questionText: "Explain GANs (Generative Adversarial Networks) and their training dynamics.",
    idealAnswer: "GANs have two networks: Generator (G) creates fake samples from noise, Discriminator (D) distinguishes real from fake. Training is a minimax game: min_G max_D E[log D(x)] + E[log(1-D(G(z)))]. D improves at detecting fakes → G improves at creating realistic samples. Challenges: mode collapse (G produces limited variety), training instability, vanishing gradients. Solutions: Wasserstein GAN (WGAN) uses Earth Mover's distance, spectral normalization, progressive growing. Applications: image generation, data augmentation, super-resolution, style transfer.",
    category: "Deep Learning",
    difficulty: "HARD" as const,
    tags: ["gans", "generative", "deep-learning"],
  },
  {
    questionText: "How do you design an ML system that handles data at scale (terabytes)?",
    idealAnswer: "Infrastructure: (1) Storage: data lake (S3 + Delta Lake) with partitioning by date/key columns, (2) Processing: Spark for batch, Flink for streaming, (3) Feature computation: distributed processing with output to feature store, (4) Training: distributed training (Horovod, PyTorch DDP) on GPU clusters, or gradient accumulation for large batches, (5) Data sampling strategies for iterative development, (6) Efficient data formats (Parquet, columnar) with predicate pushdown, (7) Data versioning (DVC, lakehouse time travel), (8) Monitoring pipeline costs and latency. Start with sampled data for prototyping, scale to full dataset for production.",
    category: "Data Engineering",
    difficulty: "HARD" as const,
    tags: ["big-data", "distributed", "infrastructure"],
  },
  {
    questionText: "Explain the bias-variance decomposition for different model families.",
    idealAnswer: "MSE = Bias² + Variance + Irreducible Error. Linear regression: high bias (assumes linear relationship), low variance — underfits complex patterns. k-NN (k=1): zero bias on training data, high variance — overfits. Deep neural networks: low bias (universal approximators), high variance without regularization. Random Forest: reduces variance via averaging while maintaining low bias of deep trees. Gradient boosting: reduces bias sequentially. Understanding this guides model selection: complex data → low-bias models with regularization; small datasets → higher-bias models.",
    category: "Machine Learning",
    difficulty: "HARD" as const,
    tags: ["bias-variance", "theory", "ml"],
  },
  {
    questionText: "How would you build an end-to-end NLP pipeline for document understanding?",
    idealAnswer: "Pipeline: (1) Document ingestion: OCR for scanned docs (Tesseract, AWS Textract), PDF parsing, (2) Preprocessing: sentence segmentation, tokenization, entity extraction, (3) Embeddings: document-level (Sentence-BERT, Doc2Vec), (4) Tasks: classification (topic/type), NER (extract key entities), relation extraction, summarization, question answering, (5) RAG architecture: chunk documents, embed and index in vector DB (Pinecone/Weaviate), retrieve relevant chunks for LLM queries, (6) Evaluation: task-specific metrics, human evaluation. Use Hugging Face pipelines for modular components.",
    category: "NLP",
    difficulty: "HARD" as const,
    tags: ["nlp", "document-understanding", "rag"],
  },
  {
    questionText: "Design an ML experiment tracking and model management system.",
    idealAnswer: "Components: (1) Experiment tracking (MLflow): log parameters, metrics, artifacts for every run, (2) Model registry: versioned models with stage labels (staging, production), approval workflow, (3) Reproducibility: log code version (git hash), data version (DVC), environment (Docker/conda), random seeds, (4) Comparison UI: compare runs across metrics, (5) Artifact storage: model files, feature importance plots, evaluation reports, (6) Integration: auto-log from scikit-learn/PyTorch, CI/CD triggers model validation. Infrastructure: MLflow server + S3 backend + PostgreSQL for metadata.",
    category: "MLOps",
    difficulty: "HARD" as const,
    tags: ["experiment-tracking", "mlflow", "mlops"],
  },
  {
    questionText: "Explain how to implement and evaluate a causal inference model from observational data.",
    idealAnswer: "Methods: (1) Propensity Score Matching: estimate P(treatment|X) with logistic regression, match treated/control by propensity score, estimate ATE, (2) Inverse Propensity Weighting (IPW): weight samples by 1/P(treatment|X), (3) Doubly Robust Estimation: combines outcome model + propensity model — consistent if either is correct, (4) Causal forests: heterogeneous treatment effects, (5) DoWhy framework for causal DAGs and sensitivity analysis. Evaluate: refutation tests (random cause, placebo treatment), sensitivity to unobserved confounders. Assumptions: positivity, consistency, no unobserved confounders (untestable).",
    category: "Statistics",
    difficulty: "HARD" as const,
    tags: ["causal-inference", "propensity-score", "statistics"],
  },
  {
    questionText: "How would you design a system for automated model retraining and deployment?",
    idealAnswer: "Architecture: (1) Data pipeline monitors new data arrival (Airflow/Prefect), (2) Drift detection triggers retraining, (3) Training pipeline: feature computation → model training → hyperparameter tuning (Optuna) → evaluation, (4) Model validation: compare to current production model on holdout + fairness checks, (5) Auto-deploy if metrics improve (canary deployment), (6) Shadow mode: run new model alongside production, compare outputs, (7) Rollback: if performance degrades, revert to previous version, (8) Alerts: Slack/PagerDuty for failures. Use: Kubeflow Pipelines or SageMaker Pipelines for orchestration, MLflow for model registry.",
    category: "MLOps",
    difficulty: "HARD" as const,
    tags: ["mlops", "automation", "deployment"],
  },
  {
    questionText: "Explain the mathematical foundations of SVM with kernels.",
    idealAnswer: "SVM maximizes margin between classes. Primal: min ½||w||² s.t. yᵢ(wᵀxᵢ+b) ≥ 1. Dual (Lagrangian): max Σαᵢ - ½ΣΣαᵢαⱼyᵢyⱼxᵢᵀxⱼ s.t. αᵢ ≥ 0, Σαᵢyᵢ = 0. The kernel trick: replace xᵢᵀxⱼ with K(xᵢ,xⱼ) to implicitly map to high-dimensional space. Common kernels: RBF K(x,y) = exp(-γ||x-y||²), polynomial K(x,y) = (xᵀy+c)^d. Mercer's theorem ensures valid kernels. Soft margin (C parameter) allows misclassification. SMO algorithm solves the dual efficiently. Key insight: only support vectors (αᵢ > 0) define the boundary.",
    category: "Machine Learning",
    difficulty: "HARD" as const,
    tags: ["svm", "kernels", "mathematics"],
  },
  {
    questionText: "How would you approach building a multi-modal ML system combining text, images, and tabular data?",
    idealAnswer: "Architecture: (1) Modality-specific encoders: BERT/Sentence-BERT for text, ResNet/ViT for images, MLP/XGBoost for tabular, (2) Fusion strategies: early fusion (concatenate features), late fusion (combine predictions), attention-based fusion (cross-modal attention), (3) Joint training with modality-specific losses + combined loss, (4) Handle missing modalities: masking with learned default embeddings, (5) Data pipeline: align modalities per sample, augment per modality. Evaluation: ablation studies per modality to measure contribution. Example: product search using image + description + attributes.",
    category: "Deep Learning",
    difficulty: "HARD" as const,
    tags: ["multi-modal", "deep-learning", "architecture"],
  },
  {
    questionText: "How do you evaluate and improve the calibration of a probabilistic model?",
    idealAnswer: "Calibration means predicted probabilities match actual frequencies (if you predict 70%, 70% should be positive). Evaluate: (1) Reliability diagram: plot predicted probability bins vs actual frequency, (2) Expected Calibration Error (ECE): weighted average of |predicted - actual| per bin, (3) Brier score: MSE of probability predictions. Improve: (1) Platt scaling: fit logistic regression on model outputs, (2) Isotonic regression: non-parametric calibration, (3) Temperature scaling: divide logits by T. Calibrate on held-out data. Critical for: medical diagnosis, risk assessment, decision-making systems.",
    category: "Machine Learning",
    difficulty: "HARD" as const,
    tags: ["calibration", "probability", "evaluation"],
  },
  {
    questionText: "Design a machine learning system for real-time bidding in online advertising.",
    idealAnswer: "Architecture: (1) Prediction: CTR (click-through rate) model predicts P(click|ad, user, context) in <10ms, (2) Bidding: bid = expected_value × P(click) with budget constraints, (3) Features: user demographics, browsing history, ad content, context (time, device, page), (4) Model: logistic regression or small neural network for latency, gradient-boosted trees for offline evaluation, (5) Feature serving: precomputed user/ad features in Redis, (6) Explore/exploit: Thompson sampling for new ads, (7) Budget pacing: spread spend evenly, (8) Feedback loop: click/conversion labels for retraining (delayed rewards). Scale: handle millions of bid requests per second with horizontal scaling.",
    category: "System Design",
    difficulty: "HARD" as const,
    tags: ["rtb", "advertising", "system-design"],
  },
  {
    questionText: "Explain how neural architecture search (NAS) works.",
    idealAnswer: "NAS automates neural network architecture design. Approaches: (1) Reinforcement learning: controller network proposes architectures, trained with reward = validation accuracy (computationally expensive), (2) Evolutionary: population of architectures, mutate and select fittest, (3) Differentiable (DARTS): relax discrete architecture choices to continuous, optimize with gradient descent — much faster, (4) One-shot NAS: train a supernet containing all possible architectures, evaluate subnetworks. Search space: layer types, connections, hyperparameters. Efficient NAS: weight sharing, early stopping, proxy tasks. Trade-off: computational cost vs finding optimal architectures.",
    category: "Deep Learning",
    difficulty: "HARD" as const,
    tags: ["nas", "architecture-search", "deep-learning"],
  },
  {
    questionText: "How would you design a responsible AI framework for your organization?",
    idealAnswer: "Framework: (1) Fairness: define protected attributes, measure disparate impact, test across subgroups, apply bias mitigation (pre/in/post-processing), (2) Transparency: model cards documenting purpose, limitations, performance by group, (3) Privacy: differential privacy, federated learning, data minimization, (4) Robustness: adversarial testing, out-of-distribution detection, (5) Accountability: audit trail, human-in-the-loop for high-stakes decisions, (6) Governance: review board, risk assessment for new models, incident response process. Continuous: monitoring for drift and bias amplification, regular fairness audits, user feedback mechanisms.",
    category: "Ethics",
    difficulty: "HARD" as const,
    tags: ["responsible-ai", "fairness", "ethics"],
  },
  {
    questionText: "Explain the EM (Expectation-Maximization) algorithm and its applications.",
    idealAnswer: "EM iteratively estimates parameters with hidden variables. E-step: compute expected values of hidden variables given current parameters (posterior probabilities). M-step: maximize likelihood with expected hidden variables fixed. Guaranteed to increase (or maintain) likelihood each iteration, converging to local maximum. Applications: (1) Gaussian Mixture Models: E-step assigns soft cluster probabilities, M-step updates means/covariances, (2) Missing data imputation, (3) Hidden Markov Models (Baum-Welch), (4) Latent factor models. Limitation: converges to local optimum — use multiple random initializations.",
    category: "Machine Learning",
    difficulty: "HARD" as const,
    tags: ["em-algorithm", "gmm", "mathematics"],
  },
  {
    questionText: "How would you build a system for automated data quality monitoring?",
    idealAnswer: "Components: (1) Schema validation: column types, nullable checks, value ranges (Great Expectations), (2) Statistical monitoring: distribution tests (KS, chi-squared) against baseline, (3) Freshness checks: data arrived on time, completeness percentage, (4) Referential integrity: foreign key validations, (5) Business rules: domain-specific assertions (prices > 0, dates in range), (6) Anomaly detection: monitor row counts, null rates, cardinality changes, (7) Dashboard and alerting: Slack/PagerDuty for failures. Pipeline: run checks after each data pipeline stage, block downstream if critical checks fail. Document expectations in code, version with data pipeline.",
    category: "Data Engineering",
    difficulty: "HARD" as const,
    tags: ["data-quality", "monitoring", "data-engineering"],
  },
  {
    questionText: "How do you implement and evaluate a knowledge graph for entity resolution?",
    idealAnswer: "Knowledge graph: (1) Schema: define entity types (Person, Company, Product) and relationships, (2) Ingestion: extract entities from structured (DBs) and unstructured (NER on text) sources, (3) Entity resolution: determine if two mentions refer to the same entity — blocking (candidate pairs), matching (feature similarity: name, address, context), clustering (connected components or probabilistic), (4) Link prediction: embed entities and relations (TransE, ComplEx) to predict missing links, (5) Evaluation: precision/recall on labeled pairs, manual audit sample. Storage: Neo4j or Amazon Neptune. Query: Cypher or SPARQL. Challenges: scalability, handling ambiguity, keeping the graph updated.",
    category: "Applied ML",
    difficulty: "HARD" as const,
    tags: ["knowledge-graph", "entity-resolution", "nlp"],
  },
  {
    questionText: "How would you approach a new data science problem from scratch?",
    idealAnswer: "Framework: (1) Problem definition: translate business question to measurable ML objective, define success criteria, (2) Data audit: what's available, quality, biases, gaps, (3) EDA: distributions, correlations, patterns, anomalies, (4) Baseline model: simple approach (logistic regression, heuristic rules) to set a benchmark, (5) Feature engineering: domain knowledge + automated feature generation, (6) Model selection: start simple, add complexity if needed, compare via cross-validation, (7) Evaluation: appropriate metrics, fairness analysis, error analysis, (8) Deployment plan: latency requirements, monitoring, retraining schedule. Iterate: the first model is never the last.",
    category: "Methodology",
    difficulty: "HARD" as const,
    tags: ["methodology", "problem-solving", "data-science"],
  },
];
