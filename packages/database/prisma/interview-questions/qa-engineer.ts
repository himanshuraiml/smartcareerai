export const qaEngineerQuestions = [
  // ==========================================
  // TEST AUTOMATION (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is test automation, and why is it important in software development?",
    idealAnswer: "Test automation is the practice of using specialized tools and scripts to execute test cases automatically, rather than performing them manually. It is important because it significantly reduces the time required for regression testing, improves test coverage, and ensures consistent and repeatable results. Automated tests can be run frequently during CI/CD pipelines, enabling faster feedback loops and earlier detection of defects. This ultimately leads to higher software quality and faster release cycles.",
    category: "Test Automation",
    difficulty: "EASY" as const,
    tags: ["test-automation", "fundamentals", "regression-testing"],
  },
  {
    questionText: "What are the key factors you consider when deciding which test cases to automate?",
    idealAnswer: "When deciding which test cases to automate, I consider factors such as test frequency, stability of the feature, complexity of manual execution, and return on investment. Tests that are executed repeatedly across multiple regression cycles, such as smoke tests and critical path tests, are ideal candidates for automation. I also prioritize tests that involve large data sets or complex calculations where manual testing is error-prone. Conversely, tests for features that change frequently or require subjective human judgment are typically better left as manual tests.",
    category: "Test Automation",
    difficulty: "EASY" as const,
    tags: ["test-automation", "test-selection", "ROI"],
  },
  {
    questionText: "What is the Page Object Model (POM) design pattern in test automation?",
    idealAnswer: "The Page Object Model is a design pattern that creates an abstraction layer between test scripts and the UI by representing each web page or component as a class. Each page class contains the locators for elements on that page and methods that represent user interactions with those elements. This pattern improves test maintainability because when the UI changes, only the page object class needs to be updated rather than every test that interacts with that page. POM also promotes code reusability and makes test scripts more readable by separating the what (test logic) from the how (page interaction details).",
    category: "Test Automation",
    difficulty: "EASY" as const,
    tags: ["test-automation", "page-object-model", "design-patterns"],
  },
  {
    questionText: "How do you handle dynamic elements and waits in test automation frameworks?",
    idealAnswer: "Handling dynamic elements requires using explicit waits that pause execution until a specific condition is met, such as an element becoming visible, clickable, or present in the DOM. I avoid using hard-coded sleep statements because they make tests slow and unreliable. Most frameworks provide built-in wait mechanisms like WebDriverWait in Selenium or cy.get with built-in retry in Cypress. For highly dynamic content, I use strategies like waiting for network requests to complete, polling for element state changes, or using custom wait conditions that check for application-specific readiness indicators.",
    category: "Test Automation",
    difficulty: "MEDIUM" as const,
    tags: ["test-automation", "dynamic-elements", "waits", "synchronization"],
  },
  {
    questionText: "Explain the concept of data-driven testing and how you implement it.",
    idealAnswer: "Data-driven testing is a methodology where test logic is separated from test data, allowing the same test script to be executed with multiple sets of input data. This is implemented by storing test data externally in sources like CSV files, Excel spreadsheets, JSON files, or databases, and then iterating over each data set during test execution. Frameworks like TestNG provide @DataProvider annotations, while Jest and Cypress support test parameterization through describe.each or cy.fixture. This approach maximizes test coverage with minimal code duplication and makes it easy to add new test scenarios by simply adding data rows without modifying the test script.",
    category: "Test Automation",
    difficulty: "MEDIUM" as const,
    tags: ["test-automation", "data-driven", "test-data-management"],
  },
  {
    questionText: "What is the difference between keyword-driven and behavior-driven test automation frameworks?",
    idealAnswer: "Keyword-driven testing uses a table-based approach where test steps are defined as keywords mapped to reusable functions, with test data stored separately in spreadsheets or tables. Each keyword represents an action like 'click', 'enter text', or 'verify', making it accessible to non-technical testers. Behavior-driven development (BDD) frameworks like Cucumber use natural language Given-When-Then syntax to describe test scenarios in business-readable format, with step definitions mapping to automation code. While both promote readability and reusability, BDD focuses more on collaboration between business stakeholders and developers, whereas keyword-driven testing focuses on abstracting technical complexity behind action keywords.",
    category: "Test Automation",
    difficulty: "MEDIUM" as const,
    tags: ["test-automation", "BDD", "keyword-driven", "frameworks"],
  },
  {
    questionText: "How do you implement parallel test execution in your automation framework, and what challenges does it present?",
    idealAnswer: "Parallel test execution is implemented by configuring the test runner to distribute tests across multiple threads, processes, or machines simultaneously. In Selenium, this can be achieved using Selenium Grid or cloud platforms like BrowserStack, while frameworks like TestNG and Jest support parallel execution through configuration. The main challenges include managing shared state and test data isolation to prevent tests from interfering with each other, handling thread-safe resource access like database connections, and ensuring proper cleanup between test runs. I address these by making tests independent and idempotent, using unique test data per thread, and employing container-based environments to provide isolated execution contexts.",
    category: "Test Automation",
    difficulty: "MEDIUM" as const,
    tags: ["test-automation", "parallel-execution", "selenium-grid", "scalability"],
  },
  {
    questionText: "How would you design a test automation framework from scratch for a large-scale microservices application?",
    idealAnswer: "For a microservices application, I would design a multi-layered framework that addresses testing at each service boundary. The architecture would include a unit test layer using Jest or Mocha for individual service logic, a contract testing layer using Pact to verify API contracts between services, an integration test layer for testing service interactions, and an end-to-end UI layer using Cypress or Playwright for critical user journeys. I would implement a shared utilities library for common functions like authentication, data setup, and API clients, along with a centralized reporting dashboard. The framework would integrate with Docker Compose for spinning up dependent services and use environment-based configuration to support multiple deployment targets. Test data management would use factory patterns for creating isolated test data per service.",
    category: "Test Automation",
    difficulty: "HARD" as const,
    tags: ["test-automation", "framework-design", "microservices", "architecture"],
  },
  {
    questionText: "Describe how you would implement a self-healing test automation mechanism to reduce test maintenance.",
    idealAnswer: "A self-healing mechanism automatically detects and adapts to UI changes that would normally cause test failures due to broken locators. I would implement this by maintaining a priority-ordered list of alternative locators for each element, including data-testid attributes, CSS selectors, XPath, and visual attributes. When the primary locator fails, the framework attempts fallback locators and, upon success, logs the change and updates the locator repository. More advanced approaches use machine learning models trained on the DOM structure to identify elements even when their attributes change significantly. The system would also include a reporting layer that notifies the team of healed tests so that the underlying locators can be permanently updated, preventing the accumulation of technical debt in the test suite.",
    category: "Test Automation",
    difficulty: "HARD" as const,
    tags: ["test-automation", "self-healing", "AI-testing", "maintenance"],
  },
  {
    questionText: "How do you measure and optimize the ROI of your test automation efforts?",
    idealAnswer: "I measure automation ROI by tracking metrics across several dimensions: time savings calculated by comparing manual execution time versus automated execution time multiplied by run frequency, defect detection rate showing bugs caught by automation before reaching production, and test coverage percentage across critical business flows. I also track automation maintenance cost including time spent updating broken tests and infrastructure costs. To optimize ROI, I focus on automating high-value regression tests first, implementing smart test selection that runs only tests affected by code changes using techniques like test impact analysis, and continuously retiring flaky or low-value tests. I present ROI data in dashboards that show trends over sprints, helping stakeholders understand the tangible value of our automation investment.",
    category: "Test Automation",
    difficulty: "HARD" as const,
    tags: ["test-automation", "ROI", "metrics", "optimization"],
  },

  // ==========================================
  // SELENIUM / CYPRESS (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is Selenium WebDriver, and how does it interact with web browsers?",
    idealAnswer: "Selenium WebDriver is an open-source browser automation tool that provides a programming interface for controlling web browsers programmatically. It communicates with browsers through browser-specific drivers like ChromeDriver or GeckoDriver, which translate WebDriver commands into native browser actions using the W3C WebDriver protocol. Unlike its predecessor Selenium RC, WebDriver directly interacts with the browser at the OS level, making it faster and more reliable. It supports multiple programming languages including Java, Python, JavaScript, and C#, allowing testers to write automation scripts in their preferred language.",
    category: "Selenium",
    difficulty: "EASY" as const,
    tags: ["selenium", "webdriver", "browser-automation", "fundamentals"],
  },
  {
    questionText: "What are the different types of locators available in Selenium, and which do you prefer?",
    idealAnswer: "Selenium provides eight types of locators: ID, Name, Class Name, Tag Name, Link Text, Partial Link Text, CSS Selector, and XPath. I prefer using data-testid or data-cy attributes with CSS selectors because they are specifically added for testing and are resilient to UI changes. When those are not available, I use ID locators as they are typically unique and fast. CSS selectors are my second choice because they are faster than XPath and widely supported. I reserve XPath for complex scenarios like traversing parent elements or locating elements by text content, as it is more flexible but slower in execution.",
    category: "Selenium",
    difficulty: "EASY" as const,
    tags: ["selenium", "locators", "CSS-selectors", "XPath"],
  },
  {
    questionText: "What is Cypress, and how does it differ from Selenium WebDriver?",
    idealAnswer: "Cypress is a modern JavaScript-based end-to-end testing framework that runs directly in the browser alongside the application, unlike Selenium which operates through external driver processes. This architecture gives Cypress native access to the DOM, network requests, and application state, enabling features like automatic waiting, time-travel debugging, and network request stubbing out of the box. However, Cypress only supports JavaScript/TypeScript and Chromium-based browsers plus Firefox, whereas Selenium supports multiple languages and browsers. Cypress is generally faster for single-browser testing and has a lower learning curve, but Selenium offers more flexibility for cross-browser testing at scale.",
    category: "Selenium",
    difficulty: "EASY" as const,
    tags: ["cypress", "selenium", "comparison", "e2e-testing"],
  },
  {
    questionText: "How do you handle iframes and multiple windows in Selenium WebDriver?",
    idealAnswer: "To handle iframes, I use the driver.switchTo().frame() method, which can accept a frame index, name/ID, or WebElement reference to switch the driver context into the iframe. After interacting with elements inside the iframe, I switch back to the main content using driver.switchTo().defaultContent() or to the parent frame using driver.switchTo().parentFrame(). For multiple windows, I store the original window handle using driver.getWindowHandle(), then iterate through driver.getWindowHandles() to identify and switch to the new window. After completing actions in the new window, I close it and switch back to the original window handle to continue the test flow.",
    category: "Selenium",
    difficulty: "MEDIUM" as const,
    tags: ["selenium", "iframes", "window-handling", "browser-context"],
  },
  {
    questionText: "Explain how Cypress handles asynchronous operations differently from Selenium.",
    idealAnswer: "Cypress handles asynchronous operations through an automatic command queue where each command is enqueued and executed in order, with built-in retry logic and automatic waiting. Unlike Selenium where you must explicitly add waits, Cypress commands automatically retry assertions for a configurable timeout period, checking the DOM repeatedly until the condition is met or the timeout expires. Cypress also provides cy.intercept() to control network requests and cy.wait() to pause for specific API calls to complete. This design eliminates most timing issues and flaky tests that plague Selenium suites, though it means Cypress commands are not standard Promises and cannot be mixed with async/await in the traditional JavaScript sense.",
    category: "Selenium",
    difficulty: "MEDIUM" as const,
    tags: ["cypress", "async", "automatic-waiting", "command-queue"],
  },
  {
    questionText: "How do you implement cross-browser testing with Selenium Grid?",
    idealAnswer: "Selenium Grid enables parallel cross-browser testing by distributing test execution across multiple machines and browsers. I set up a Grid hub that acts as a central coordinator and register node machines with different browser and OS configurations. Tests specify desired capabilities including browser type, version, and platform, and the hub routes each test to an appropriate node. In modern Selenium 4, Grid can be deployed as a standalone server, hub-node configuration, or fully distributed mode with separate router, distributor, and session map components. I typically containerize the Grid using the official Docker images and orchestrate it with Docker Compose or Kubernetes for scalable, reproducible cross-browser testing in CI/CD pipelines.",
    category: "Selenium",
    difficulty: "MEDIUM" as const,
    tags: ["selenium", "selenium-grid", "cross-browser", "distributed-testing"],
  },
  {
    questionText: "How do you handle file uploads and downloads in Cypress and Selenium?",
    idealAnswer: "In Selenium, file uploads are handled by sending the file path directly to the file input element using sendKeys(), which works for standard HTML file inputs. For custom upload widgets, I may need to interact with the underlying input element or use tools like AutoIt or Robot class for native OS dialogs. File downloads require configuring browser preferences to set a default download directory and disable download prompts, then verifying the file exists in the expected location. In Cypress, I use the cypress-file-upload plugin or the selectFile command to attach files to input elements, and for downloads, I intercept the download request or read the file from the downloads folder using cy.readFile(). Both frameworks require careful handling of download verification timing.",
    category: "Selenium",
    difficulty: "MEDIUM" as const,
    tags: ["selenium", "cypress", "file-upload", "file-download"],
  },
  {
    questionText: "How would you implement visual regression testing using Cypress or Selenium?",
    idealAnswer: "Visual regression testing captures screenshots of UI components and compares them against baseline images to detect unintended visual changes. With Cypress, I use plugins like cypress-image-snapshot or Percy that integrate directly into the test workflow. For Selenium, tools like Applitools Eyes or BackstopJS provide similar capabilities with AI-powered comparison that ignores dynamic content and anti-aliasing differences. I implement this by capturing baseline screenshots for each critical page and component state, storing them in version control, and running comparisons on each pull request. Key challenges include handling dynamic content like dates and animations, which I address by mocking time-dependent data and disabling animations during visual tests. I also configure appropriate comparison thresholds to prevent false positives from sub-pixel rendering differences across environments.",
    category: "Selenium",
    difficulty: "HARD" as const,
    tags: ["visual-regression", "cypress", "selenium", "screenshot-testing"],
  },
  {
    questionText: "Describe how you would build a custom Cypress plugin to handle authentication across multiple test suites efficiently.",
    idealAnswer: "I would create a custom Cypress plugin that programmatically authenticates users by bypassing the UI login flow, storing session tokens, and restoring them before each test. The plugin would expose a cy.login() custom command that makes direct API calls to the authentication endpoint, caches the returned JWT and refresh tokens, and sets them as cookies or localStorage entries. I would use Cypress session caching with cy.session() to preserve authentication state across tests within a spec file, dramatically reducing test execution time. For multiple user roles, the command would accept a role parameter and maintain separate cached sessions per role. The plugin would also handle token expiration by checking validity before reuse and refreshing when necessary, and include cleanup hooks to invalidate sessions after test completion to prevent state leakage.",
    category: "Selenium",
    difficulty: "HARD" as const,
    tags: ["cypress", "plugins", "authentication", "custom-commands"],
  },
  {
    questionText: "How do you debug and resolve flaky tests in Selenium or Cypress test suites?",
    idealAnswer: "Flaky test resolution begins with identifying the root cause through detailed analysis of failure patterns, screenshots, videos, and logs from multiple runs. Common causes include timing issues, test interdependencies, environment instability, and non-deterministic data. For timing issues in Selenium, I replace implicit waits with explicit waits targeting specific conditions and ensure proper synchronization points. In Cypress, I leverage its built-in retry-ability and add appropriate assertions rather than arbitrary waits. For test interdependencies, I refactor tests to be fully independent with proper setup and teardown. I also implement a quarantine mechanism that isolates known flaky tests, runs them separately with increased retries, and tracks their pass rates over time. Additionally, I use deterministic test data factories and mock external services to eliminate environmental variability.",
    category: "Selenium",
    difficulty: "HARD" as const,
    tags: ["selenium", "cypress", "flaky-tests", "debugging", "stability"],
  },

  // ==========================================
  // MANUAL TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is the difference between functional testing and non-functional testing?",
    idealAnswer: "Functional testing validates that the software behaves according to specified requirements by testing what the system does, including features like login, data processing, and calculations. Non-functional testing evaluates how the system performs by testing quality attributes such as performance, usability, security, reliability, and scalability. For example, verifying that a user can successfully submit a form is functional testing, while measuring how quickly the form loads under heavy traffic is non-functional testing. Both types are essential for delivering quality software, as a system that functions correctly but performs poorly under load will still fail to meet user expectations.",
    category: "Manual Testing",
    difficulty: "EASY" as const,
    tags: ["manual-testing", "functional-testing", "non-functional-testing", "fundamentals"],
  },
  {
    questionText: "What is the difference between smoke testing and sanity testing?",
    idealAnswer: "Smoke testing is a broad, shallow level of testing performed on a new build to verify that the most critical functionalities work before proceeding with more detailed testing. It acts as a gatekeeper that determines whether the build is stable enough for further testing. Sanity testing is a narrow, focused subset of testing performed after a minor change or bug fix to verify that the specific change works correctly and has not broken related functionality. While smoke testing covers many features superficially, sanity testing covers a few features deeply. Smoke testing is usually scripted and planned, whereas sanity testing is often unscripted and performed by the tester who identified the original issue.",
    category: "Manual Testing",
    difficulty: "EASY" as const,
    tags: ["manual-testing", "smoke-testing", "sanity-testing", "build-verification"],
  },
  {
    questionText: "What are the key components of a well-written test case?",
    idealAnswer: "A well-written test case includes a unique identifier, a descriptive title, preconditions that specify the required setup state, clearly numbered step-by-step instructions, expected results for each step, and the actual result field for recording outcomes. It should also include the test priority, associated requirement or user story ID, test data requirements, and the environment details. The steps should be written in simple, unambiguous language that any tester can follow without needing additional context. Good test cases are atomic, testing one specific scenario, and independent so they can be executed in any order without depending on other test case outcomes.",
    category: "Manual Testing",
    difficulty: "EASY" as const,
    tags: ["manual-testing", "test-cases", "test-documentation", "best-practices"],
  },
  {
    questionText: "Explain the concept of equivalence partitioning and boundary value analysis with examples.",
    idealAnswer: "Equivalence partitioning divides input data into groups where all values in a partition are expected to produce the same behavior, reducing the number of test cases needed. For an age field accepting 18-65, the partitions would be: invalid low (less than 18), valid (18-65), and invalid high (greater than 65), requiring only one test per partition. Boundary value analysis focuses on testing values at the edges of partitions where defects most commonly occur. For the same age field, boundary values would be 17, 18, 19, 64, 65, and 66. Combining both techniques provides efficient test coverage: equivalence partitioning ensures all logical groups are tested, while boundary value analysis targets the most error-prone values at partition boundaries.",
    category: "Manual Testing",
    difficulty: "MEDIUM" as const,
    tags: ["manual-testing", "equivalence-partitioning", "boundary-value", "test-design"],
  },
  {
    questionText: "What is exploratory testing, and how does it complement scripted testing?",
    idealAnswer: "Exploratory testing is a simultaneous approach to test design, execution, and learning where the tester dynamically creates and executes tests based on their understanding of the application, domain knowledge, and observations during the session. Unlike scripted testing where test cases are predefined, exploratory testing relies on the tester's creativity, intuition, and critical thinking to discover defects that formal test cases might miss. It is typically conducted in time-boxed sessions with a charter defining the scope and objective. Exploratory testing complements scripted testing by finding edge cases, usability issues, and integration defects that are difficult to anticipate during test design, while scripted tests provide consistent coverage of known requirements and regression scenarios.",
    category: "Manual Testing",
    difficulty: "MEDIUM" as const,
    tags: ["manual-testing", "exploratory-testing", "test-design", "session-based"],
  },
  {
    questionText: "How do you prioritize test cases when there is limited time for testing?",
    idealAnswer: "I use a risk-based testing approach that prioritizes test cases based on the probability and impact of failure. First, I identify the most critical business flows and features that directly affect revenue or user safety and test those first. Next, I consider areas with recent code changes or known complexity, as they have a higher probability of containing defects. I categorize tests into tiers: Tier 1 covers critical path and high-risk scenarios that must always be executed, Tier 2 covers important but lower-risk features, and Tier 3 covers edge cases and nice-to-have coverage. I also leverage test execution history and defect data to identify areas where bugs have historically clustered. This approach ensures that even with limited time, the testing effort is focused where it provides the most value in risk reduction.",
    category: "Manual Testing",
    difficulty: "MEDIUM" as const,
    tags: ["manual-testing", "test-prioritization", "risk-based-testing", "time-management"],
  },
  {
    questionText: "What is the difference between retesting and regression testing?",
    idealAnswer: "Retesting is the process of executing the exact same test case that previously failed to verify that a reported defect has been successfully fixed by the development team. It is narrowly focused on the specific bug fix and confirms the defect resolution. Regression testing is a broader activity that verifies the bug fix has not introduced new defects or broken existing functionality in other parts of the application. While retesting targets the specific failure scenario, regression testing covers a wider range of related and potentially impacted features. Both are essential after a bug fix: retesting confirms the fix works, and regression testing ensures the fix does not create side effects. Retesting is typically performed by the tester who reported the bug, while regression testing may be distributed across the team or automated.",
    category: "Manual Testing",
    difficulty: "MEDIUM" as const,
    tags: ["manual-testing", "retesting", "regression-testing", "defect-management"],
  },
  {
    questionText: "How do you design test cases for a complex feature with many interdependent conditions using decision tables?",
    idealAnswer: "Decision tables systematically map all combinations of input conditions to their expected outcomes, ensuring complete coverage of complex business logic. I start by identifying all relevant conditions and their possible values, then list all possible actions or outcomes. For a feature with N binary conditions, there are 2^N possible combinations, though I can reduce these using techniques like collapsing redundant rules where certain conditions do not affect the outcome. For example, testing a loan approval system with conditions for credit score, income level, employment status, and existing debt would generate a decision table covering each valid combination. I then derive one test case per unique rule in the table. This method is superior to ad-hoc testing for complex features because it guarantees that no condition combination is overlooked, and each rule traces back to a specific business requirement for traceability.",
    category: "Manual Testing",
    difficulty: "HARD" as const,
    tags: ["manual-testing", "decision-tables", "test-design", "complex-logic"],
  },
  {
    questionText: "Describe how you would create and manage a comprehensive test plan for a major product release.",
    idealAnswer: "I would start by analyzing the release scope including new features, enhancements, and bug fixes, then define the test strategy covering test levels, types, environments, and entry/exit criteria. The test plan document would include the objectives, scope, resource allocation, schedule, risk assessment with mitigation strategies, tool requirements, and communication protocols. I would break the plan into phases: requirement analysis and test design, test environment setup, test execution across functional, integration, regression, performance, and security testing, followed by user acceptance testing. Risk-based prioritization would guide effort allocation, and I would establish clear metrics like defect density, test coverage, and defect leakage rate to measure quality. The plan would include contingency strategies for schedule compression, a defect triage process with defined severity and priority classifications, and sign-off criteria that must be met before approving the release.",
    category: "Manual Testing",
    difficulty: "HARD" as const,
    tags: ["manual-testing", "test-plan", "release-management", "strategy"],
  },
  {
    questionText: "How do you apply state transition testing for a complex workflow like an e-commerce order lifecycle?",
    idealAnswer: "State transition testing models the system as a finite state machine where I identify all possible states, valid transitions, triggering events, guard conditions, and associated actions. For an e-commerce order, states might include Created, Payment Pending, Payment Confirmed, Processing, Shipped, Delivered, Returned, Refunded, and Cancelled, with transitions triggered by events like payment submission, shipment dispatch, or cancellation request. I create a state transition diagram and derive a state transition table listing every state-event-condition-action-next-state combination. Test cases are designed to cover all valid transitions (positive paths), all invalid transitions (attempting actions not allowed in the current state), and sequences that cover multiple state changes to verify the complete lifecycle. I also test concurrent state changes, timeout-triggered transitions, and edge cases like cancelling an order during shipment to ensure robust handling of complex real-world scenarios.",
    category: "Manual Testing",
    difficulty: "HARD" as const,
    tags: ["manual-testing", "state-transition", "test-design", "workflow-testing"],
  },

  // ==========================================
  // SQL FOR TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "How do you use SQL SELECT statements to verify test data in a database?",
    idealAnswer: "SQL SELECT statements are fundamental for verifying that application operations correctly create, modify, or retrieve data in the database. I use SELECT with WHERE clauses to query specific records and verify their field values match expected results after a test action, such as confirming a new user record exists after registration. I also use aggregate functions like COUNT, SUM, and AVG to verify data integrity, such as confirming the total number of order items matches what the UI displays. Combining SELECT with JOIN operations allows me to verify data relationships across multiple tables, ensuring referential integrity is maintained. This database-level verification complements UI-level checks by confirming that the backend data layer is correct even if the frontend displays information accurately.",
    category: "SQL for Testing",
    difficulty: "EASY" as const,
    tags: ["SQL", "database-testing", "data-verification", "SELECT"],
  },
  {
    questionText: "What is the difference between INNER JOIN and LEFT JOIN, and when would you use each in testing?",
    idealAnswer: "An INNER JOIN returns only rows that have matching records in both joined tables, effectively filtering out non-matching rows. A LEFT JOIN returns all rows from the left table and matching rows from the right table, with NULL values for right-table columns where no match exists. In testing, I use INNER JOIN when I need to verify that related records exist in both tables, such as confirming all orders have associated customer records. I use LEFT JOIN when I need to find orphaned or missing records, such as identifying customers who have no orders or products with no associated inventory records. LEFT JOIN is particularly valuable for data integrity testing because the NULL results reveal broken relationships.",
    category: "SQL for Testing",
    difficulty: "EASY" as const,
    tags: ["SQL", "JOIN", "database-testing", "data-integrity"],
  },
  {
    questionText: "How do you use SQL to set up test data before running tests?",
    idealAnswer: "I use INSERT statements to create necessary test records in the database before executing tests, ensuring a known and consistent starting state. For complex test scenarios, I create SQL scripts that insert data across multiple related tables in the correct order to maintain foreign key constraints. I use transactions with BEGIN and ROLLBACK to allow easy cleanup after tests, or alternatively use DELETE and TRUNCATE statements in teardown scripts to remove test-specific data. I also leverage database fixtures or seed scripts that can be run repeatedly to reset the database to a baseline state. When working with automated tests, I integrate these SQL operations into test setup and teardown hooks to ensure each test starts with clean, predictable data.",
    category: "SQL for Testing",
    difficulty: "EASY" as const,
    tags: ["SQL", "test-data", "database-setup", "fixtures"],
  },
  {
    questionText: "How do you write SQL queries to identify data inconsistencies or duplicates in a database?",
    idealAnswer: "To find duplicates, I use GROUP BY with HAVING COUNT(*) > 1 on the columns that should be unique, such as finding duplicate email addresses with SELECT email, COUNT(*) FROM users GROUP BY email HAVING COUNT(*) > 1. For data inconsistencies, I use conditional aggregation and CASE statements to flag records violating business rules, like orders with negative totals or users with invalid status combinations. I also use NOT IN or NOT EXISTS subqueries to find orphaned records where expected parent records are missing. Cross-table validation queries using JOIN with WHERE conditions help identify referential integrity violations. These queries are essential for database health checks and are often incorporated into automated regression suites that run periodically to catch data quality issues early.",
    category: "SQL for Testing",
    difficulty: "MEDIUM" as const,
    tags: ["SQL", "data-integrity", "duplicates", "data-quality"],
  },
  {
    questionText: "Explain how you would use SQL transactions in test scripts to ensure data isolation.",
    idealAnswer: "I wrap test data operations in SQL transactions using BEGIN TRANSACTION at the start and ROLLBACK at the end, which ensures all changes made during the test are reverted and do not affect other tests or the database state. This approach provides complete data isolation because the test operates within its own transactional scope, and other concurrent tests see the original data due to transaction isolation levels. For tests that need to commit data, I use SAVEPOINT to create intermediate checkpoints that allow partial rollbacks. I also set the transaction isolation level appropriately: READ COMMITTED for standard tests to prevent dirty reads, and SERIALIZABLE for tests that verify concurrent access scenarios. This transactional approach eliminates the need for complex cleanup scripts and prevents test data from accumulating in shared test environments.",
    category: "SQL for Testing",
    difficulty: "MEDIUM" as const,
    tags: ["SQL", "transactions", "data-isolation", "test-cleanup"],
  },
  {
    questionText: "How do you use SQL to validate that database migrations have been applied correctly?",
    idealAnswer: "After a migration runs, I verify structural changes by querying the information_schema or system catalogs to confirm new tables, columns, indexes, and constraints exist with the correct data types and properties. For data migrations, I write validation queries that check record counts, verify data transformations match expected rules, and ensure no data loss occurred by comparing aggregates before and after. I also verify that existing functionality is not broken by running queries that represent common application access patterns. For Prisma-based migrations, I compare the output of prisma migrate status with expected migration history and use prisma db pull to verify the database schema matches the Prisma schema definition. I maintain a migration test suite that covers both the upgrade and rollback paths to ensure migrations can be safely applied and reverted in production.",
    category: "SQL for Testing",
    difficulty: "MEDIUM" as const,
    tags: ["SQL", "migrations", "schema-validation", "database-testing"],
  },
  {
    questionText: "How do you use SQL window functions for testing data ordering and ranking scenarios?",
    idealAnswer: "Window functions like ROW_NUMBER(), RANK(), DENSE_RANK(), and LEAD/LAG are invaluable for testing scenarios that involve ordered data, pagination, and relative comparisons. I use ROW_NUMBER() OVER (PARTITION BY category ORDER BY score DESC) to verify that leaderboard rankings are calculated correctly, or to test pagination by confirming the correct records appear on each page. LEAD and LAG functions help me verify sequential processing by comparing each record with its predecessor or successor, such as ensuring transaction timestamps are in chronological order. SUM() OVER (ORDER BY date) as running totals help verify cumulative calculations like account balances. These queries directly mirror the business logic the application implements, making them powerful tools for validating complex data processing features at the database level.",
    category: "SQL for Testing",
    difficulty: "MEDIUM" as const,
    tags: ["SQL", "window-functions", "ranking", "pagination-testing"],
  },
  {
    questionText: "Describe how you would design a SQL-based test data generation strategy for a complex relational database with many foreign key dependencies.",
    idealAnswer: "I would first map the complete dependency graph of the database schema, identifying the order in which tables must be populated to satisfy foreign key constraints. I would then create a layered data factory that generates records bottom-up: starting with reference tables and parent entities, then building up to child and junction tables. Each factory function would accept optional overrides while providing sensible defaults, using sequences for unique fields and random selection from valid foreign key values. For complex scenarios, I would use Common Table Expressions (CTEs) with INSERT RETURNING to chain insertions and capture generated IDs for use in subsequent inserts within a single statement. The strategy would include data profiles for different test scenarios such as minimal, typical, and stress data sets, and would support both deterministic data for reproducible tests and randomized data for exploratory testing. All generation scripts would be wrapped in transactions for easy cleanup and idempotency.",
    category: "SQL for Testing",
    difficulty: "HARD" as const,
    tags: ["SQL", "test-data-generation", "foreign-keys", "data-factory"],
  },
  {
    questionText: "How would you write SQL queries to detect race conditions and concurrency issues in a database?",
    idealAnswer: "To detect race conditions, I write queries that identify symptoms such as lost updates, duplicate records that should be unique, and inconsistent aggregates. For example, I query for cases where a concurrent booking system has oversold by comparing the count of confirmed bookings against the available capacity. I use queries with timestamp analysis to find records created within milliseconds of each other that have conflicting states, indicating concurrent access issues. For proactive testing, I design SQL scripts that simulate concurrent modifications using separate database sessions and verify outcomes against expected isolation level guarantees. I also query database lock tables and deadlock logs to identify contention patterns. Additionally, I use checksums and hash comparisons on critical aggregate data to detect inconsistencies that may only appear under concurrent load, and I instrument tests with pg_advisory_lock or similar mechanisms to create controlled race condition scenarios.",
    category: "SQL for Testing",
    difficulty: "HARD" as const,
    tags: ["SQL", "concurrency", "race-conditions", "database-testing"],
  },
  {
    questionText: "How do you use SQL to perform comprehensive data validation after a large-scale data migration between different database systems?",
    idealAnswer: "For cross-system data migration validation, I implement a multi-phase verification approach. First, I perform record count reconciliation by comparing row counts for every migrated table between source and target databases using linked server queries or exported counts. Second, I run checksum comparisons using hash aggregates on key columns to verify data integrity without comparing individual records. Third, I execute detailed sampling queries that randomly select records from both systems and compare them field by field, handling data type differences and encoding transformations. I create exception tables that capture any discrepancies found during validation for analysis and remediation. For data transformations applied during migration, I write inverse transformation queries to verify the mapping logic is correct. The validation suite also covers referential integrity verification in the target system, unique constraint validation, and comparison of aggregate statistics like sums, averages, and distributions across critical business metrics to ensure analytical accuracy is preserved.",
    category: "SQL for Testing",
    difficulty: "HARD" as const,
    tags: ["SQL", "data-migration", "validation", "cross-system"],
  },

  // ==========================================
  // JEST / UNIT TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is unit testing, and what makes a good unit test?",
    idealAnswer: "Unit testing is the practice of testing individual units of code, typically functions or methods, in isolation from their dependencies to verify they produce the correct output for given inputs. A good unit test follows the FIRST principles: Fast (executes in milliseconds), Independent (does not depend on other tests or external state), Repeatable (produces the same result every time), Self-validating (automatically determines pass or fail), and Timely (written close to the time the code is developed). Each test should focus on a single behavior, have a clear Arrange-Act-Assert structure, and use descriptive names that explain the scenario and expected outcome. Good unit tests also serve as living documentation of the code's intended behavior.",
    category: "Jest/Unit Testing",
    difficulty: "EASY" as const,
    tags: ["unit-testing", "jest", "fundamentals", "best-practices"],
  },
  {
    questionText: "What are mocks, stubs, and spies in Jest, and when would you use each?",
    idealAnswer: "Mocks are simulated objects that replace real dependencies and can be configured to return specific values while tracking how they were called. In Jest, you create mocks using jest.fn() or jest.mock() to replace entire modules. Stubs are a subset of mocks that provide predefined responses to calls made during the test, without tracking call information. Spies, created with jest.spyOn(), wrap real functions to track calls while optionally preserving the original implementation. I use mocks when I need to fully replace an external dependency like an API client, stubs when I need a simple predetermined response from a dependency, and spies when I want to verify that an existing function is called correctly while still allowing its actual code to execute.",
    category: "Jest/Unit Testing",
    difficulty: "EASY" as const,
    tags: ["jest", "mocks", "stubs", "spies", "test-doubles"],
  },
  {
    questionText: "How do you structure a Jest test file and organize your test suites?",
    idealAnswer: "I structure Jest test files by placing them alongside the source files they test with a .test.ts or .spec.ts extension, mirroring the source directory structure. Each test file uses describe blocks to group related tests by function or feature, and nested describe blocks for sub-categories like success cases, error cases, and edge cases. Individual test cases use the it or test function with descriptive names that read as complete sentences, such as 'it should return the user when a valid ID is provided.' I use beforeEach and afterEach hooks within describe blocks for shared setup and teardown, keeping tests DRY while maintaining independence. I also organize helper functions and shared fixtures in a __fixtures__ or __helpers__ directory adjacent to the test files for reusability.",
    category: "Jest/Unit Testing",
    difficulty: "EASY" as const,
    tags: ["jest", "test-organization", "describe", "test-structure"],
  },
  {
    questionText: "How do you test asynchronous code in Jest, including Promises and async/await?",
    idealAnswer: "Jest provides several approaches for testing asynchronous code. The preferred method is using async/await with the test function declared as async, allowing natural use of await and try/catch for assertions. For Promise-based code, I can return the Promise from the test and chain .then() for assertions, or use Jest's resolves and rejects matchers like expect(asyncFn()).resolves.toBe(value). For callback-based code, Jest provides the done callback parameter that must be called when the async operation completes. I always ensure assertions are actually reached by verifying expected assertion counts with expect.assertions(n), which prevents false positives where a test passes because the async code silently fails and assertions are never executed. For timer-based async code, I use jest.useFakeTimers() to control time advancement manually.",
    category: "Jest/Unit Testing",
    difficulty: "MEDIUM" as const,
    tags: ["jest", "async-testing", "promises", "async-await"],
  },
  {
    questionText: "How do you measure and improve code coverage in Jest?",
    idealAnswer: "Jest has built-in code coverage reporting activated with the --coverage flag, which generates reports showing line, branch, function, and statement coverage percentages for each file. I configure coverage thresholds in jest.config.js to enforce minimum coverage requirements, typically setting global thresholds and higher thresholds for critical business logic modules. To improve coverage, I analyze the uncovered branches report to identify untested conditional paths, error handlers, and edge cases. However, I focus on meaningful coverage rather than chasing 100% metrics, as high coverage with shallow assertions provides false confidence. I prioritize covering critical business logic, error handling, and boundary conditions, while excluding generated code, configuration files, and trivial getters/setters using the coveragePathIgnorePatterns configuration.",
    category: "Jest/Unit Testing",
    difficulty: "MEDIUM" as const,
    tags: ["jest", "code-coverage", "metrics", "quality"],
  },
  {
    questionText: "Explain how you would use Jest's snapshot testing feature and when it is appropriate.",
    idealAnswer: "Jest snapshot testing captures the rendered output of a component or data structure and saves it as a reference file, then compares subsequent test runs against this snapshot to detect unexpected changes. It is created using expect(component).toMatchSnapshot() or toMatchInlineSnapshot() for smaller outputs embedded directly in the test file. Snapshot testing is appropriate for UI components where you want to detect unintended rendering changes, serialized data structures, and API response formats. However, it should not be the only testing strategy because it only catches changes, not correctness. Snapshots should be kept small and focused by testing individual components rather than large page trees. When a snapshot test fails due to an intentional change, I update it using jest --updateSnapshot after verifying the new output is correct, and I review snapshot diffs carefully in code reviews.",
    category: "Jest/Unit Testing",
    difficulty: "MEDIUM" as const,
    tags: ["jest", "snapshot-testing", "UI-testing", "regression"],
  },
  {
    questionText: "How do you test error handling and edge cases in Jest?",
    idealAnswer: "I test error handling using expect().toThrow() for synchronous functions and expect().rejects.toThrow() for async functions, verifying both the error type and message. For functions that should throw specific error classes, I use toThrowError(CustomError) or toThrow(expect.objectContaining({code: 'ERR_NOT_FOUND'})). I systematically test edge cases by providing null, undefined, empty strings, empty arrays, extremely large values, negative numbers, and special characters as inputs. For boundary conditions, I test values at exact limits like minimum and maximum valid inputs. I also test that error handlers clean up resources properly by verifying that connections are closed and temporary files are deleted even when errors occur. Each error scenario gets its own test case with a descriptive name like 'should throw ValidationError when email format is invalid', ensuring the error handling behavior is fully documented through tests.",
    category: "Jest/Unit Testing",
    difficulty: "MEDIUM" as const,
    tags: ["jest", "error-handling", "edge-cases", "validation-testing"],
  },
  {
    questionText: "How would you implement a comprehensive test strategy for a complex Redux/Zustand store with side effects?",
    idealAnswer: "I would test state management at multiple levels: individual reducers/slices, action creators, selectors, middleware, and integration between store and components. For pure reducers, I write isolated unit tests verifying state transitions for each action type, including initial state and edge cases. For thunks or side effects, I mock API clients and verify the correct sequence of dispatched actions using middleware like redux-mock-store or by inspecting the store state after awaiting the async action. Selectors are tested by providing carefully crafted state objects and verifying the derived output, including memoization behavior. For Zustand specifically, I use the store's getState and setState methods directly in tests, and mock the store for component tests using jest.mock. Integration tests verify that components correctly read from and dispatch to the store, and I test error states, loading states, and race conditions where multiple actions fire in rapid succession.",
    category: "Jest/Unit Testing",
    difficulty: "HARD" as const,
    tags: ["jest", "state-management", "redux", "zustand", "side-effects"],
  },
  {
    questionText: "How do you design a testing strategy for dependency injection and inversion of control patterns in a Node.js service?",
    idealAnswer: "I structure services to accept dependencies through constructor injection or factory function parameters, making them inherently testable by allowing mock injection. Each service class receives interfaces rather than concrete implementations, enabling substitution of test doubles. In tests, I create lightweight mock implementations of each dependency that track calls and return controlled responses. For complex dependency graphs, I use a test container configuration that wires together real implementations for integration tests and mock implementations for unit tests. I test each service in isolation by injecting mocks for all dependencies, then write integration tests that wire real implementations together with an in-memory database. This layered approach catches both unit-level logic errors and integration-level wiring issues. I also verify that dependency injection containers correctly resolve all dependencies by writing container validation tests that instantiate the full dependency tree.",
    category: "Jest/Unit Testing",
    difficulty: "HARD" as const,
    tags: ["jest", "dependency-injection", "IoC", "architecture-testing"],
  },
  {
    questionText: "How would you implement mutation testing to evaluate the effectiveness of your Jest test suite?",
    idealAnswer: "Mutation testing evaluates test suite quality by introducing small code changes called mutants, such as flipping comparison operators, removing function calls, or changing return values, then running the test suite to see if it catches these mutations. I implement this using tools like Stryker Mutator, which is configured through stryker.conf.js to specify source files, test files, and mutation operators. A mutant is 'killed' when at least one test fails, and a 'survived' mutant indicates a gap in test coverage where the tests do not adequately verify the code behavior. I analyze survived mutants to identify undertested areas and write additional tests targeting the specific conditions that allowed mutants to survive. Mutation testing provides a more meaningful quality metric than code coverage because it measures how well tests actually verify behavior rather than just which lines are executed. I integrate mutation testing into CI as a periodic job rather than on every commit due to its computational cost.",
    category: "Jest/Unit Testing",
    difficulty: "HARD" as const,
    tags: ["jest", "mutation-testing", "test-quality", "stryker"],
  },

  // ==========================================
  // PERFORMANCE TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is performance testing, and what are its main types?",
    idealAnswer: "Performance testing evaluates how a system behaves under various conditions of load, stress, and resource utilization to ensure it meets performance requirements. The main types include load testing, which verifies the system handles expected concurrent user loads; stress testing, which pushes beyond normal capacity to find breaking points; endurance or soak testing, which runs sustained loads over extended periods to detect memory leaks and resource degradation; and spike testing, which evaluates response to sudden traffic surges. Each type serves a different purpose: load testing validates baseline performance, stress testing reveals system limits, endurance testing uncovers long-running issues, and spike testing verifies auto-scaling and recovery capabilities.",
    category: "Performance Testing",
    difficulty: "EASY" as const,
    tags: ["performance-testing", "load-testing", "stress-testing", "fundamentals"],
  },
  {
    questionText: "What are the key performance metrics you monitor during a performance test?",
    idealAnswer: "The key performance metrics include response time (average, median, 90th, 95th, and 99th percentiles), throughput (requests per second), error rate (percentage of failed requests), and concurrent users. I also monitor system-level metrics such as CPU utilization, memory usage, disk I/O, and network bandwidth on both application and database servers. For web applications, I track metrics like Time to First Byte (TTFB), page load time, and Largest Contentful Paint (LCP). Database-specific metrics include query execution time, connection pool utilization, and lock contention. I use percentile-based response time metrics rather than averages because averages can mask long tail latencies that affect a significant portion of users.",
    category: "Performance Testing",
    difficulty: "EASY" as const,
    tags: ["performance-testing", "metrics", "response-time", "throughput"],
  },
  {
    questionText: "What tools are commonly used for performance testing, and how do you choose between them?",
    idealAnswer: "Common performance testing tools include JMeter, an open-source Java-based tool with a GUI for creating test plans and extensive plugin ecosystem; k6, a modern JavaScript-based tool designed for developer workflows with excellent CI/CD integration; Gatling, a Scala-based tool with powerful DSL for complex scenarios and detailed HTML reports; and Locust, a Python-based tool that lets you define user behavior in code. I choose based on team skills, project requirements, and infrastructure. For API-heavy microservices, I prefer k6 for its scripting flexibility and lightweight resource usage. For teams less comfortable with code, JMeter's GUI is beneficial. For large-scale distributed testing, Gatling or k6 with cloud execution offer better scalability. The choice also depends on protocol support needed, as JMeter supports the widest range of protocols.",
    category: "Performance Testing",
    difficulty: "EASY" as const,
    tags: ["performance-testing", "JMeter", "k6", "Gatling", "tools"],
  },
  {
    questionText: "How do you design a realistic load test scenario that simulates actual user behavior?",
    idealAnswer: "I start by analyzing production traffic patterns using analytics and APM tools to understand user journey distributions, peak usage times, and geographic distribution. I model different user personas with distinct behavior patterns, such as browsers who mostly view pages, active users who perform CRUD operations, and power users who generate heavy queries. Each persona has a weighted think time between actions, simulating real human pauses. I configure ramp-up patterns that mimic actual traffic growth rather than instantly hitting full load, and I include a mix of concurrent operations reflecting production ratios like 60% reads and 40% writes. The test data set must be representative in size and variety to prevent caching bias. I also simulate different network conditions and incorporate realistic session lengths, including idle periods and session timeouts.",
    category: "Performance Testing",
    difficulty: "MEDIUM" as const,
    tags: ["performance-testing", "load-testing", "user-simulation", "test-design"],
  },
  {
    questionText: "How do you identify and diagnose performance bottlenecks in a web application?",
    idealAnswer: "I use a systematic approach starting from the top and drilling down. First, I examine the performance test results to identify which endpoints or operations have the highest response times and error rates. Then I correlate these with infrastructure metrics from monitoring tools to determine if the bottleneck is at the application, database, or infrastructure level. For application-level issues, I use APM tools like New Relic or Datadog to trace individual requests and identify slow methods, N+1 query patterns, or inefficient algorithms. For database bottlenecks, I analyze slow query logs, execution plans, and connection pool usage. I also check for external dependency latency by reviewing API call times to third-party services. Common bottlenecks include unoptimized database queries, insufficient connection pooling, memory leaks causing garbage collection pauses, and thread contention in concurrent code.",
    category: "Performance Testing",
    difficulty: "MEDIUM" as const,
    tags: ["performance-testing", "bottleneck-analysis", "diagnostics", "APM"],
  },
  {
    questionText: "What is the difference between performance testing in monolithic versus microservices architectures?",
    idealAnswer: "In monolithic architectures, performance testing is more straightforward because the entire application runs as a single unit, making it easier to simulate end-to-end scenarios and identify bottlenecks in a single deployment. In microservices, performance testing must account for inter-service communication latency, network overhead, distributed tracing complexity, and cascading failure scenarios. I test microservices at multiple levels: individual service benchmarks to establish baselines, service-to-service integration tests to measure communication overhead, and end-to-end workflow tests that traverse multiple services. Microservices testing also requires evaluating retry logic, circuit breaker behavior, and service discovery under load. The distributed nature means bottlenecks may shift between services under load, requiring correlation of metrics across all services using distributed tracing tools like Jaeger or Zipkin to follow requests through the entire call chain.",
    category: "Performance Testing",
    difficulty: "MEDIUM" as const,
    tags: ["performance-testing", "microservices", "distributed-systems", "architecture"],
  },
  {
    questionText: "How do you perform database performance testing and optimize query performance?",
    idealAnswer: "I test database performance by measuring query execution times under realistic data volumes and concurrent access patterns. I use EXPLAIN ANALYZE to examine query execution plans and identify full table scans, missing indexes, or suboptimal join strategies. Load tests simulate concurrent database connections to verify connection pool sizing and detect deadlocks or lock contention issues. I benchmark critical queries with production-scale data volumes because performance characteristics change dramatically with data growth. For optimization, I add targeted indexes based on EXPLAIN output, rewrite queries to eliminate N+1 patterns using eager loading, implement pagination for large result sets, and use materialized views or caching for expensive aggregate queries. I also test database connection pool exhaustion scenarios and validate that the application degrades gracefully when the database is under heavy load.",
    category: "Performance Testing",
    difficulty: "MEDIUM" as const,
    tags: ["performance-testing", "database", "query-optimization", "indexing"],
  },
  {
    questionText: "How would you design and implement a performance testing strategy for a system that must handle sudden traffic spikes of 10x normal load?",
    idealAnswer: "I would design a multi-phase testing strategy starting with baseline load tests at normal traffic levels to establish performance benchmarks, then gradually increase to 2x, 5x, and finally 10x normal load to identify scaling thresholds and breaking points. The spike test scenario would simulate sudden ramps from normal to 10x load within seconds to evaluate auto-scaling response time and queue backpressure behavior. I would test both the scale-up behavior (how quickly the system adapts) and scale-down behavior (how gracefully it returns to normal). Critical measurements include time to scale, request queue depth during scaling, error rates during the transition period, and any data consistency issues from concurrent processing. I would also implement chaos engineering practices by combining load with service failures to verify that the system maintains stability when both high traffic and infrastructure issues occur simultaneously. The strategy would include testing circuit breakers, bulkhead patterns, and graceful degradation mechanisms that shed non-critical load during peak periods.",
    category: "Performance Testing",
    difficulty: "HARD" as const,
    tags: ["performance-testing", "spike-testing", "auto-scaling", "resilience"],
  },
  {
    questionText: "Describe how you would implement continuous performance testing in a CI/CD pipeline.",
    idealAnswer: "I would integrate performance testing into the CI/CD pipeline at multiple stages: lightweight performance benchmarks run on every pull request using tools like k6 with reduced load profiles to catch performance regressions early, more comprehensive load tests run on staging deployments after merge, and full-scale performance test suites run on a scheduled basis against production-like environments. Each stage has defined performance budgets with automated pass/fail thresholds for response time percentiles, throughput, and error rates. I would establish baseline metrics from production and automatically compare PR results against these baselines, flagging regressions that exceed configured thresholds. The system would include historical trend dashboards showing performance metrics over releases, enabling teams to spot gradual degradation. I would also implement performance test result storage and comparison APIs that integrate with pull request reviews, automatically posting performance comparison comments. Infrastructure costs are managed by using ephemeral load-generation environments that spin up only during test execution.",
    category: "Performance Testing",
    difficulty: "HARD" as const,
    tags: ["performance-testing", "CI-CD", "continuous-testing", "automation"],
  },
  {
    questionText: "How do you perform memory leak detection and long-running stability testing for a Node.js application?",
    idealAnswer: "For Node.js memory leak detection, I run endurance tests that sustain moderate load over extended periods of 8-24 hours while monitoring heap memory usage, event loop lag, and garbage collection frequency and duration. I use tools like clinic.js, heapdump, and node --inspect to capture heap snapshots at intervals and compare them to identify growing object counts that indicate leaks. Common Node.js leak sources include unclosed event listeners, growing caches without eviction, unresolved promises accumulating in memory, and closures retaining references to large objects. During soak testing, I monitor not just memory but also file descriptor counts, database connection pool growth, and Redis connection counts for resource leaks beyond memory. I implement automated alerting that triggers when memory growth exceeds expected garbage collection patterns, and I use allocation timeline analysis to trace leaked objects back to the specific code paths that created them. The stability test suite also verifies graceful behavior when approaching memory limits, including proper out-of-memory handling and process restart recovery.",
    category: "Performance Testing",
    difficulty: "HARD" as const,
    tags: ["performance-testing", "memory-leak", "Node.js", "soak-testing"],
  },

  // ==========================================
  // API TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is API testing, and why is it important in a microservices architecture?",
    idealAnswer: "API testing validates that application programming interfaces meet their functional, reliability, performance, and security specifications by sending requests and verifying responses without involving the user interface. In microservices architecture, API testing is critical because services communicate primarily through APIs, making them the primary integration points where defects can manifest. Testing at the API level is faster and more stable than UI testing, provides earlier defect detection, and can cover scenarios that are difficult or impossible to test through the UI. It also enables testing of individual services in isolation using mocked dependencies, which supports parallel development and independent deployment of microservices.",
    category: "API Testing",
    difficulty: "EASY" as const,
    tags: ["API-testing", "microservices", "fundamentals", "integration"],
  },
  {
    questionText: "What are the key aspects you validate when testing a REST API endpoint?",
    idealAnswer: "When testing a REST API endpoint, I validate the HTTP status code to ensure the correct response code is returned for various scenarios such as 200 for success, 201 for creation, 400 for bad request, 401 for unauthorized, and 404 for not found. I verify the response body structure and data types match the API specification, and I check that response headers include correct content-type, caching directives, and CORS headers. I also validate error response formats for consistency, test request parameter validation by sending invalid inputs, verify authentication and authorization requirements, and measure response time against performance benchmarks. Additionally, I test idempotency for PUT and DELETE operations and verify proper handling of edge cases like empty request bodies, missing required fields, and excessively large payloads.",
    category: "API Testing",
    difficulty: "EASY" as const,
    tags: ["API-testing", "REST", "validation", "HTTP-status-codes"],
  },
  {
    questionText: "What tools do you use for API testing, and what are their strengths?",
    idealAnswer: "I use Postman for manual exploratory API testing and creating shareable collections that serve as living documentation, with its built-in test scripting using JavaScript for assertions. For automated API testing in CI/CD, I use tools like Supertest with Jest for Node.js applications, which allows writing HTTP assertions directly in test suites. REST Assured is excellent for Java projects with its fluent BDD-style API. For performance-focused API testing, I use k6 or Artillery. I also use Swagger/OpenAPI tools for contract validation and Pact for consumer-driven contract testing between services. The choice depends on the project context: Postman for ad-hoc exploration and team collaboration, Supertest/Jest for integrated automated testing, and specialized tools for performance and contract testing scenarios.",
    category: "API Testing",
    difficulty: "EASY" as const,
    tags: ["API-testing", "Postman", "Supertest", "tools"],
  },
  {
    questionText: "How do you test API authentication and authorization, including JWT token handling?",
    idealAnswer: "I test authentication by verifying that protected endpoints return 401 Unauthorized when no token is provided, when an expired token is sent, when a malformed token is used, and when a token with an invalid signature is presented. For JWT specifically, I test token generation with correct claims and expiration, token refresh flows, and logout/token revocation scenarios. Authorization testing verifies that authenticated users can only access resources they are permitted to, testing role-based access control by attempting operations with different user roles and verifying 403 Forbidden responses for unauthorized actions. I also test edge cases like concurrent token refresh requests, token reuse after refresh, and behavior when the user's role changes between token issuance and usage. Security-focused tests include verifying that tokens do not contain sensitive information in the payload and that the API is not vulnerable to token manipulation attacks.",
    category: "API Testing",
    difficulty: "MEDIUM" as const,
    tags: ["API-testing", "authentication", "authorization", "JWT", "security"],
  },
  {
    questionText: "What is contract testing, and how do you implement it for microservices?",
    idealAnswer: "Contract testing verifies that a service's API interactions conform to agreed-upon contracts between the consumer (service making the request) and the provider (service handling the request). I implement consumer-driven contract testing using Pact, where the consumer defines the expected API interactions including request format and expected response, generating a contract file. The provider then verifies against this contract, ensuring it can fulfill all consumer expectations. This approach catches integration issues early without requiring all services to be deployed together. Contracts are stored in a Pact Broker that tracks compatibility between service versions and enables 'can I deploy' checks before releasing new versions. This is far more efficient than traditional integration testing because each service can validate contracts independently in their own CI pipeline, and it catches breaking API changes before they reach shared test environments.",
    category: "API Testing",
    difficulty: "MEDIUM" as const,
    tags: ["API-testing", "contract-testing", "Pact", "microservices"],
  },
  {
    questionText: "How do you test API error handling, including validation errors and edge cases?",
    idealAnswer: "I systematically test error handling by sending requests with various invalid inputs and verifying the API returns appropriate error responses. This includes testing with missing required fields, invalid data types, values exceeding maximum lengths, strings where numbers are expected, special characters, SQL injection payloads, and oversized request bodies. I verify that error responses follow a consistent format with a meaningful error code, human-readable message, and field-level validation details when applicable. I also test rate limiting by exceeding request thresholds and verifying 429 Too Many Requests responses. Edge cases include testing with concurrent requests that may cause conflicts, sending requests during database maintenance windows, and testing timeout behavior for downstream service failures. Each error scenario should be documented and covered by automated tests to prevent regressions in error handling quality.",
    category: "API Testing",
    difficulty: "MEDIUM" as const,
    tags: ["API-testing", "error-handling", "validation", "edge-cases"],
  },
  {
    questionText: "How do you test pagination, filtering, and sorting in REST API endpoints?",
    idealAnswer: "For pagination, I test that the API correctly returns the requested page size, includes total count or next page indicators, handles edge cases like requesting pages beyond available data, and maintains consistent ordering across pages so no items are skipped or duplicated. I verify that cursor-based pagination produces correct results when items are added or deleted between page requests. For filtering, I test each filter parameter individually and in combination, verify that invalid filter values return appropriate errors, and ensure that filters correctly narrow the result set by cross-referencing with direct database queries. For sorting, I verify ascending and descending order for each sortable field, test multi-field sorting, and confirm that default sort order is applied when no sort parameter is specified. I also test performance implications by verifying that paginated endpoints with filters remain performant on large data sets.",
    category: "API Testing",
    difficulty: "MEDIUM" as const,
    tags: ["API-testing", "pagination", "filtering", "sorting"],
  },
  {
    questionText: "How would you implement comprehensive API testing for a GraphQL endpoint versus a REST endpoint?",
    idealAnswer: "GraphQL testing differs significantly from REST because a single endpoint serves multiple operation types, clients can request arbitrary field combinations, and query complexity can vary widely. I test query operations by verifying correct data retrieval for various field selections, nested object resolution, and pagination with connections and edges. Mutation tests verify data modifications, input validation, and correct response payloads including partial success scenarios for batch operations. I test error handling for invalid queries, unauthorized field access, and schema validation. A critical aspect unique to GraphQL is query complexity and depth limiting, where I verify that deeply nested or computationally expensive queries are rejected with appropriate errors. I also test query batching, N+1 query resolution via DataLoader, subscription functionality for real-time updates, and schema introspection security. For performance, I test that query complexity analysis correctly prevents resource-intensive queries from degrading system performance, which is not a concern with REST's predefined endpoints.",
    category: "API Testing",
    difficulty: "HARD" as const,
    tags: ["API-testing", "GraphQL", "REST", "query-testing"],
  },
  {
    questionText: "How do you design an API test framework that supports testing across multiple microservices with complex data dependencies?",
    idealAnswer: "I design the framework with several key layers: a service client layer that provides typed HTTP clients for each microservice, a data factory layer that creates test data through API calls in the correct order to satisfy inter-service dependencies, a test orchestration layer that manages multi-service test workflows, and a verification layer that validates outcomes across multiple services. The data factory uses a builder pattern to construct complex test scenarios, automatically creating prerequisite data in dependent services before the test and cleaning it up afterward. I implement service virtualization using tools like WireMock or Mountebank to mock unavailable or unstable dependencies, allowing each service to be tested independently. The framework includes a centralized configuration system for managing environment-specific URLs, authentication, and test data parameters. For cross-service transaction testing, I implement saga pattern verification that checks the final state across all involved services after a distributed operation completes.",
    category: "API Testing",
    difficulty: "HARD" as const,
    tags: ["API-testing", "framework-design", "microservices", "test-architecture"],
  },
  {
    questionText: "How do you test API versioning strategies and backward compatibility?",
    idealAnswer: "I test API versioning by maintaining test suites for each supported API version and running them in parallel to ensure all versions continue to function correctly when new versions are released. For backward compatibility, I create a compatibility test suite that uses the old version's request formats against the new version to verify that existing clients are not broken. I test version negotiation mechanisms whether via URL path versioning, header versioning, or query parameter versioning, verifying that the server correctly routes requests to the appropriate version handler. Deprecation testing verifies that deprecated endpoints return appropriate deprecation warnings in headers while still functioning correctly during the sunset period. I also implement contract tests that lock down the response structure of stable API versions, ensuring that schema changes are only allowed in new versions. For breaking change detection, I use tools that compare OpenAPI specifications between versions and automatically flag removals, type changes, or required field additions that would break existing consumers.",
    category: "API Testing",
    difficulty: "HARD" as const,
    tags: ["API-testing", "versioning", "backward-compatibility", "contracts"],
  },

  // ==========================================
  // CI/CD TESTING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is CI/CD, and how does testing fit into the pipeline?",
    idealAnswer: "Continuous Integration (CI) is the practice of frequently merging code changes into a shared repository, where each merge triggers automated builds and tests to detect issues early. Continuous Delivery (CD) extends this by automatically preparing code for release to production through additional testing and deployment stages. Testing is integral to every stage: unit tests and linting run during the build phase, integration and API tests run after deployment to a test environment, and end-to-end and smoke tests run after deployment to staging or production. This layered testing approach follows the test pyramid principle, where fast and numerous unit tests form the base, with progressively fewer but more comprehensive integration and end-to-end tests at higher levels.",
    category: "CI/CD Testing",
    difficulty: "EASY" as const,
    tags: ["CI-CD", "pipeline", "continuous-integration", "fundamentals"],
  },
  {
    questionText: "What is the test pyramid, and how does it guide your testing strategy in a CI/CD pipeline?",
    idealAnswer: "The test pyramid is a testing strategy model with three layers: a large base of fast unit tests, a middle layer of integration tests, and a small top of end-to-end UI tests. Unit tests form the foundation because they are fast, reliable, and cheap to maintain, catching most logic errors. Integration tests verify that components work together correctly, including database queries and API contracts. End-to-end tests validate complete user workflows but are slower, more brittle, and more expensive to maintain, so they cover only critical paths. In a CI/CD pipeline, I run unit tests on every commit for immediate feedback, integration tests on branch merges, and end-to-end tests on deployment to staging. This distribution ensures fast pipeline execution times while maintaining comprehensive coverage, typically following a 70-20-10 ratio of unit to integration to end-to-end tests.",
    category: "CI/CD Testing",
    difficulty: "EASY" as const,
    tags: ["CI-CD", "test-pyramid", "testing-strategy", "test-distribution"],
  },
  {
    questionText: "How do you handle test environment management in a CI/CD pipeline?",
    idealAnswer: "I manage test environments using infrastructure-as-code tools like Docker Compose for local and CI environments, and Terraform or Kubernetes manifests for cloud environments. Each CI pipeline run creates isolated ephemeral environments with all required services, databases, and dependencies using containerization, ensuring tests run in consistent, reproducible conditions. I use environment variables and configuration files to manage environment-specific settings like database URLs, API endpoints, and feature flags. Database state is managed through migration scripts that create the schema and seed scripts that populate baseline test data. After tests complete, the environment is torn down to free resources and prevent state leakage. For shared staging environments, I implement locking mechanisms to prevent concurrent deployments from interfering with each other.",
    category: "CI/CD Testing",
    difficulty: "EASY" as const,
    tags: ["CI-CD", "environment-management", "Docker", "infrastructure"],
  },
  {
    questionText: "How do you implement quality gates in a CI/CD pipeline?",
    idealAnswer: "Quality gates are automated checkpoints that must be passed before code progresses to the next pipeline stage. I implement them as pipeline step conditions that evaluate specific criteria: code coverage must meet minimum thresholds, all unit and integration tests must pass with zero failures, static code analysis from tools like SonarQube must show no critical issues, security vulnerability scans must reveal no high-severity findings, and code review approvals must be obtained. Each gate has clearly defined pass/fail criteria that are enforced automatically, preventing unacceptable code from reaching production. I configure gates progressively, with stricter criteria at later stages. The pipeline reports gate results prominently in pull request checks so developers get immediate feedback. I also track gate metrics over time to identify trends and adjust thresholds as the codebase matures.",
    category: "CI/CD Testing",
    difficulty: "MEDIUM" as const,
    tags: ["CI-CD", "quality-gates", "code-quality", "pipeline"],
  },
  {
    questionText: "How do you manage test data in CI/CD pipelines to ensure consistent and repeatable test results?",
    idealAnswer: "I use a layered approach to test data management in CI/CD. At the unit test level, test data is hardcoded within test fixtures and factories, ensuring complete isolation. For integration tests, I use database migration scripts to create the schema and seed scripts to populate baseline data, with each pipeline run starting from a clean database state. I implement data factories using libraries like Faker or custom builders that generate realistic but unique test data on each run, preventing conflicts from hardcoded IDs. For end-to-end tests that require complex data states, I create API-based data setup utilities that call the application's own APIs to create test scenarios programmatically. I avoid using shared persistent test databases in CI because they accumulate stale data and cause flaky tests. Instead, each pipeline run provisions an ephemeral database container, runs migrations, seeds data, executes tests, and destroys the database.",
    category: "CI/CD Testing",
    difficulty: "MEDIUM" as const,
    tags: ["CI-CD", "test-data", "data-management", "fixtures"],
  },
  {
    questionText: "What strategies do you use to keep CI/CD pipeline execution times manageable as the test suite grows?",
    idealAnswer: "I use several strategies to optimize pipeline speed. Test parallelization distributes tests across multiple runners or containers, with tools like Jest's --shard flag or Cypress parallelization splitting suites across machines. Test impact analysis uses code change detection to run only tests affected by the modified files, dramatically reducing execution time for incremental changes. I organize the pipeline into sequential stages with fast-fail behavior, running quick unit tests first and only proceeding to slower integration and end-to-end tests if they pass. Caching strategies include storing dependency installations, Docker layer caches, and compiled artifacts between pipeline runs. I also maintain test suite health by regularly removing redundant tests, optimizing slow tests, and quarantining flaky tests. For large codebases, I implement a tiered approach where a fast subset runs on every PR and the full suite runs nightly or before release.",
    category: "CI/CD Testing",
    difficulty: "MEDIUM" as const,
    tags: ["CI-CD", "pipeline-optimization", "parallelization", "caching"],
  },
  {
    questionText: "How do you implement and manage feature flags in the context of CI/CD testing?",
    idealAnswer: "Feature flags allow deploying code with new features disabled, then enabling them gradually through configuration rather than code changes. In CI/CD testing, I ensure tests cover both flag-on and flag-off states by parameterizing test suites to run with different flag configurations. I use feature flag management platforms like LaunchDarkly or custom implementations that integrate with the test environment configuration. During CI, tests for the new feature run with the flag enabled, while regression tests run with the flag disabled to ensure existing behavior is not affected. I also write specific tests that verify the flag toggle mechanism itself works correctly, including default values and fallback behavior when the flag service is unavailable. As features become permanently enabled, I create cleanup tasks to remove the flag conditions from both code and tests, preventing flag technical debt from accumulating.",
    category: "CI/CD Testing",
    difficulty: "MEDIUM" as const,
    tags: ["CI-CD", "feature-flags", "testing-strategy", "gradual-rollout"],
  },
  {
    questionText: "How would you design a testing strategy for canary deployments and blue-green deployment patterns?",
    idealAnswer: "For canary deployments, I design a progressive testing strategy where automated smoke tests run immediately against the canary instances to verify basic functionality, followed by continuous monitoring of error rates, response times, and business metrics against the baseline production instances. I implement automated rollback triggers based on statistical comparison between canary and production metrics, using tools like Kayenta for automated canary analysis. For blue-green deployments, I run the complete regression test suite against the green (new) environment before switching traffic, including database migration verification, API contract tests, and critical end-to-end user journeys. I also test the traffic switching mechanism itself, including partial traffic routing for gradual migration and instant rollback capability. The strategy includes testing post-deployment verification that confirms the new version is serving traffic correctly, and I implement synthetic monitoring that continuously runs critical test scenarios against production to detect issues that slip through pre-deployment testing.",
    category: "CI/CD Testing",
    difficulty: "HARD" as const,
    tags: ["CI-CD", "canary-deployment", "blue-green", "deployment-testing"],
  },
  {
    questionText: "How do you implement end-to-end testing in a pipeline with multiple microservices that are deployed independently?",
    idealAnswer: "I implement a multi-layered integration testing strategy that accounts for independent deployment cycles. Each microservice has its own pipeline with unit and contract tests that run on every change. Consumer-driven contract tests using Pact verify API compatibility between services without requiring them to be co-deployed. For end-to-end testing, I maintain a dedicated integration environment where services are deployed upon passing their individual pipelines, and a suite of end-to-end tests runs against this environment on a scheduled basis and after each service deployment. I use service virtualization to mock unstable or slow external dependencies during testing. The end-to-end suite is organized by business capability rather than by service, testing complete user journeys that span multiple services. I implement a compatibility matrix that tracks which service versions have been tested together, enabling the CI system to detect incompatible version combinations before they reach production. Deployment orchestration ensures that dependent service updates are coordinated when breaking changes are necessary.",
    category: "CI/CD Testing",
    difficulty: "HARD" as const,
    tags: ["CI-CD", "microservices", "e2e-testing", "independent-deployment"],
  },
  {
    questionText: "How would you implement a comprehensive testing strategy for infrastructure-as-code changes in a CI/CD pipeline?",
    idealAnswer: "I test infrastructure-as-code changes at multiple levels, starting with static analysis using tools like tflint for Terraform or cfn-lint for CloudFormation to catch syntax errors and best practice violations. Policy-as-code tools like OPA Rego or Sentinel verify that infrastructure changes comply with security and compliance requirements, such as ensuring encryption is enabled and public access is restricted. I use terraform plan output parsing to verify expected changes and detect destructive operations that require approval. For functional testing, I use tools like Terratest or Kitchen-Terraform that provision real infrastructure in an isolated test account, run validation tests against it, and tear it down afterward. I also implement chaos testing for infrastructure by verifying that auto-scaling, failover, and disaster recovery mechanisms work correctly when infrastructure components fail. The pipeline includes drift detection that compares actual infrastructure state with the declared configuration, and cost estimation tools that predict the financial impact of infrastructure changes before they are applied.",
    category: "CI/CD Testing",
    difficulty: "HARD" as const,
    tags: ["CI-CD", "infrastructure-as-code", "Terraform", "DevOps-testing"],
  },

  // ==========================================
  // TEST STRATEGY / PLANNING (10 questions: 3 EASY, 4 MEDIUM, 3 HARD)
  // ==========================================
  {
    questionText: "What is the difference between a test strategy and a test plan?",
    idealAnswer: "A test strategy is a high-level document that defines the overall approach to testing across the organization or project, including testing objectives, test levels, types of testing, tools and environments, entry and exit criteria, and risk management approaches. It is typically a static document that changes infrequently. A test plan is a more detailed, project-specific document that describes how the test strategy will be implemented for a particular release or feature, including specific test cases, schedules, resource assignments, and milestone deliverables. While the test strategy sets the direction and standards, the test plan provides the actionable roadmap for executing testing activities within a specific timeframe. A good test plan aligns with the overall test strategy while adapting to the specific context and constraints of the project.",
    category: "Test Strategy",
    difficulty: "EASY" as const,
    tags: ["test-strategy", "test-plan", "documentation", "fundamentals"],
  },
  {
    questionText: "What are entry and exit criteria in testing, and why are they important?",
    idealAnswer: "Entry criteria are the conditions that must be met before testing can begin, such as code deployment to the test environment, test data availability, test environment stability, and completion of unit testing by developers. Exit criteria are the conditions that must be satisfied to conclude a testing phase, such as all critical and high-priority test cases executed, no open critical or high-severity defects, test coverage meeting defined thresholds, and performance benchmarks achieved. These criteria are important because they provide objective, measurable checkpoints that prevent premature testing starts and premature declarations of testing completion. Without clear entry criteria, testers may waste time on unstable builds, and without exit criteria, releases may go out with inadequate testing or teams may over-test low-risk features while under-testing critical ones.",
    category: "Test Strategy",
    difficulty: "EASY" as const,
    tags: ["test-strategy", "entry-criteria", "exit-criteria", "quality-gates"],
  },
  {
    questionText: "How do you estimate the effort required for testing a new feature?",
    idealAnswer: "I estimate testing effort by analyzing the feature complexity, scope of affected areas, and types of testing required. I break the feature into testable components and estimate time for test case design, test data preparation, environment setup, test execution, defect reporting, and retesting. I consider factors like the number of user scenarios, integration points with other systems, and the need for cross-browser or cross-platform testing. Historical data from similar features provides a baseline, which I adjust based on team familiarity and technical risk. I use three-point estimation with optimistic, realistic, and pessimistic scenarios to account for uncertainty, and I include buffer for regression testing, environment issues, and defect investigation. I present estimates as ranges rather than fixed numbers and refine them as more information becomes available during the development cycle.",
    category: "Test Strategy",
    difficulty: "EASY" as const,
    tags: ["test-strategy", "estimation", "planning", "effort"],
  },
  {
    questionText: "How do you implement risk-based testing, and what factors determine risk priority?",
    idealAnswer: "Risk-based testing prioritizes testing effort based on the probability and impact of potential failures. I create a risk matrix by identifying features and areas of the system, then assessing each for two dimensions: likelihood of failure, based on code complexity, change frequency, developer experience, and technology maturity, and business impact of failure, based on user impact, revenue effect, regulatory consequences, and reputation damage. I multiply these factors to calculate a risk score that drives test prioritization and depth. High-risk areas receive thorough testing with extensive positive, negative, and edge case coverage, while low-risk areas receive lighter coverage focused on critical paths only. This approach is implemented as a living process where risk assessments are updated as new information emerges during development and testing. The risk model also informs test automation investment decisions, prioritizing automation for high-risk, frequently-executed test scenarios.",
    category: "Test Strategy",
    difficulty: "MEDIUM" as const,
    tags: ["test-strategy", "risk-based-testing", "prioritization", "risk-analysis"],
  },
  {
    questionText: "How do you determine the right balance between manual and automated testing?",
    idealAnswer: "The balance depends on the project's specific context including release frequency, team skills, budget, and the nature of the application. I automate tests that are executed frequently, are stable and repeatable, require large data sets, or are critical regression checks, as these provide the highest ROI for automation. Manual testing is better suited for exploratory testing, usability evaluation, visual inspection, ad-hoc scenarios, and tests for features that change frequently during active development. For a typical web application with regular releases, I aim for 70-80% automated coverage of regression tests and maintain a dedicated manual testing effort for new features and exploratory sessions. The key principle is that automation should complement, not replace, manual testing. I also consider the maintenance cost of automated tests and avoid automating tests where the maintenance burden exceeds the value they provide.",
    category: "Test Strategy",
    difficulty: "MEDIUM" as const,
    tags: ["test-strategy", "automation-vs-manual", "ROI", "balance"],
  },
  {
    questionText: "How do you measure and communicate testing effectiveness and quality metrics to stakeholders?",
    idealAnswer: "I track and report a combination of process metrics and product metrics. Process metrics include test case execution rate, automation coverage percentage, defect detection rate per testing phase, and test cycle time. Product metrics include defect density per module, defect leakage rate to production, mean time to detect and resolve defects, and customer-reported issues post-release. I present these metrics in dashboards that show trends over releases, making it easy to identify improvements or regressions. For stakeholder communication, I translate technical metrics into business-relevant insights, such as 'our automation caught 15 regression bugs that would have taken 3 days to find manually' or 'defect leakage decreased by 40% compared to last quarter.' I also track the cost of quality including prevention costs, detection costs, and failure costs to demonstrate the ROI of testing investments and justify resource requests.",
    category: "Test Strategy",
    difficulty: "MEDIUM" as const,
    tags: ["test-strategy", "metrics", "stakeholder-communication", "quality"],
  },
  {
    questionText: "How do you integrate testing into an Agile/Scrum workflow?",
    idealAnswer: "In Agile, testing is integrated throughout the sprint rather than being a separate phase. During sprint planning, I participate in story estimation and identify testing needs for each user story. I collaborate with developers to define acceptance criteria that are testable and specific. Test case design begins as soon as stories are groomed, and I use BDD-style scenarios that serve as both requirements and test cases. During the sprint, I perform continuous testing as features are completed, providing immediate feedback to developers rather than batching defects at the end. I maintain and update automated regression tests within the sprint, treating test automation as a first-class development activity. In sprint reviews, I demonstrate quality metrics and any notable defects found. Retrospectives include discussion of testing process improvements. This embedded approach ensures quality is built in rather than tested in, and testing never becomes a bottleneck at the end of the sprint.",
    category: "Test Strategy",
    difficulty: "MEDIUM" as const,
    tags: ["test-strategy", "Agile", "Scrum", "shift-left"],
  },
  {
    questionText: "How would you design a testing strategy for a product that needs to support 20 different browser and device combinations?",
    idealAnswer: "I would implement a tiered cross-browser testing strategy that balances coverage with practical constraints. First, I analyze usage analytics to identify the top browser and device combinations that cover 90% or more of the user base and designate these as Tier 1 for full regression testing. Tier 2 covers the next most popular combinations with critical path testing only, and Tier 3 covers remaining combinations with smoke testing. I leverage cloud testing platforms like BrowserStack or Sauce Labs for on-demand access to all combinations without maintaining local infrastructure. The automation framework uses responsive design testing techniques that verify layout breakpoints and touch interactions for mobile devices. I implement visual regression testing with platform-specific baselines to catch rendering differences across browsers. For efficiency, I parallelize cross-browser test execution across the cloud grid, and I use a progressive enhancement testing approach where core functionality is verified on all browsers while advanced features are tested on their supported browsers. The strategy includes a regular review cycle where the tier assignments are updated based on evolving usage patterns.",
    category: "Test Strategy",
    difficulty: "HARD" as const,
    tags: ["test-strategy", "cross-browser", "device-testing", "compatibility"],
  },
  {
    questionText: "How would you build a quality assurance culture in an organization transitioning from no formal testing to a comprehensive QA practice?",
    idealAnswer: "I would take a phased approach starting with quick wins that demonstrate value. Phase one establishes foundational practices: setting up a CI pipeline with basic linting and unit test requirements, introducing code review with quality checklists, and implementing a defect tracking process. Phase two introduces systematic testing by creating test plans for critical features, establishing testing standards and templates, and beginning automation of the most valuable regression tests. Phase three matures the practice by implementing comprehensive test strategies, introducing specialized testing like performance and security, and building monitoring and observability for production quality metrics. Throughout the transition, I focus on education and enablement by conducting workshops on testing techniques, pairing with developers to write tests, and sharing defect analysis reports that quantify the cost of quality issues. I advocate for quality as a team responsibility rather than a QA-only concern, promoting practices like developer-written unit tests, shared ownership of integration tests, and collaborative test design sessions. Success metrics track both quality improvements and team adoption rates.",
    category: "Test Strategy",
    difficulty: "HARD" as const,
    tags: ["test-strategy", "QA-culture", "organizational-change", "process-improvement"],
  },
  {
    questionText: "How do you create a test strategy for a regulated industry application where compliance and audit trails are required?",
    idealAnswer: "For regulated industries like healthcare, finance, or aerospace, the test strategy must address compliance requirements from frameworks such as FDA 21 CFR Part 11, SOX, HIPAA, or DO-178C. I start by mapping each regulatory requirement to specific testing activities and acceptance criteria, creating a requirements traceability matrix that links every requirement to its verification method. Test documentation must be comprehensive and include detailed test procedures, expected results, actual results, tester identification, and execution timestamps to satisfy audit requirements. I implement electronic signatures and version control for all test artifacts, with change history tracked at the document level. The strategy includes validation of the test tools themselves to ensure they produce reliable results, as regulators require tool validation. I define a formal defect classification and resolution process with mandatory root cause analysis for critical defects, and I maintain an audit-ready test evidence repository with immutable test execution records. Risk-based testing is particularly important in regulated contexts because it provides documented justification for testing scope decisions that auditors expect to see.",
    category: "Test Strategy",
    difficulty: "HARD" as const,
    tags: ["test-strategy", "compliance", "regulated-industry", "audit"],
  },

  // ==========================================
  // SECURITY TESTING (10 questions: 3 EASY, 3 MEDIUM, 4 HARD)
  // ==========================================
  {
    questionText: "What is security testing, and what are its main types?",
    idealAnswer: "Security testing evaluates the security mechanisms of an application to identify vulnerabilities, threats, and risks that could lead to unauthorized access, data breaches, or system compromise. The main types include vulnerability scanning using automated tools to detect known vulnerabilities, penetration testing that simulates real-world attacks to find exploitable weaknesses, security auditing that reviews code and configurations against security standards, and risk assessment that evaluates the overall security posture. Static Application Security Testing (SAST) analyzes source code for vulnerabilities without executing it, while Dynamic Application Security Testing (DAST) tests the running application from an external perspective. Interactive Application Security Testing (IAST) combines both approaches by analyzing the application during execution. Each type addresses different aspects of security and together they provide comprehensive security assurance.",
    category: "Security Testing",
    difficulty: "EASY" as const,
    tags: ["security-testing", "SAST", "DAST", "vulnerability-scanning"],
  },
  {
    questionText: "What is the OWASP Top 10, and why is it important for QA engineers?",
    idealAnswer: "The OWASP Top 10 is a standard awareness document published by the Open Web Application Security Project that identifies the ten most critical web application security risks. The current list includes issues like broken access control, cryptographic failures, injection attacks, insecure design, security misconfiguration, vulnerable components, identification and authentication failures, software and data integrity failures, security logging and monitoring failures, and server-side request forgery. It is important for QA engineers because it provides a prioritized checklist of the most common and dangerous vulnerabilities to test for in web applications. By incorporating OWASP Top 10 testing into the regular test strategy, QA teams can catch the majority of common security issues before they reach production. I use it as a foundation for security test case design and as a benchmark for evaluating application security posture.",
    category: "Security Testing",
    difficulty: "EASY" as const,
    tags: ["security-testing", "OWASP", "web-security", "best-practices"],
  },
  {
    questionText: "What is SQL injection, and how do you test for it?",
    idealAnswer: "SQL injection is a vulnerability where an attacker can insert or manipulate SQL queries through user input fields, potentially gaining unauthorized access to or modification of database data. I test for SQL injection by entering SQL syntax in input fields such as single quotes, OR 1=1 conditions, UNION SELECT statements, and comment sequences like -- to see if the application processes them as SQL rather than plain text. I verify that the application uses parameterized queries or prepared statements by checking if special characters in inputs are properly escaped. I also use automated scanning tools like SQLMap or OWASP ZAP that systematically test all input vectors for various injection techniques including error-based, blind, and time-based injection. Beyond basic form fields, I test HTTP headers, cookies, URL parameters, and API request bodies for injection vulnerabilities, as these are often overlooked attack surfaces.",
    category: "Security Testing",
    difficulty: "EASY" as const,
    tags: ["security-testing", "SQL-injection", "OWASP", "input-validation"],
  },
  {
    questionText: "How do you test for Cross-Site Scripting (XSS) vulnerabilities?",
    idealAnswer: "I test for XSS by attempting to inject JavaScript code through all user input points and verifying whether the application executes the injected script or properly sanitizes it. For reflected XSS, I inject payloads like <script>alert('XSS')</script> in URL parameters and form fields and check if they are reflected back in the response without encoding. For stored XSS, I inject payloads in data that is saved and later displayed to other users, such as comments, profile fields, and message boards. For DOM-based XSS, I analyze client-side JavaScript that processes URL fragments or user input and inject payloads that exploit insecure DOM manipulation like innerHTML assignment. I test various payload encodings including HTML entities, URL encoding, and JavaScript unicode escapes to bypass input filters. I also verify that the application sets appropriate Content-Security-Policy headers, uses HttpOnly and Secure flags on cookies, and properly encodes output in different contexts such as HTML content, attributes, JavaScript, and CSS.",
    category: "Security Testing",
    difficulty: "MEDIUM" as const,
    tags: ["security-testing", "XSS", "cross-site-scripting", "web-security"],
  },
  {
    questionText: "How do you test authentication and session management security?",
    idealAnswer: "I test authentication security by verifying brute force protection through rate limiting and account lockout after failed attempts, password policy enforcement including minimum length and complexity requirements, and secure credential storage using salted hashing algorithms. For session management, I verify that session tokens are sufficiently long and random, are transmitted only over HTTPS, have appropriate expiration timeouts, and are invalidated on logout. I test for session fixation by checking if the application regenerates session IDs after login, and for session hijacking by verifying HttpOnly and Secure cookie flags. I also test multi-factor authentication flows, password reset mechanisms for information leakage, and account enumeration through login error messages. Additional checks include verifying that sensitive operations require re-authentication, that concurrent session handling is properly implemented, and that session data is not exposed in URLs or local storage.",
    category: "Security Testing",
    difficulty: "MEDIUM" as const,
    tags: ["security-testing", "authentication", "session-management", "OWASP"],
  },
  {
    questionText: "What tools do you use for security testing, and how do you integrate them into the development workflow?",
    idealAnswer: "I use a combination of SAST, DAST, and dependency scanning tools integrated at different pipeline stages. For SAST, tools like SonarQube, Semgrep, or CodeQL analyze source code in the CI pipeline on every pull request, catching vulnerabilities like injection flaws, hardcoded secrets, and insecure patterns before code is merged. For dependency scanning, Snyk or npm audit identifies known vulnerabilities in third-party libraries and blocks deployment of affected versions. For DAST, OWASP ZAP runs automated scans against deployed applications in staging environments, testing for runtime vulnerabilities like XSS, CSRF, and misconfigurations. I configure these tools with severity-based gating where critical and high vulnerabilities block the pipeline, while medium and low vulnerabilities generate warnings for review. Findings are automatically triaged into the team's issue tracker with severity classifications, and I maintain suppression lists for accepted risks with documented justifications to prevent false positive fatigue.",
    category: "Security Testing",
    difficulty: "MEDIUM" as const,
    tags: ["security-testing", "tools", "SAST", "DAST", "CI-CD"],
  },
  {
    questionText: "How do you perform authorization testing to verify that users cannot access resources or actions beyond their privileges?",
    idealAnswer: "I perform authorization testing using both horizontal and vertical privilege escalation checks. Vertical testing verifies that lower-privileged users cannot access higher-privileged functions by systematically attempting admin operations with regular user credentials, testing every API endpoint with tokens from different role levels, and verifying that the response is 403 Forbidden rather than a data leak or error that reveals information. Horizontal testing verifies that users cannot access other users' data by manipulating resource identifiers in URLs and API parameters, such as changing a user ID in the request to another user's ID. I create a comprehensive access control matrix mapping every resource and action to authorized roles, then automate tests for each cell in the matrix. I also test for insecure direct object references by manipulating sequential IDs, testing GUID predictability, and verifying that server-side authorization checks are performed regardless of what the client sends. For complex permission systems, I test inheritance, delegation, and permission boundary scenarios to ensure the authorization logic handles all combinations correctly.",
    category: "Security Testing",
    difficulty: "HARD" as const,
    tags: ["security-testing", "authorization", "access-control", "privilege-escalation"],
  },
  {
    questionText: "How do you design and execute a comprehensive penetration testing plan for a web application?",
    idealAnswer: "I design a penetration test plan following a structured methodology like OWASP Testing Guide or PTES. The plan begins with reconnaissance gathering information about the application through both passive techniques like WHOIS, DNS enumeration, and public code repositories, and active techniques like port scanning and service fingerprinting. The testing phase follows a systematic checklist covering authentication bypass, session management flaws, injection vulnerabilities across all input vectors, business logic flaws, file upload vulnerabilities, API security, and infrastructure misconfigurations. I document each finding with proof-of-concept exploits that demonstrate the vulnerability's real-world impact, severity rating using CVSS scoring, and specific remediation recommendations. The plan includes rules of engagement defining scope, timing, and escalation procedures, and excludes denial-of-service attacks against production systems. After remediation, I perform retesting to verify fixes are effective and have not introduced new vulnerabilities. The final report includes an executive summary for management and a detailed technical report for developers.",
    category: "Security Testing",
    difficulty: "HARD" as const,
    tags: ["security-testing", "penetration-testing", "OWASP", "methodology"],
  },
  {
    questionText: "How do you test for API-specific security vulnerabilities beyond the standard OWASP Top 10?",
    idealAnswer: "API-specific security testing covers the OWASP API Security Top 10, which addresses vulnerabilities unique to API architectures. I test for broken object-level authorization (BOLA) by manipulating resource IDs across different authenticated sessions, and broken function-level authorization by attempting privileged API operations with regular user tokens. Mass assignment testing involves sending extra fields in request bodies to see if the API binds them to internal model properties, potentially allowing privilege escalation or data manipulation. I test rate limiting exhaustion by exceeding documented limits and verifying proper throttling, and I check for excessive data exposure by analyzing response payloads for fields that should not be visible to the requesting user's role. I also test GraphQL-specific vulnerabilities like introspection exposure, query depth attacks, and batch query abuse. For JWT-based APIs, I test algorithm confusion attacks by changing the algorithm header to 'none' or switching from RS256 to HS256. I verify that API keys are not exposed in client-side code, that CORS policies are properly restrictive, and that webhook endpoints validate request signatures to prevent server-side request forgery.",
    category: "Security Testing",
    difficulty: "HARD" as const,
    tags: ["security-testing", "API-security", "OWASP-API", "BOLA"],
  },
  {
    questionText: "How do you implement a comprehensive security testing strategy for a CI/CD pipeline that includes both automated scanning and manual security review?",
    idealAnswer: "I implement a defense-in-depth security testing strategy across the entire SDLC. In the IDE and pre-commit phase, developers use security linting plugins and pre-commit hooks that scan for secrets, insecure patterns, and banned functions. During CI, the pipeline runs SAST analysis with tools like Semgrep or CodeQL, dependency vulnerability scanning with Snyk, container image scanning with Trivy, and infrastructure-as-code scanning with Checkov. On deployment to staging, DAST scanners like OWASP ZAP perform automated penetration testing, and IAST agents instrument the application during functional test execution to detect runtime vulnerabilities. Manual security reviews are scheduled at key milestones: threat modeling during design, security code review for high-risk changes using a defined review checklist, and quarterly penetration tests by internal security champions or external firms. All findings feed into a unified vulnerability management system with SLA-based remediation timelines tied to severity. I track security debt alongside technical debt and report security posture metrics to stakeholders, including vulnerability trend analysis, mean time to remediate, and escape rate of vulnerabilities reaching production.",
    category: "Security Testing",
    difficulty: "HARD" as const,
    tags: ["security-testing", "CI-CD-security", "DevSecOps", "comprehensive-strategy"],
  },
];
